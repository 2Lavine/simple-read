
# Introduction to real Systems

## Real Time Systems

Definition
* A real-time system is one which "controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time
- must respond as rapidly
	- Real-time responses are  understood to be in the order of milliseconds
 * uarantee response within  "deadlines”
 
## Real Time Systems Classification
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402260951042.png)
## Streaming Data
Definition
- Streaming data is data that is continuously generated by different sources. 
- Such data should be processed incrementally using Stream Processing techniques without **having access to all of the data**.

A data stream is a possibly unbounded sequence of tuples.

## Transactional Data Streams
* Transactional data streams are **logs of interaction between entities.** 
 * Event Logs of Websites
	 *  logs are monitored for applications such as performance monitoring, personalization. 
	 * The information on this interaction continuously gets appended to the log in the form of new log entries.
 * Credit Card Purchases:
	 * Vendors of credit card monitor purchases by their credit card holders to detect anomalies that indicate possible fraudulent use of a credit card issued by them. 
	 * Log of credit card transactions forms a continuous data stream of transactional data.
## Measurement Data Streams
These types of data streams are produced as a result of monitoring the state of entities of interest.
 * Data from Sensors
 * Large-scale communication networks require continuous monitoring for many tasks, such as locating bottlenecks, or diagnosing attacks on the network. The data monitored consists of the packet header information across a set of network routers. Thus this data can be viewed as a data stream.

## Example

Road Traffic Monitoring Systems
 * Sensor embedded along expressways report on current traffic on the road.
 * Data from the sensor:
	 * Identifier of a car as well as its speed.
	 * The id of the expressway and the segment and lane of the expressway on which the car is traveling, as well the direction in which the car is traveling.
 * Queries evaluated with the data for the following:
	 * To manage the flow of traffic, a query would compute the average speed of cars over a segment. If the speed drops below a threshold, commuters about to travel on that segment can be notified to take alternative routes.
	 * Again, based on average speed of vehicles over a segment, the application might alert operators to possible accidents that are causing a slow down on that segment.
	 * On toll roads, the data stream is used to automatically deduct toll from the account of those vehicle owners that have tags recognized by the sensors.

## Stream Data Consumption
Three possible usages of streams:
 * Real Time Operations
	 * Data can be processed directly from streams for operational use.
	 * Example: Updating Road Displays regarding Traffic conditions
 * Real Time Analysis / Analytics
	 * Data can be processed directly from streams for decision purposes
	 * Example: Intruder detection when hacker tries to breach into systems
 * Persisting for later use
	 * Data can be saved on to databases for later retrieval and process.
	 * This is typical application scenario that is handled even for non-stream based data inflows.
	 * Example: Saving weather data from sensors into databases for historical purposes.
## Architectural Overview
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402260958929.png)

## Architectural Components – Twitter Example
Collection (Ingestion) tier
* Example: When a user posts a tweet, it is collected by the Twitter services.
Message queuing tier
 * Example: Twitter runs data centers in locations across the globe, and conceivably the  collection of a tweet doesn’t happen in the same location as the analysis of the tweet.
Analysis tier
 * Example: Lot of processing can be done on those 140 characters; at a minimum Twitter needs to identify the followers of a tweet.
Long-term storage tier
 * To analyze tweets going back in time, selected tweets are stored in a persistent data store.
In-memory data store tier
 * The tweets that are mere seconds old are most likely held in an in-memory data store.
Data access
 * All Twitter clients need to be connected to Twitter to access the service.



# Messaging Interaction Patterns
## System Interactions
* Systems/Applications/Devices/Servers communicate to one another to perform a business process.
Example
 * Ingestion and collection is where data comes into the system and starts its journey; from here it will progress through the rest of the system.
 * One application invokes another to perform a function.
 * An application calls a database for data storage or retrieval.

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261003631.png)

## Interaction Patterns
* Types of Interaction patterns
 * Request/Response Pattern (synchronous)
 * Request/Acknowledge Pattern
 * Asynchronous Patterns
 * Client emulation (Half Async)
 * Server Emulation (Half Async)
 * Both sides (Full Async)
 * One-way pattern
 * Paired One-way Pattern
 * Publish/Subscribe Pattern
 * Broadcast
 * Stream pattern
## Request / Response pattern
* Request: client makes a request to a service.
	* This may be to take an action 
		* such as send a text message, apply for a job, or buy an airline ticket
	* Or to request data 
		* such as perform a search on Google or find the current weather in their city.
* Response: The service sends a response to the client.
## One Way Pattern or Fire & Forget pattern

In the one-way pattern the client doesn’t even know whether therequest was received by the service.
It is colloquially called Fire & Forget pattern.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261006402.png)

## Request / Acknowledge pattern
The request/acknowledge pattern can be used to make subsequent requests, to check the status of the initial request or get a final response.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261007593.png)

## Half Async R/R Pattern (client)

The client makes a call to the served through a new thread and continues its task. 
- When the response arrives, the results are passed to the main thread.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261008719.png)

The same can be done by the service also i.e., Half Async (server)



## Half Async Implementation
Uses Multi Threading
 * Client Process **spawns a new thread object** that makes a synchronous call to server.
 * The Thread is made aware of the caller object and **a sink method** in the client process for it to invoke when the server responds.
 * Upon obtaining result from the server, the client process is updated the value by the thread through the delegated operation and thread is killed
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261010105.png)
## Full Async pattern
Full-Async , occurs when both client and server perform their work asynchronously;
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261010330.png)

## Publish / Subscribe Pattern
Message-based data systems starts with a producer publishing a message to broker; 
- the messages are often sent to a topic of logical grouping; 
Next,the message is sent to all the consumers subscribed to that topic.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261012191.png)
## Stream Pattern
The stream pattern flips things, and service becomes client.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261013039.png)

# Data Flow
## Producer & Consumer – A traditional Overview
Data gets created by several mechanisms
 * Manual Entry
 * Events
 * Devices etc.
The produced data is available for consumption.
Data gets consumed by several applications
 * Traditionally stored in databases
Consumers
 * The system or person querying the database
 * Or, the data store itself may be considered a consumer
## Case Example for Discussion
Consider an Internet Airline Ticket Booking Use case
 * You can use your experience of online airline ticket booking of any airline
Identify the data supplied by the passenger for booking the ticket.
 * EG: Flight Itinerary, Passenger Details, Meal/Seat choices, Payment details etc.
Identify the data received by the passenger to enable booking.
Identify the source and destination for data movement.
 * That is, for the data you identified, identify Where-From and Where-To does data flow in the booking process. (where-from/to can be any actors/applications)

A Quick 20 Minute Activity with Peers(In Class Discussion follows this)
## Concepts Discussed in Activity
The complexity of Integration
 * Data Transformations and Format variations
	 * Binary, Json, CSV etc.
 * Adapters
	 * Many to many adaptations
 * Protocols
	 * TCP, HTTP, REST
 * Schema Changes
 * Load on Source Systems
The role of the broker
 * Hub and Spoke pattern
 * Kafka is a Broker
## Integration Challenges in Real Time Systems
Multiple source ingestion
Streaming / real-time ingestion
Scalability
Parallel processing
Data Quality
Rapid Extraction
Cost Effective
User Friendly
## About Kafka
- Kafka is a Broker
* Distributed, Fault Tolerant, Resilient Architecture
* Horizontal Scalability
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261105463.png)
## Kafka Use Cases
Messaging Systems
Activity Tracking
Gather metrics from several locations
Application Log gathering
Stream Processing (using Kafka streams API)
De-coupling of System Dependencies
Integration with Spark, Flink, Storm, Hadoop and many other Big Data technologies
Micro services Pub/Sub
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261107469.png)



# Streaming Platform: Architecture & Deployment
## Architecture and Deployment 
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261109384.png)
# Implementation Platform Concepts
## Kafka Terms & Terminologies
Producer: Application that sends the messages. 
Consumer : Application that receives the messages. 
Message : Information that is sent from the producer to a consumer through Apache Kafka.
**Connection : A connection is a TCP connection between your application and the Kafka broker.**
Topic : A Topic is a category/feed name to which messages are stored and published.
Partition : Kafka topics are divided into a number of partitions, which allows you to split data across multiple brokers.
Replicas : A replica of a partition is a "backup" of a partition. Replicas never read or write data.
- They are used to prevent data loss.
Consumer Group: A consumer group includes the set of consumer processes that are subscribing to a specific topic.

Offset: The offset is a unique identifier of a record within a partition. It denotes the position of the consumer in the partition.


Node: A node is a single computer in the Apache Kafka cluster.
Cluster : A cluster is a group of nodes i.e., a group of computers.

## Producer
A producer is a component of an application that sends data to Kafka Queue
 * Typically this is aa Kafka component API used by the consumer application to send messages.
Examples of producers could be:
 * Sensor application sending temperature data at periodic information
 * A website sending user log in data
 * A stock trading information sending stock prices
## Consumer
A consumer is a component of an application that reads data from Kafka Queue
 * Typically a Kafka component API used by a client to read messages.
Examples of consumers can be:
 * A Meteorological application that publishes weather data on its website
 * A audit trail application that monitors possible intrusions in the form of unauthorised logins.
 * An analytics application that provides real time stock price trends
## Kafka Topics
Topics: A particular stream of data
 You cannot query topics, so we use producers and consumers. (Explained later)
 * A topic is identified by its **name** .
 * A topic has a particular type of data (**message**)
 * Similar to a table in a database (**without constraints**)
 * You can have as many topics as you want
 * The sequence of messages is called **data stream**
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261119912.png)

## Partitions in Topics
Topics are split in partitions
 * Messages in each partition is ordered
 * Each message within a partition gets an incremental id, called offset
Messages are immutable – i.e., cannot be updated.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261121543.png)

## Example: trucksGPS
- Consider that you have a feet of trucks, each reports its GPS Position
You can setup a topic called trucksGPS that contains the position of ALL trucks
- Each truck will send a message to Kafka for example every 20 seconds 
- each message contain the Truck ID and Truck Position (Latitude & Longitude)
We can create that topic with 10 partitions (arbitrary, as per your choice)
## Offsets in Topic Partitions - Details
Offset only have a meaning for a specific partition
 * That is: Offset 2 in Partition 0 does not have the same data as offset 2 in Partition 1
Data is kept only for a limited time (default is one week)
Data is immutable; i.e. once the data is written to a partition, it cannot be changed.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261127810.png)

## Brokers
A broker can be thought of as a **Server** that 
- hosts message queues and facilitates as node 
- where producers write messages and consumers read messages.

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261128257.png)

Typically there can be multiple topics in a Broker and multiple partitions within each topic
## Brokers in Kafka Cluster
A Kafka cluster is composed of multiple brokers (servers)
- Each broker is identified with its ID (integer only)
- Each broker contains certain topic partitions

After connecting to any broker (called a bootstrap broker), you will be connected to the - entire cluster
- A good choice is to start with 3 brokers, but some large clusters can be 100+ brokers.
- Broker numbers can start with an arbitrary number. Here we chose 101

## Topic Replication Factor
Topics should have a replication factor > 1 (usually 2 or 3)
This is for failover purposes; i.e., if a broker is down another broker serves data
Example: Topic-A with 2 partitions and replication factor of 2
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261132685.png)
## Concept of Leaders in Kafka
A “leader” in Kafka is typically a Master in traditional replication terminology
- An ‘In-Sync-Replica” in Kafka is typically the slave
- There can be one leader broker and several in-sync-replica brokers
At any time, ONLY ONE broker can be a leader for a given partiion ONLY THAT LEADER can receive and serve data for the partition
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261135074.png)
## Producers in Multi-broker context
Producers write data to topics (which is made up of several partitions)
Producers **automatically** know to which brokers & partition to write
In case of Broker failures Producers will automatically recover

## Producers and acknowledgement
Producer can choose to receive an acknowledgement
- acks = 0 : Producer will not wait for acknowledgement (possible data loss)
- acks = 1 : Producer will wait for Leader acknowledgement (limited data loss)
- acks = all : Producer will await Leader + replica acknowledgement (no data loss)

## Producers and Key
Producer can choose to send a key with the message (string, number, etc.)
- If key is null (not specified) data is sent round robin (broker 101 then 102, 103 …)
- If a key is sent, then all messages for that key will always go to the same partition
A key is basically sent if you need **message ordering for a specific field (eg: truckid)**
## Consumers reading from a partitions
Consumers read data from a topic (identified by a name)
Data is read in order within **each partition.**
A consumer can be read by **only one partition** 
 * This will ensure that a message is not re-read and processed in by two consumers.
 * This means there should be **as many number of consumers as there are partitions.**
	 * If you have more consumers than partition some consumers may be inactive
 * Two comsumers in same Comsuerm groups can not read same partition
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261150608.png)

## Consumer Groups
A particular message may need to be read by multiple applications
Each consumer within a group reads from exclusive partitions
 Example: 
 * The temperature data may need to be read by a meterelogical website as well as by an analytics system and also by a dashboard displaying data.
 * Since a message in partition can be read by only one partition to avoid duplicate processing Kafka introduced Consumer Groups.
 * This way each message is read by a Consumer from each Group from its allotted partition independently of other consumer Groups.
So a Consumer Group essentially represents an Application.


![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261155773.png)

## Consumer Offsets
Kafka stores the offsets at which a consumer group has been reading.
The offsets gets committed live in **a Kafka topic named: consumeroffsets**
- When a consumer group has processed data received from Kafka, it should be committing the offsets.
- If a consumer dies, it will be able to recommencing reading back from where it left off; thanks to the committed consumer offsets

## Delivery Semantics for consumers
Consumers choose when to commit offsets
There are three delivery semantics
 * At most once:
	 * Offsets are committed as soon as the message is received.
	 * If processing goes wrong, the message will be lost (i.e. it wont read again).
 * At least once:
	 * Offsets are committed after the message is processed (preferred option) .
	 * If the process goes wrong, the message will be read again.
	 * This can result in duplicate processing of message. 
		 * Hence client has to make sure that the processing is idempotent (i.e., reprocessing will not impact the application)
 * Exactly once:
	 * Can be achieved using Kafka -> Kafka workflows using Kafka streams API.
	 * For Kafka -> External System workflows, use an idempotent consumer.

## Kafka Broker Discovery
* Every Kafka broker is also called a bootstrap server.
	 - This means the you connect to one broker , and you will get connected to the entire cluster!
* Each broker knows about all brokers, topics and partitions,
	* That is, the brokers contain the full cluster metadata.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261202640.png)

## Zookeeper
Kafka Zookeeper is a distributed coordination service for distributed systems. It maintains configuration information, naming and helps distributed synchronisation by coordinating between the brokers in Kafka cluster.

* Zookeeper manages brokers
	* Keeps a list of brokers, their locations, their replicas, the partitions in them etc.
* Zookeeper helps in performing leader election for partitions.
* Zookeeper assigns leaders and monitors which brokers are down.
* Zookeeper sends notifications to Kafka in case of changes.
	* Example, new topic created, broker dies, broker comes up, topic deleted etc.

Kafka CANNOT work without Zookeeper
* Things due to change with v4.0 when KRaft will evolve.
* ZooZookeeper has a leader (handles writes) the rest of the servers are followers (handles reads).
* Zookeeper does NOT store consumer offsets (initial versions did).

## Kafka Guarantees
* Messages are appended to a topic-partition in the order they are sent
* Consumers read messages in the order stored in a topic-partition
* With a replication factor of N, producers and consumers can tolerate up to (N - 1) brokers being down
* This is why a replication factor of 3 is a good choice:
 * Allows for one broker to be taken down for maintenance
 * Protects against system crash when another broker fails
* As long as the number of partitions remains same for a topic (i.e. No new partitions get created) , the same key will go to the same partition

## Case example of Architecture & Deployment
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261208911.png)


## Overall View of Kafka


## Data Analysis - Stream Data Queries
## Data Analytics: Querying the data
Conventional Data Processing vs Stream Data Processing
Conventional Processing: Static Data
Real Time Processing: Streaming Data
## Traditional vs Stream Data Queries
One-time vs Continuous queries
 * Traditional Databases query issued once and result obtained.
 * Real Time system Query is Registered and periodically executed.
Notion of time
 * Traditional Databases do not have a notion of time. Data is written and remains until updated again.
 * Stream Databases the data represent sequence of data for same entity and queries are executed over time (eg: Average speed of cars in last 5 min)
Unbounded Data Sets
 * In traditional database the data rarely grow when the query is executed, and if it does then cursor stability handles
 * In streaming data the data is continuously growing and query should be executed on unbounded datasets
Unreliable Data
 * Traditional Database work with generally reliable data
 * Streaming Applications must be capable of working with unreliable data. This may be due to dropped messages or delays or data coming out of order.
Reactive Capability
 * Traditional databases are passive. Generally a human initiates the query and receives the results.
 * Streaming Applications (though may have human interactions) generally receive data from sensors and require (or have) self monitoring nature. Example if the traffic in a road is high, the system may direct drivers to alternate routes which is the reactive capability of Streaming Systems.
## Stream Data Models & Query Languages
Timestamp
 * Implicit Timestamp
	 * Is assigned to an arriving tuple by the stream management system.
	 * This may simply be a sequence number that imposes a total order on the tuples in the stream.
 * Explicit Timestamp
	 * It is a property of the tuple itself
	 * Typically the data is sent with a timestamp attribute which has real-world meaning
Windows
 * For unbounded data streams the queries are executed over subset or part of the data.
 * This requires specification of a window that could be the number of tuples or a time window (based on timestamp).



## Query Comparison: Data Store vs Streams

Data Store
 * Assume data has been persisted in a table and we wish to get the average speed of the car on a particular date (for an expressway segment+direction)
 ![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261459134.png)

Streams
 * Assume we are processing a real time stream and are evaluating current traffic speed (for an expressway segment+direction); where current = 5 minute window
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261459153.png)

## Summary

* Real Time System Characteristics & Challenges
* Collection Tier:
 * Interaction Patterns & Messaging
* Message Queue Tier:
 * Role of Publish Subscribe model
 * Kafka Architecture
* Analysis Tier:
 * Queries on Stream
 * Concept of Timestamp & Windows



## References & Acknowledgement

References:

Streaming Data – Understanding the Real-Time Pipeline by Andrew G

Psaltis, Manning Publication, 2017

Stream Data Management Edited By Nuaman Chaudhry, Kevin Shaw and Mahdi Abdelguerfi, Springer Publications, 2005.

Apache Kafka Series - Learn Apache Kafka for Beginners by Stéphane

Maarek, Packt Publishing.

Understanding Tool Integration for Big Data Architecture by Patrick McFadin, O'Reilly Media, Inc.

Acknowledgement

The figures and diagrams in this book has been adapted / customised from the above sources.



## Master of Technology

Architecting Real Time Systems

Module is a part of SWE5003 - Specialist Cert - Engineering Big Data

Real Time Processing and Technologies

Suria R Asai

Institute of Systems Science, National University of Singapore

© 2019-24 NUS. The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied.

Total: 69 Slides

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Learning Objectives

* Design real time data ingestion tier with protection against data loss.
 * Understand and implement the data collection patterns as necessary.
 * Identify and analyse the non-functional requirements of a data ingestion component
 * Design low -latency, high- throughput, and fault-tolerant messaging system.
* Understand technologies and tools used for stream ingestion.
* Implement an effective real time streaming application using processing framework.
* Understand technologies and tools used for stream processing.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Agenda

Introduction to Real Time Use Cases

Design Real Time Data Ingestion

Stream Ingestion Technologies and Tools

Design Real Time Data Processing

Stream Processing Technologies and Tools

Summary

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Introduction to Real Time Use Cases

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Categorizing Based on Latency and Size

In Memory Computing (Spark, Hana and VoltDB)

100K events (100MBs)

-------------Size of Data Handled Per Second ------------------>

Interactive Processing (Spark QL, Drill, Bi g Q uer y , O L A P )

Map Reduce (Spark Core and Hadoop)

1K e v ents (1MBs)

Indexed Storage ( RDBM S , S QL )

Real Time Analytics (Complex Event Processing- CEP, Stream Processing)

100 events

(10 KBs)

seconds	minutes	hours

---------------------- Time to act ------------------------->

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Hybrid Synchronous/Asynchronous Data Pipeline

* Consider a data pipeline as
* a software-driven automobile or
* manufacturing assembly line.
 * In terms of synchronous processing, each station along the assembly line does one job really well.
 * But success of each job relies on the upstream job to accomplish its predefined goals.
* In practical situations, with complex data pipeline, some jobs need to be run synchronously, following the assembly line style of processing, and others can be run fully asynchronously.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time System Categorization- Criticality

Main factor distinguishes real-time systems from batch and online applications: timeliness . Unfortunately, this is a rather limited definition; a more precise one is needed.

One particular pragmatic scheme, based on time and criticality

| Time <br />Criticality  | Slow | Fast |
| :-: | :-: | :-: |
| Soft | Machine Monitoring Services | Human Machine Interfacing Systems |
| Hard | Missile Defence Systems | Airbag Control Systems |

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time System Categorization vs Attributes

* Hard, fast embedded systems tend, in computing terms, to be small (or maybe a small, localized part of a larger system).
 * Computation times are short (typically, in the tens of milliseconds or faster), and deadlines are critical.
 * Software complexity is usually low, especially in safety-critical work.
 * A good example is the airbag deployment system in motor vehicles.

| Attribute <br />Category  | Execution<br />Time | Deadlines | Software Size | Software<br />Complexity |
| :-: | :-: | :-: | :-: | :-: |
| Hard-Fast | **** | **** | * | * |
| Hard-Slow | * | **** | ***** | ***** |
| Soft-Fast | **** | ** | ***** | ***** |
| Soft-Slow | * | ** | ***** | ***** |

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Feedback Structure of Real-Time Systems

| | | | | | | | | |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| | | | | | | | | |
| | Actuator<br />Processing | | | <br />Computation | | Sensor<br />Processing | | |
| | | | | | | | | |

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Event Types

Stimulus events are generated by the environment and act on the system. These events can be produced asynchronously (i.e., aperiodically). For example, a user pressing a button on a phone set generates a stimulus event to act on the system.

Response events act on the environment and are usually produced by the system in response to some stimulus events. For example, consider a chemical plant where as soon as the temperature exceeds 100 º C, the system responds by switching off the heater.

A delay constraint captures the minimum time between events e1 and e2.

A deadline constraint captures the permissible maximum separation between any two arbitrary events e1 and e2.

A duration constraint on an event specifies the time period over which the event acts.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time Streaming Platform

Stream

processing

Real Time Stream P r oce s s i ng

Consumers/

P r e s e n t a tion

Real Time Stream Vi suali z a t i on

R e al Time Stream Sources

R e al Time Stream Sharing

R e al Time Ingestion

D3.JS

Kibana Grafana Google Charts Tableau Public JChart

Cloud Data Studio Amazon Quicksight Azure PowerBI React JS DataWrapper Dygraphs

Flot

Leaflet OpenHeatMaps

Apache HDFS

Apache HBase Cassandra

Apache MiNiFi

Apache NiFi

Apache Kafka

MQTT Broker Google Pub Sub AWS Kinesis Events Hub

Druid

Big Table/Query

AWS S3 Dynamo

Azure Cosmos Elastic Search

Google Data Flow

Social Media Stream

Kinesis Analytics

Azure Streams Apache Flink Apache Storm Apache Beam

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Disciplines that affect real-time systems

Processing Fr a m e w ork

Streaming Application Examples:

Data Science A lg ori t hms

Pr o g r a mming Languages

Someone you are following on Twitter posts a tweet, and moments later you see the tweet in your Twitter client.

You are tracking flights around Changi using the real-time Live Flight Tracking service from FlightAware [(](https://flightaware.com/live/airport/WSSS) [h](https://flightaware.com/live/airport/WSSS) [t](https://flightaware.com/live/airport/WSSS) [t](https://flightaware.com/live/airport/WSSS) [p](https://flightaware.com/live/airport/WSSS) [s://](https://flightaware.com/live/airport/WSSS) [f](https://flightaware.com/live/airport/WSSS) [li](https://flightaware.com/live/airport/WSSS) [gh](https://flightaware.com/live/airport/WSSS) [t](https://flightaware.com/live/airport/WSSS) [a](https://flightaware.com/live/airport/WSSS) [w](https://flightaware.com/live/airport/WSSS) [a](https://flightaware.com/live/airport/WSSS) [r](https://flightaware.com/live/airport/WSSS) [e.](https://flightaware.com/live/airport/WSSS) [c](https://flightaware.com/live/airport/WSSS) [om/l](https://flightaware.com/live/airport/WSSS) [iv](https://flightaware.com/live/airport/WSSS) [e](https://flightaware.com/live/airport/WSSS) [/](https://flightaware.com/live/airport/WSSS) [ai](https://flightaware.com/live/airport/WSSS) [r](https://flightaware.com/live/airport/WSSS) [p ](https://flightaware.com/live/airport/WSSS) [ ](https://flightaware.com/live/airport/WSSS) [ort/WSSS](https://flightaware.com/live/airport/WSSS) [ ](https://flightaware.com/live/airport/WSSS)[).](https://flightaware.com/live/airport/WSSS)

You are using the NASDAQ Real Time Quotes application ( [www.nasdaq.com/](http://www.nasdaq.com/) [ ](http://www.nasdaq.com/)quotes/real- time) to track your favorite stocks.

Data St ruc t u res

O pe ra ting Systems

Real time s y s t ems

O p e r a ti o n s Research

Software Engin e ering

Q u e u e ing Theory

Control Syst e ms

Computer

Architecture

Solution

Engineering

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Smartness Everywhere with Streams

Smart	 Smart Transportation City

Smart H o me

Streaming Services

Smart Logistics

Optimization

Smart H o s p i t a l s

Smart Energy Mana g e m e n t

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real-Time Analytics Case Studies	- 1

* Smart Cities
 * Example: CCTV cameras help to solve crimes in the city
 * Example: Smart waste management projects
 * Goal is to mine insights that provide better infrastructure and sustainability to a city’s future outlook. Different types of electronic data collection sensors to supply information which is used to manage assets and resources efficiently.
 * Data collected from citizens , devices , and assets that is processed and analyzed to monitor and manage traffic and transportation systems, power plants, water supply networks, waste management, law enforcement, information systems, schools, libraries, hospitals, and other community services.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Smart Homes
 * Example: Mesh WiFi Network: Improve Connectivity and
* Keep Smart Devices Secure. ... Google Mesh Home
 * Example: A Great Home Hub as a Foundation for Your Connected Home. ... Samsung Cloud Connect
 * Example: Smart Speaker with Integrated Hub. ... Amazon Alexa
 * Example: Smart Plug with No Hub Required to Get Started. ...
 * Example:	Smart Bulbs: Smart Lighting the Easy Way. ...
 * Example: Smart Thermostat for Automated Climate Control.
 * Water Saving, Home Security, Voice and Video Streaming Facilities, Waste Management and many many more.
 * Data collected from people , devices , and assets that is processed and analyzed for better family lifestyle.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Smart Hospitals
 * Example: Shift from disease treatment to health management.
 * Example: Care delivery is transforming to improve the quality of care.
 * Example: AI, robotics, and other new technologies can improve treatment precision and dramatically decrease the probability of error.
 * Example: Hospitals are becoming just one component of larger, interdependent ecosystems that include multiple other facilities (e.g., primary care providers, clinics, pharmacies, rehabilitation centers).
 * Example: New technologies to move toward outpatient care, since they make it possible to establish strong integration among the various entities, which improves quality of care..
 * Optimized and automated processes built on environment of interconnected computing assets, particularly based on Internet of things (IoT), to improve existing patient care procedures and introduce new capabilities.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Smart Energy Initiative
 * Example: Shift from traditional grids to “smart grids”. Smart grids are designed to react to changes in power usage by consumers, often incorporate artificial intelligence features.
 * Example: A smart meter, which records electricity consumption, is used by power producers for monitoring and billing purposes.
 * Example: Smart solutions for two-way communication channels, utility
* meters and accurate meter reading techniques to boost productivity.
 * Example: Sun, wind and data centers. While utilities flirt with emerging technologies, tech firms are also looking for increased access to renewable power sources for their data centers.
 * Optimized and automated processes for real time systems in which energy production (often, renewable energy production) and infrastructure are integrated using various technologies and digital interfaces.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Smart Logistics and Transportation
 * Example: Traffic and Accident management
 * Example: Delivery Drones, First Responders, Accurate Navigation Devices
 * Example: Satellite communication services used to ensure the transmission of data is both continuous and reliable. This includes remote locations and times/places where terrestrial communications are absent or unreliable.
 * Example: Satellite technology to enable the regular maintenance and upkeep of public transport. For example, if a carriage on a train is overused or requires maintenance, the vehicle can notify the transfer hub of this before it creates a problem.
 * Optimized and automated processes built on environment of interconnected computing assets, particularly based on Internet of things (IoT), to improve existing logistical and transportation capabilities.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## More Examples . . .

Algorithmic Trading, Stock Market Surveillance,

Smart Patient Care

Monitoring a production line

Supply chain optimizations

Intrusion, Surveillance and Fraud Detection

Smart Car

Geofencing, Vehicle, and Wildlife tracking

Sports analytics

Context-aware promotions and advertising

Computer system and network monitoring

Predictive Maintenance

Geospatial data processing

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Design Real Time Ingestion

Types o f data so u r c es

Ingestion practices and policies for Real Time Streaming Systems

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming data architecture - Focus

Browser, IoT Device, Machine, IoE etc

Browser, IoT Device, Machine, IoE etc

Collection (In g e s t i o n ) Tier

Message Qu e u e ing Tier

In Memory Data

Store

Fu r th e r D a t a

Access Tier

Long Term Storage

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Stream Engineering Life Cycle

Machine Learning

Data Management

Data Architecture

Software Engineering

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion and Pipelines

Data moves from source systems into storage, with ingestion as an intermediate step. Data ingestion implies data movement and is a key component of the data engineering lifecycle.

Stream Processing

Stream Processing

Analytical Database

Streaming Platform

Dashboard Alerts

Archive Database

A data pipeline is the combination of architecture, systems, and processes that move

data through the stages of the data engineering lifecycle.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Designing ingestion architecture:

* Bounded versus unbounded
 * Unbounded data happens either sporadically or continuous, ongoing and flowing.
 * Bounded data is a convenient way of bucketing data across some sort of boundary, such
* as time.
* Frequency - Ingestion processes can be a batch, micro-batch, or real-time.
* Synchronous versus asynchronous - series of dependent steps (synchronous systems), or operate without any dependencies (asynchronous systems).
* Serialization and deserialization
* Throughput and elastic scalability
* Reliability and durability
* Payload
* Push versus pull patterns

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time Decisions

How does it effectively learn from data, and dissociate signal from noise?

How can it integrate expert knowledge with observed patterns?

How can it understand the context (Where, When, Who, Where) and act accordingly?

How can it comprehend the consequences of and interference between different actions?

How does it plan for causality that are not instantaneous , but

take place over time, across control iterations, and can fail?

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Every Day Data Bombardment in Organizations

HTTP Proxy Logs

Email and Email Meta Data

VPN Logs

Firewall Events

DNS

Syslogs (*nix and Windows)

Security Endpoints

Threat Intelligence

IDS Events

* Wireless Access Points
* Mobile Device Management
* Public Data Feed
* Event Streams
* Events per second
 * Peak Time
* Data Arrival Per Day
 * Diversity
 * Volume
 * Fast Moving

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time Stream Batch Systems

Example - How we use Kappa Architecture for Streaming Systems

Stream Data Platform

Security & Fraud Detection

Real Time

time series Analytics

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Hybrid – Example Anomaly Detection

Detect Anomalies

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real-time ETL tools

* Data streams from one or more message brokers need to be aggregated, transformed and structured before data can be analyzed with SQL-based analytics tools.
 * This would be done by an ETL tool or platform receives queries from users, fetches events from message queues and applies the query, to generate a result - often performing additional joins, transformations on aggregations on the data.
 * The result may be an API call, an action, a visualization, an alert, or in some cases a new data stream.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Protocols Used for IoT platform

* MQTT – MQTT protocol uses a publish/subscribe architecture.
 * The central communication point of this protocol is MQTT broker. Every client includes a topic name while publishing data to the broker. Topics are responsible for routing information for the broker. Each client that wants to receive messages subscribes to a particular topic and the broker delivers all messages with the matching topic to the client.
* COAP – COAP Protocol (Constrained Application Protocol).
 * Web-based protocol designed to connect with lightweight devices to the Internet of things(IoT). Like HTTP protocols, COAP is also used Request-Response model. It also allows to make API calls GET, PUT, POST, DELETE data via URL.
* AMQP – AMQP (Advanced Messaging Queuing Protocol)
 * Ppen standard for passing messages in between applications and organizations. It connects system, provides business processes with the information they need.
* HTTP – Hyper Text Transfer Protocol
 * Standard protocol for web services used in IoT analytics solutions. The most popular architectural style called RESTFul is widely used on mobile and web application and must be considered on IoT Solutions.
* DDS – Data Distribution Service
 * Standard for real-time IoT analytics, scalable and high-performance machine-to-machine communication. DDS can be deployed in both low footprint devices and on the cloud as well.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Spectrum of devices working holistically

Cloud / Fog Computing

IoT Edge Compute

Real Time Stream

Edge level Machine Learning

Edge level Analytics

Cloud Micro Services

Data Intensive Tasks

Kappa - Message Pipeline

Big Data Store & NoSQL

Machine Learning

Firmware or Mobile OS or Raspberry

Embedded or General Purpose CPU or GPU

Insight Engineering

Constrained Resource Management

Lambda - Functions

Millions of Edge Devices

Thousands of Fog Computing

Hundreds of Cloud Computing

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Types of Time

* Event time
 * Event time is the time at which an event is generated in a source system, including the timestamp of the original event itself.
* Ingestion time
 * After data is created, it is ingested somewhere. Ingestion time is the time at which an event is ingested from source systems, into a message queue, cache, memory, object storage, a database, or any place else that data is stored.
* Processing time
 * Processing time is the time at which data is processed, which is typically some sort of transformation.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Stream Ingestion Technologies and Tools

Selective Discussion

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming Ingestion Technologies

* Classification of stream ingestion frameworks that collect and buffer messages, based on offered features.
 * Horizontal scaling enhances data scale, reliability, and durability.
 * Kafka and Kinesis partition (shard) streams to support horizontal scaling.
 * Kafka and Kinesis only support pull subscriptions over push.
 * Operational overhead
 * Google Cloud Pub/Sub is a fully managed service.
 * Apache Kafka can be either raw or fully managed serverless offering.
 * Autoscaling (Multisite and multiregional)
 * Replay Abilities
 * Message Size
 * Fanout
 * Delivery guarantee, ordering and processing abilities

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time (Stream) Ingestion Systems

* Open Source Projects
 * [Apache](https://kafka.apache.org/) [ ](https://kafka.apache.org/) [Kafka](https://kafka.apache.org/)
 * [Apache](https://nifi.apache.org/) [ ](https://nifi.apache.org/) [Nifi](https://nifi.apache.org/)
 * [Apache](http://samza.apache.org/) [ ](http://samza.apache.org/) [Samza](http://samza.apache.org/)
 * [Apache](https://flume.apache.org/) [ ](https://flume.apache.org/) [Flume](https://flume.apache.org/)
 * [Apache](https://gobblin.apache.org/) [ ](https://gobblin.apache.org/) [Gobblin](https://gobblin.apache.org/)
 * [Apache](https://sqoop.apache.org/) [ ](https://sqoop.apache.org/) [Sqoop](https://sqoop.apache.org/)
 * [Fluentd](https://www.fluentd.org/)
 * [Apache](https://storm.apache.org/) [ ](https://storm.apache.org/) [Storm](https://storm.apache.org/)

* Commercial Projects
 * [Striim](https://www.striim.com/)
 * [Wavefront](https://docs.wavefront.com/wavefrontintroduction.html)
 * [Precisely](https://www.precisely.com/product/precisely-connect/connect) [ ](https://www.precisely.com/product/precisely-connect/connect) [Connect](https://www.precisely.com/product/precisely-connect/connect)
 * [Amazon](https://aws.amazon.com/kinesis/) [ ](https://aws.amazon.com/kinesis/) [Kinesis](https://aws.amazon.com/kinesis/)
 * [Google](https://cloud.google.com/pubsub) [ ](https://cloud.google.com/pubsub) [PubSub](https://cloud.google.com/pubsub)
 * [Azure](https://azure.microsoft.com/en-us/services/stream-analytics/) [ ](https://azure.microsoft.com/en-us/services/stream-analytics/) [Stream](https://azure.microsoft.com/en-us/services/stream-analytics/) [ ](https://azure.microsoft.com/en-us/services/stream-analytics/) [Analytics](https://azure.microsoft.com/en-us/services/stream-analytics/)

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Open Source : Apache Kafka

* Apache Kafka is an open-source message broker project to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.
 * Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.
 * Store streams of records in a fault-tolerant durable way.
 * Kafka is a distributed , partitioned , replicated commit log service.
 * Kafka has a modern cluster-centric design that offers strong
* durability
 * Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization. It can be scaled without downtime.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Open Source : Apache NIFI

* Apache NIFI supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic .
 * Nifi includes a web-based user interface
 * Nifi provides seamless experience between design, control, feedback, and monitoring,
 * Nifi manages data provenance, SSL, SSH, HTTPS, encrypted content,
* pluggable role-based authentication/authorization.
 * Nifi is highly configurable with loss tolerant vs guaranteed delivery, low latency vs high throughput, and dynamic prioritization
 * Nifi flow can be modified at runtime , and it uses back pressure.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Open Source : Apache Samza

* Apache Samza is a distributed stream processing
* framework.
 * Samza uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management.
 * Samza provides a very simple callback-based “ process message ” API
* comparable to MapReduce.
 * Samza manages snapshotting and restoration of a stream processor’s state. When the processor is restarted, Samza restores its state to a consistent snapshot.
 * Samza is built to handle large amounts of state (many gigabytes per partition). Whenever a machine in the cluster fails, Samza works with YARN to transparently migrate our tasks to another machine.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Open Source : Apache Flume

* Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.
 * Flume has a simple and flexible architecture based on streaming
* data flows.
 * Flume is robust and fault tolerant with tunable reliability
* mechanisms and many failover and recovery mechanisms.
 * Features include multiple sources , in-memory channel, disk channels , sinks to HDFS and HBase using Kite API, support for elastic search using HTTP API
 * Flume uses a simple extensible data model that allows for online
* analytic application.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Open Source : Apache Sqoop

* Apache Sqoop supports incremental loads of a single table or a free form SQL query, saved jobs which can be run multiple times to import updates made to a database since the last import.
 * Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.
 * Sqoop imports can also be used to populate tables in Hive or HBase.
 * Sqoop exports can be used to put data from Hadoop into a relational database.
 * Sqoop got the name from sql+hadoop

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Apache Storm has many use cases: real-time analytics , online machine learning, continuous computation, distributed RPC, ETL, and more.
 * Storm is a distributed real-time computation system .
 * Storm makes it easy to reliably process unbounded streams of data , doing for real-time processing
 * Storm has many use cases: real-time analytics, online machine learning, continuous computation, distributed RPC, ETL, and more.
 * Storm is fast : a benchmark clocked it at over a million tuples processed per second per node.
 * Storm is scalable , fault-tolerant , guarantees our data will be processed, and
* is easy to set up and operate.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Ingestion Tools : Others

[Kinesis](https://aws.amazon.com/kinesis/) [ ](https://aws.amazon.com/kinesis/)by AWS

[DataTorrent](https://dt-docs.readthedocs.io/en/latest/#welcome-to-datatorrent-rts)

[Databus](https://github.com/linkedin/databus/wiki)

[Morphlines](https://blog.cloudera.com/introducing-morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-hadoop/) [ ](https://blog.cloudera.com/introducing-morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-hadoop/)by Cloudera

[Heka](https://hekad.readthedocs.io/en/v0.10.0/) [ ](https://hekad.readthedocs.io/en/v0.10.0/)by Mozilla

[White](https://engineering.linkedin.com/teams/data) [ ](https://engineering.linkedin.com/teams/data) [Elephant](https://engineering.linkedin.com/teams/data) [ ](https://engineering.linkedin.com/teams/data) by Linked In And Many More . . . .

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Design for Real Time Processing

F ou n dati o n s

Types Use Ca s e s

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## What happens when someone views a job posting on LinkedIn?

Monitoring systems - Companies pay LinkedIn to post their job openings, so monitoring tracks views etc.

Relevance and recommendations- It’s annoying for users to see the same thing over and over again, so feed has a scoring process that keeps track of views and recommendations.

Preventing abuse - LinkedIn doesn’t want people to be able to scrape all the jobs, submit spam, or otherwise violate the terms of service.

Job poster analytics - The companies who post their job openings want to see stats. Import into Hadoop and Data Warehouse - For LinkedIn’s internal business analytics.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming Applications

“We would like to create a system that keeps track of all available parking spaces, identifies when a car parks, and knows how long the car remains in a given spot. This process should be automated as much as possible.”

* Streaming data is not stationary . Streaming encapsulates
* the now—it records events and actions as they occur in flight .
* Time Series and Event Streams - Moving from a stationary
* data mindset to one that interprets data as it flows over time, in terms of streams of unbounded data across many views and moments in time
* More Examples:
 * Click-stream data of websites
 * Application logs
 * Data coming over a TCP port

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Use Case: Tracking Customer Satisfaction

What if I told you two customers came into the coffee shop, ordered drinks, and left the store with their drinks. . . . .

Customer orders:

{customer.order:initialized}

Payment made:

{customer.order:payment:processed}

Order queued:

{customer.order:queued}

Order fulfilled:

{customer.order:fulfilled}

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Foundations of Stream Processing

Building Reliable Streaming Data Systems

Managing Streaming Data Accountability

Dealing with Data Problems in Flight

Data Gatekeepers

Selecting the Right Data Protocol for the Job

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Buzzwords related to event-stream processing

Batch Processing

Continuous Processing

Stream Analytics

Event-driven Applications

Transactional Applications

CEP - Complex Event Processing

Stream

Processing

SEDA – Staged Event Driven Architecture

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming isn’t always a straight line

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Several Possibilities

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Types of Stream Processing Engines

* Open Source Compositional Engines: In a compositional stream processing engines, developers define the Directed Acyclic Graph (DAG) in advance and then process the data. This may simplify code, but also means developers need to plan their architecture carefully to avoid inefficient processing.
 * Challenges: Compositional stream processing are considered the “first generation” of stream processing and can be
* complex and difficult to manage.
 * Examples: Compositional engines include Samza, Apex and Apache Storm.
* Managed Declarative Engines: Developers use declarative engines to chain stream processing functions. The engine calculates the DAG as it ingests the data. Developers can specify the DAG explicitly in their code, and the engine optimizes it on the fly.
 * Challenges: While declarative engines are easier to manage, and have readily-available managed service options, they still require major investments in data engineering to set up the data pipeline, from source to eventual storage and analysis.
 * Examples: Declarative engines include Apache Spark and Flink, both of which are provided as a managed offering.
* Fully Managed Self-Service Engines: A new category of stream processing engines is emerging, which not only manages the DAG but offers an end-to-end solution including ingestion of streaming data into storage infrastructure, organizing the data and facilitating streaming analytics.
 * Examples: Upsolver is a fully managed stream processing engine which handles huge volumes of streaming data, stores it in a high-performance cloud data lake architecture, and enables real-time access to data and SQL-based analytics.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## The processing flow of Structured Streaming

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Example - Processing Stream of Events

Reference Book: Learning Spark, 2nd Edition By Jules Damji, Denny Lee, Brooke Wenig and Tathagata Das

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Popular Streaming Platforms

[Spotify](https://medium.com/%40saranyar645/spotify-tech-stack-72c34dba1d3e) [ ](https://medium.com/%40saranyar645/spotify-tech-stack-72c34dba1d3e)uses Kafka, Java and Python Services hosted in AWS (S3, CloudFront, Storm and Cassandra). Analytics is done using Google cloud services such as Bigtable and BigQuery.

[Uber](https://www.uber.com/en-SG/blog/kappa-architecture-data-stream-processing) [ ](https://www.uber.com/en-SG/blog/kappa-architecture-data-stream-processing)uses Apache Kafka, Apache Spark Stream and Apache Plink for a scalable backfilling strategy using stateful stream pipelines.

[Netflix](https://www.infoq.com/articles/netflix-migrating-stream-processing/) [ ](https://www.infoq.com/articles/netflix-migrating-stream-processing/)moved its movie recommendation from batch ETL to near real time streaming pipelines.

[Pinterest](https://medium.com/pinterest-engineering/how-pinterest-fights-misinformation-hate-speech-and-self-harm-content-with-machine-learning-1806b73b40ef) [ ](https://medium.com/pinterest-engineering/how-pinterest-fights-misinformation-hate-speech-and-self-harm-content-with-machine-learning-1806b73b40ef)fights misinformation, hate speech, and self-harm content with streams and machine learning.

[Airbnb](https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d) [ ](https://medium.com/airbnb-engineering/scaling-spark-streaming-for-logging-event-ingestion-4a03141d135d)scales Spark streaming manages massive amount of

logging events from Kafka in near real-time.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Key use cases for stream processing

Realtime fraud detection & payments

IoT sensor data

Realtime dashboards, e.g., medical BI dashboard

Log, traffic, and network

monitoring

Context-aware online advertising & user behaviour tracking

Geofencing and vehicle

tracking

Cybersecurity

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Challenges of Stream Processing

* Maintaining a potentially large state in a reliable manner for data streaming applications
 * Efficiently and quickly delivering messages for applications to process
 * Dealing with streaming data that arrives out of order
 * Joining with batch data to enrich the incoming streaming data
 * End-to-end, exactly once guarantee delivery of data even where there is failure
 * Dealing with an uneven data arrival rate
* Complex low-level APIs at the level of physical operators, complex state management options and relaxing exactly-once guarantee.
* Integration in end-to-end applications, enabling interactive queries, reasoning about consistency across two systems, and avoid rewriting in separate systems by using a unified API.
* Operational challenges on fault recovery, state management and scheduling in case of failure / code updates / rescaling / stragglers / monitoring / cost.
* Cost and performance on optimizing throughput for distributed streaming system while supporting latency-sensitive use case.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Stream Processing Technologies and Tools

Selective Discussion

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Real Time (Stream) Processing Systems

[Apache](https://spark.apache.org/)[ ](https://spark.apache.org/)[Spark](https://spark.apache.org/)

[Apache](http://storm.apache.org/)[ Storm](http://storm.apache.org/)

[Apache](http://samza.apache.org/)[ ](http://samza.apache.org/)[Samza](http://samza.apache.org/)

[Apache](http://flink.apache.org/)[ ](http://flink.apache.org/)[Flink](http://flink.apache.org/)

[Amazon](http://aws.amazon.com/kinesis/data-streams/)[ ](http://aws.amazon.com/kinesis/data-streams/)[Kinesis](http://aws.amazon.com/kinesis/data-streams/)[ ](http://aws.amazon.com/kinesis/data-streams/)[Streams](http://aws.amazon.com/kinesis/data-streams/)

[Apache](http://apex.apache.org/)[ ](http://apex.apache.org/)[Apex](http://apex.apache.org/)

[Apache](http://flume.apache.org/)[ ](http://flume.apache.org/)[Flume](http://flume.apache.org/)

[Spring](https://spring.io/projects/spring-cloud-dataflow)[ ](https://spring.io/projects/spring-cloud-dataflow)[Cloud](https://spring.io/projects/spring-cloud-dataflow)[ ](https://spring.io/projects/spring-cloud-dataflow)[Data ](https://spring.io/projects/spring-cloud-dataflow)[Flow](https://spring.io/projects/spring-cloud-dataflow)

[Google](https://cloud.google.com/dataflow)[ ](https://cloud.google.com/dataflow)[Cloud ](https://cloud.google.com/dataflow)[Dataflow](https://cloud.google.com/dataflow)

[Apache](https://pulsar.apache.org/)[ ](https://pulsar.apache.org/)[Pulsar](https://pulsar.apache.org/)

[IBM](https://www.ibm.com/cloud/streaming-analytics)[ ](https://www.ibm.com/cloud/streaming-analytics)[Streams](https://www.ibm.com/cloud/streaming-analytics)

[Apache](https://heron.apache.org/)[ ](https://heron.apache.org/)[Heron](https://heron.apache.org/)

[Apache](https://kafka.apache.org/documentation/streams/)[ ](https://kafka.apache.org/documentation/streams/)[Kafka ](https://kafka.apache.org/documentation/streams/)[Streams](https://kafka.apache.org/documentation/streams/)

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Apache Spark Streaming

* Spark is an open-source distributed general-purpose cluster
* computing framework. Spark’s in-memory data processing engine conducts analytics, ETL, machine learning and graph processing on data in motion or at rest. It offers high-level APIs for the programming languages: Python, Java, Scala, R, and SQL.
* Pros:
 * Apache Spark is a mature product with a large community, proven in
* production for many use cases, and readily supports SQL querying.
* Cons:
 * Spark can be complex to set up and implement
 * It is not a true streaming engine (it performs very fast batch processing)
 * Latency of a few seconds, which eliminates some real-time analytics use
* cases

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Amazon Kinesis Streams

* Amazon Kinesis Streams is a durable and scalable real time service. It can collect gigabytes of data per seconds from hundreds of thousands of sources, including database event streams, website clickstreams, financial transactions, IT logs, social media feeds, and location-tracking events.
* Pros:
 * A robust managed service that is easy to set up and maintain
 * Integrates with Amazon’s extensive big data toolset
* Cons:
 * Commercial cloud service, priced per hour per shard

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Apache APex

* Apex offers a platform for batch and stream processing using Hadoop’s data-in-motion architecture by YARN. The platform provides integration with different data platforms.
* Pros:
 * Design focuses on enterprise readiness
 * Strong processing guarantees (end-to-end exactly once)
 * Highly scalable, high throughput with low latency
 * Secure, supports fault-tolerance and multi-tenancy
* Cons:
 * Apex is no longer widely used and no vendor is currently supporting this framework at scale
 * Limited support for SQL
 * Difficult to find skilled users

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Spring Cloud Data Flow

* Spring Cloud Data Flow is a microservice-based streaming and batch processing platform. It provides developers with the unique tools needed to create data pipelines for common use cases. Used to ingest data or for ETL import/export, event streaming, and predictive analysis.
* Pros
 * Developers can deploy using DSL, Shell, REST-APIs, and Admin-UI.
 * Allows you to scale stream and batch pipelines without interrupting data flows
 * Great integrations with platforms like Kafka and ElasticSearch
* Cons
 * Visual user interface could use some improvements
 * Features like the monitoring tools need more development

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Google Cloud Dataflow

* Cloud Dataflow is a Google-powered processing platform designed to execute data processing pipelines. Helps to develop simple streaming data pipelines with lower data latency. Google Cloud Dataflow has a serverless approach that shifts developers' focus to programming instead of managing countless server clusters.
* Pros
 * It’s fully managed and removes operational complexities
 * Minimize pipeline latency
 * Provides access native integrations with AI Platform, BigQuery
 * Unified stream and data processing analysis
 * Real-time AI-powered processing patterns
* Cons
 * Restricted to only Cloud Datastore service
 * BigQuery/DataFlow in streaming mode can be expensive
 * Google Content Delivery Network doesn’t work with custom sources
 * It’s not suited for experimental data processing jobs

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Apache Pulsar

* Apache Pulsar is a cloud-native, distributed messaging and streaming platform. Originally deployed inside Yahoo, Pulsar serves as the consolidated messaging platform connecting Yahoo Finance, Yahoo Mail, and Flick to data. Pulsar provides a high-performance solution for server-to- server messaging and geo-replication of messages across clusters.
* Pros
 * Easy to Integrate with existing applications
 * Low publish latency with strong durability guarantees
 * Supports high-level APIs for Java, Go, Python, C++, and C#.
 * Built-in geo-aware replication that allows replicating data across data centers in different geographical locations.
 * Full end-to-end encryption from the client to the storage nodes
* Cons
 * It has a small community and forums might not be helpful
 * Requires higher operational complexity
 * It does not allow consumers to acknowledge message from a different thread

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## IBM Streams

* IBM Streams enables you to build real-time analytical applications using Streams Processing Language (SPL) or Python. It’s developer-friendly and enables us to deploy applications that can run in the IBM Cloud. Streams powers a Stream Analytics service that allows us to ingest and analyze millions of events per second.
* Pros
 * End-to-end processing with sub-millisecond latency.
 * A comprehensive set of toolkits
 * Allows you to link to IDE for collaboration with other applications
 * Allows you to use Stream runtime using Java and Python
 * Visual-driven interface
* Cons
 * Creating reports can be overwhelming
 * Steep learning curve

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Apache Heron

* Apache Heron is a distributed stream processing engine developed at Twitter. According to the creators at Twitter, the scale and diversity of Twitter data has increased, and Heron is a real-time analytics platform to process streaming.
* Pros
 * Heron topologies continuously monitor themselves and automatically notify the developer when something is not right.
 * Designed to support multi-tenancy and uses containers
 * Has a backpressure mechanism that dynamically adjusts the rate of data flow.
* Cons
 * Slow in feature implementation
 * Does not have popular fanbase

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Summary & References

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Concluding Remarks

* Build an end-to-end streaming application based on the concept of creating a system that processes real-time information.
* A system is said to be real-time when quantitative expressions of time are necessary to describe the behaviour of the system.
 * A real-time task is one that is associated with some time constraints.
 * A real-time task is classified into either hard, firm, or soft real-time type depending on the consequences of a task failing to meet its timing constraints.
* The characteristic features of a hard real-time system include embedded, feedback and distributed structure, and safety- criticality. It is possible though that some hard real-time systems may not have these features.

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## References

Real-Time Systems Design and Analysis Book by Philip A. Laplante 1992

Real-Time Concepts for Embedded Systems Book by Caroline Yao and Qing Li 2003

Designing Data-Intensive Applications The Big Ideas Behind Reliable, Scalable, and Maintainable Systems by Martin Kleppmann, 2019

Modern Data Engineering with Apache Spark: A Hands-On Guide for Building Mission-Critical Streaming Applications by Scott Haines

Real-Time Systems by Rajib Mall

ATA/S-ARTS/Real Time Processing and

Technologies

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Master of Technology

Architecting Real Time Systems

Module is a part of SWE5003 - Specialist Cert - Engineering Big Data

Real Time Processing via Kafka

Suria R Asai

Institute of Systems Science, National University of Singapore

© 2019-24 NUS. The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied.

Total: 80 Slides



## Learning Objectives

* Understand the challenges in Real Time Streaming Ingestion
* Understand how Kafka used for stream ingestion
* Implement an effective real time streaming ingestion application
* using Kafka.
 * Topics and Partitions, Events, Kafka Cluster and Brokers
 * Consumer Groups and Processor Topologies
 * Streams and Tables
 * Stateless Versus Stateful Processing
 * Stateless Processing



## Agenda

* Ingestion Challenges
* Kafka Programming
 * Kafka Deployment and Configurations
 * Create a Kafka Producer
 * Create a Kafka Consumer
 * Build a Spark Streaming Application with Kafka
 * State fullness and State lessness
* KStreams, KTable and Practices
* Summary



Kafka Components

Kaf k a A r chite c t u r e , Core a n d No n - Core C o mpon e n ts

Producer and Consumer Configurations



## Kappa Architecture
Kappa architecture finds its applications in real-time processing of distinct events.
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261555172.png)

## Kappa Useful In . . .
Kappa architecture can be deployed for those data processing enterprise models where:
- Multiple data events or queries are logged in a queue to be catered against a distributed file system storage or history.
- The order of the events and queries is not predetermined. Stream processing platforms can interact with database at any time.
- It is resilient and highly available as handling Terabytes of storage is required for each node of the system to support replication.
## Pros and Cons of Kappa architecture
Pros
 * Kappa architecture can be used to develop data systems that are online learners and therefore don’t need the batch layer.
 * Kappa architecture can be deployed with fixed memory.
 * Re-processing is required only when the code changes.
 * Kappa Architecture can be used for horizontally scalable systems.
 * Able to deal with never-ending streams of events.
 * Fewer resources are required:
 * Effective machine learning done on the real time basis.
 * Real-time or near-real-time processing easily possible using frameworks.
 * Detecting patterns in time-series data is easy.
Cons
 * Absence of batch layer might result in errors during data processing or while updating the database that requires having an exception manager to reprocess the data or reconciliation.
## Core	and Non-core Kafka
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402261557972.png)


## Kafka Core APIs
The [Producer API](https://kafka.apache.org/documentation.html#producerapi) [ ](https://kafka.apache.org/documentation.html#producerapi)allows an application to publish a stream of records to one or more Kafka topics.
The [Consumer ](https://kafka.apache.org/documentation.html#consumerapi) [API](https://kafka.apache.org/documentation.html#consumerapi) [ ](https://kafka.apache.org/documentation.html#consumerapi)allows an application to subscribe to one or more topics and process the stream of records produced to them.
The [Streams API](https://kafka.apache.org/documentation/streams) [ ](https://kafka.apache.org/documentation/streams)allows an application to act as a stream processor , consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.
The [Connector](https://kafka.apache.org/documentation.html#connect) [ ](https://kafka.apache.org/documentation.html#connect) [API](https://kafka.apache.org/documentation.html#connect) [ ](https://kafka.apache.org/documentation.html#connect)allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.

In Kafka the communication between the clients and the servers is done with a simple, high-performance, language agnostic **TCP protocol .**
## Message Topics Configuration
* Retention Period : The messages in the topic need to be stored for a defined period of time to save space irrespective of throughput.
* Space Retention Policy : We can also configure Kafka topics to clear messages when
* the size reaches the threshold mentioned in the configuration.
* Offset : Each message in Kafka is assigned with a number called as an offset.
 * Topics consist of many partitions. Each partition stores messages in the sequence in which they arrive.
 * Consumers acknowledge messages with an offset, which means that all the messages before that message offset are received by the consumer.
* Partition : Each Kafka topic consists of a fixed number of partitions.
* Compaction : Topic compaction helps in removing duplicates from a large number of messages.
* Leader : Partitions are replicated across the Kafka cluster based on the replication factor specified. One is a leader and the rest follows.
* Buffering : Kafka buffers messages both at the producer and consumer side to
* increase throughput and reduce Input Output (IO).
## Recollection Slide

Kafka Partitions

Each partition is an ordered, immutable sequence of records that is continually appended to—a structured commit log. The records in the partitions are each assigned a sequential id number called the offset that uniquely identifies each record within the partition.

Partitions are fault-tolerant; they are replicated across the Kafka brokers.

Kafka partitions (Ref: https://kafka.apache.org/documentation/)



## Message Partitions

Each partition has its leader that serves messages to the consumer that wants to read the message from the partition. If the leader fails a new leader is elected and continues to serve messages to the consumers. This helps in achieving high throughput and latency.

* High throughput : Partitions are a way to achieve parallelism in Kafka. The degree of parallelism in a single consumer group depends on the number of partitions it is reading from.
* Increase producer memory : As the partitions increase, we need
* to configure more memory on the producer side.
* High availability issue : Kafka is known as high-availability, high- throughput, and distributed messaging system.
 * Brokers in Kafka store thousands of partitions of different topics. Reading and writing to partitions happens through the leader of that partition. Generally, if the leader fails, electing a new leader takes only a few milliseconds with help of Controllers (one of the elected brokers).



## Replication Configurations

* Replicas of message logs for each topic partition are maintained across different servers in a Kafka cluster.
 * Quorum-based approach : In this approach, the leader will mark messages committed only when the majority of replicas have an acknowledged receiving the message.
 * If the leader fails, the election of the new a leader will only happen with coordination between followers. Zookeeper follows a quorum-based approach for leader election.
 * Primary backup approach : Kafka follows a different approach to maintaining replicas; the leader in Kafka waits for an acknowledgement from all the followers before marking the message as committed.
 * If the leader fails, any of the followers can take over as leader.
 * Write : All the leaders and followers have their own local log where they maintain the log end offset that represents the tail of the log.
 * The last committed message offset is called the High Watermark. When a client requests to write a message to partition, it first picks the leader of the partition from Zookeeper and creates a write request.
 * Read : All the reads happen through the leader only. The message that is acknowledged successfully by the leader will be available for the client to read.



## Role of Zoo Keeper

* Choosing a controller :
 * The controller is one of the brokers responsible for partition management with respect
* to leader election, topic creation, partition creation, and replica management. Kafka uses Zookeeper's metadata information to elect a controller.
* Brokers metadata:
 * Zookeeper records the state of each of the brokers that are part of the Kafka cluster. It records all relevant metadata about each broker in a cluster.
* Topic metadata:
 * Zookeeper records topic metadata such as the number of partitions, specific configuration
* parameters, and so on.
* Client quota information:
 * With newer versions of Kafka, quota features have been introduced. Quotas enforce byte-rate thresholds on clients to read and write messages to a Kafka topic. All the information and states are maintained by Zookeeper.
* Kafka topic ACLs:
 * Kafka has an in-built authorization module that is defined as Access Control Lists ( ACLs ). These ACLs determine user roles and what kind of read and write permissions each of these roles has on respective topics. Kafka uses Zookeeper to store all ACLs.



Recollection Slide

## Kafka Producers Configurations

* Kafka producers publish messages as per Kafka protocols defined by the makers of Kafka. Some key producer internals are:
 * Bootstrapping Kafka broker URLs :
 * The Producer connects to at least one broker to fetch metadata about the Kafka cluster. Producer iterates through a list of Kafka broker addresses until it finds the one to connect to fetch cluster metadata.
 * Data serialization:
 * Kafka producer serializes every message data object into ByteArrays before sending any record to the respective broker over the wire. Similarly, it converts any byte sequence received from the broker as a response to the message object.
 * Determining topic partition: producer to determine which topic partition data needs to be sent.
 * Determining the leader of the partition : producer's responsibility to determine the leader of the partition to which it will write messages.
 * Failure handling/retry ability: Handling failure responses or number of retries.
 * Batching: Batching ensures reduced I/O and optimum utilization of producer memory.

Internal implementation or the sequence of steps in Producer APIs may differ for respective programming languages. Some of the steps can be done in parallel using threads or callbacks.



## Sequence diagram of delivery time breakdown inside Kafka Producer



Recollection Slide

## Kafka Consumers Configurations

* Kafka Consumer API is used for reading the data from Kafka queues. Some key consumer internals are:
 * Subscribing to a topic : Consumer operations start with subscribing to a topic. If consumer is part of a consumer group, it will be assigned a subset of partitions from that topic.
 * Consumer offset position : Every consumer is responsible for maintaining its own consumer offset. Consumer offsets are maintained by consumer APIs.
 * Replay / rewind / skip messages : Using consumer APIs, any consumer application can pass the starting offsets to read messages from topic partitions.
 * Heartbeats : It is the consumer's responsibility to ensure that it sends regular heartbeat
* signals to the Kafka broker.
 * Offset commits : It is the responsibility of the consumer application to track their partition offset and commit it.
 * Deserialization : Kafka producers serialize objects into byte arrays before they are sent to Kafka.

With Kafka, every consumer has a unique identity and they are in full control of how they want to read data from each Kafka topic partition. Every consumer has its own consumer offset that is maintained in Zookeeper and they set it to the next location when they read data from a Kafka topic.



## Commits and Offset

If the committed offset is smaller than the offset of the last message the client processed, the messages between the last processed offset and the committed offset will be processed twice.

If the committed offset is larger than the offset of the last message the client actually processed, all messages between the last processed offset and the committed offset will be missed by the consumer group.



## Other Configurations

* Broker Configs
 * Listeners, Threads, Cluster, Logging, Security, SSL, Protocols, & Providers
* Topic Configs
 * Configurations pertinent to topics have both a server default as well an optional per-topic override. If no per-topic configuration is given the server default is used.
* Kafka Connect Configs
 * Configurations to define connectors that move large collections of data into and
* out of Kafka.
* Kafka Streams Configs
 * Kafka Streams related configurations and templates useful in building a client application.
* AdminClient Configs
 * Configurations used by an administrative client



## Kafka Schema

* Kafka Schema is the additional structure, or schema, imposed on the message content so that it can be easily understood.
 * Simplistic systems, such as Javascript Object Notation (JSON) and Extensible Markup Language (XML).
 * Developers favor the use of Apache Avro, which is a compact serialization framework where schema and
* payloads are dealt separately.
* A consistent data format is important in Kafka, as it allows writing and reading messages to be decoupled.

Images from [https://docs.confluent.io/platform/current/schema-registry/index.htm](https://docs.confluent.io/platform/current/schema-registry/index.html) [l](https://docs.confluent.io/platform/current/schema-registry/index.html)[ ](https://docs.confluent.io/platform/current/schema-registry/index.html)and [https://yokota.blog/2020/07/11/putting-several-event-types-in-the-same-topic-revisited/](https://yokota.blog/2020/07/11/putting-several-event-types-in-the-same-topic-revisited/)



## Kafka Use-Cases	- 1

* Activity Tracking
 * Example: Linked in’s original use case.
 * The messages are published to one or more topics, which are then
* consumed by applications on the backend.
 * These applications may be generating reports, feeding machine learning systems, updating search results, or performing other operations that are necessary to provide a rich user experience.
* Messaging
 * Example: applications sending notifications (such as emails) to users.
 * A single application can then read all the messages to be sent and handle them consistently, including formatting, collecting multiple messages and apply preferences.



* Metrics and Logging, Commit Logs
 * Example: Collect system metrics and logs from multiple applications
* producing the same type of message shines.
 * The messages are taken offline and routed to dedicated log search systems such as Elasticsearch published to one or more topics, which are then consumed by applications on the backend.
 * Direct commit log access for database and other web servers.
* Stream Processing
 * Example: functional pipelines on Kafka streams.
 * Streams operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for efficient processing by other applications, or transforming messages using data from multiple sources.



## Windows Tools
JDK, Scala, Kafka, Zookeeper and Offset Explorer



## Confluent Control Center





Kafka Core Processing

Comm a n d L ine T o ol and Of f s e t Vis u a l is e r

Application to show end-to-end producer/subscriber Demo and W o rkshop



## Events

A lot of the existing literature on Kafka, including the official documentation, uses a variety of terms to describe the data in a topic, including messages, records, and events.

Application-level headers contain optional metadata about an event.

Keys are also optional, but play an important role in how data is distributed across partitions.

Each event is associated with a timestamp.

The value contains the actual message contents, encoded as a byte array. It’s up to clients to deserialize the raw bytes into a more meaningful structure (e.g., a JSON object or Avro record).



## Kafka as Hub - 1

Reference: Kafka Streams in Action



## Kafka as Message Broker - 2

Kafka is a message broker. Producers send messages to Kafka, and those messages are stored and made

available to consumers via subscriptions to topics.



## Kafka as Streams – 3.



## Kafka Producer Components

A Kafka producer has three mandatory properties:

bootstrap.servers - List of host:port pairs of brokers that the producer will use to establish initial connection to the Kafka cluster.

key.serializer - Name of a class that will be used to serialize the keys of the records

value.serializer - Name of a class that will

be used to serialize the values of the records

Properties kafkaProps = new Properties(); kafkaProps.put("bootstrap.servers", "broker1:9092,broker2:9092"); kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); producer = new KafkaProducer<String, String>(kafkaProps);



## Creating a Kafka Producer

* Creating a Kafka producer involves the following steps:
 * Required configuration.
 * Creating a producer object.
 * Setting up a producer record.
 * Creating a custom partition if required.
 * Additional configuration.
 * Properties object contains the put method that is used to put the configuration key-
* value pair in place.
 * Serializer class is used for both key and value strings.
 * Producer object provides the producer with specific information about broker servers, serializer classes, and other configurations.
 * Produce r accepts the ProducerRecor d object to send records to the ProducerRecord topic. It contains a topic name, partition number, timestamp, key, and value.
 * Kafka callback interface send() can accept an object that implements the callback interface.



## Producer Example

import java.util.Properties;

import java.util.concurrent.Future;

import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import sg.edu.iss.kafkademo.util.KafkaConstants;

public class ProducerDemo {

public static void main(String[] args) {

Properties producerProps = new Properties(); producerProps.put("bootstrap.servers", KafkaConstants.BOOTSTRAPSERVERS); producerProps.put("key.serializer", KafkaConstants.KSERIALIZER); producerProps.put("value.serializer", KafkaConstants.KSERIALIZER); producerProps.put("acks", "all");

producerProps.put("retries", 1);

producerProps.put("batch.size", 20000);

producerProps.put("linger.ms", 1);

producerProps.put("buffer.memory", 24568545);

KafkaProducer <String, String> producer =

new KafkaProducer<String, String>(producerProps); for (int i = 0; i < 2000; i++) {

ProducerRecord data =

new ProducerRecord<String, String>("test1", "Hello this is record " + i); Future<RecordMetadata> recordMetadata = producer.send(data) ;

}

producer.close();

}

}



## Additional Producer Configurations

Kafka provides you with an API to implement your own partition.

Other optional configuration properties available for Kafka producer

| buffer.memory | The amount of memory that producer can use to buffer a message |
| :-: | :-: |
| request.timeout.ms | Time out to avoid keeping records queued indefinitely |
| acks | acks=0: Producer will not wait for any acknowledgment ; acks=1: Producer will receive an ack as soon as the leader has written the message; ack=2 Producer will only receive ack when all replicas write |
| batch.size | Allows to batch the messages on partition up to configured amount of size |
| linger.ms | Amount of time that a producer should wait for additional messages |
| compression.type | Available compressions are GZIP, Snappy, or LZ4 |
| retires | Number of times producer will retry sending before throwing exception |
| max.in.flight.reque sts.per.connection | Number of messages producer can send to brokers without waiting for a response |
| partitioner.class | Custom partitioner for producer if any |
| timeout.ms | Amount of time a leader will wait for its followers to acknowledge the message |



## Common messaging publishing patterns

* Fire-and-forget : In this pattern, producers only care about sending messages to Kafka queues. They really do not wait for any success or failure response from Kafka.
* One message transfers : In this pattern, producer sends one message at a time.
 * In synchronous mode, producer sends the message and waits for a success or failure response before retrying the message or throwing the exception.
 * In asynchronous mode, producer sends the message and receives the success or failure response as a callback function.
* Batching : In this pattern, producers send multiple records to the same partition in a batch. The amount of memory required by a batch and wait time before sending the batch to Kafka is controlled by producer configuration parameters.

(Ref: https://kafka.apache.org/documentation/)



## Kafka Consumer Components

https:/[/w](http://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/)w[w.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/](http://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/)



## Creating a Kafka Consumer

* Creating a Kafka consumer involves the following steps:
 * Consumer configuration
 * KafkaCon s u m e r object
 * Subscription and polling
 * Commit and offset
 * Additional configuration
 * Properties object contains the put method that is used to put the
* configuration key-value pair in place.
 * Deserializer class is used used to convert ByteArray to the required object.
 * KafkaConsumer tell the consumer object about brokers IP to connect, the group name, deserialization and offset strategy.
 * Subscription , Polling and Committing : Consumer has to subscribe to some topic to receive data. Polling is fetching data from the Kafka topic. Kafka commits the offset of messages that it reads successfully.



## Consumer Example - 1

import java.util.ArrayList; import java.util.List; import java.util.Map;

import java.util.Properties;

import org.apache.kafka.clients.consumer.*; import org.apache.kafka.common.TopicPartition; import org.slf4j.Logger;

import org.slf4j.LoggerFactory;

import sg.edu.iss.kafkademo.util.KafkaConstants; public class ConsumerDemo {

public static void main(String[] args) {

Logger logger = LoggerFactory.getLogger(ConsumerDemo.class.getName()); String topic = "test1";

List<String> topicList = new ArrayList();

topicList.add(topic);

Properties consumerProperties = new Properties(); consumerProperties.put("bootstrap.servers", KafkaConstants.BOOTSTRAPSERVERS); consumerProperties.put("group.id", "DemoGroup"); consumerProperties.put("key.deserializer", KafkaConstants.KDSERIALIZER); consumerProperties.put("value.deserializer", KafkaConstants.KDSERIALIZER); consumerProperties.put("enable.auto.commit", "true"); consumerProperties.put("auto.commit.interval.ms", "1000");

consumerProperties.put("session.timeout.ms", "30000");



## Consumer Example – 2.

try

KafkaConsumer <String, String> demoKafkaConsumer =

new KafkaConsumer<String, String>(consumerProperties); demoKafkaConsumer.subscribe(topicList); logger.info("Subscribed to topic " + topic);

int i = 0;

try {	while (true) {

ConsumerRecords<String, String> records = demoKafkaConsumer.poll(500); for (ConsumerRecord<String, String> record : records)

logger.info("offset = " + record.offset() + "key =" + record.key() + "value =" +

record.value() ); demoKafkaConsumer.commitAsync(new OffsetCommitCallback() {

public void onComplete(Map<TopicPartition, OffsetAndMetadata> map, Exception e) { }

});

}

} catch (Exception ex) {

} finally {

try { demoKafkaConsumer.commitSync();

} finally { demoKafkaConsumer.close();

}

}

}

}



## Additional Consumer Configurations

Other optional configuration properties available for Kafka consumer

| enable.auto.commit | If true, then consumer will automatically commit the message offset after the configured interval of time |
| :-: | :-: |
| fetch.min.bytes | minimum amount of data in bytes that the Kafka server needs to return for a fetch request |
| auto.offset.reset | latest: This value, if set to latest, means that the consumer will start reading from the latest message from the partition available at that time when consumer started. earliest: This value, if set to earliest, means that the consumer will start reading data from the beginning of the partition, which means that it will read all the data from the partition.<br />none: This value, if set to none, means that an exception will be thrown to the consumer. |
| request.timeout.ms | maximum amount of time that consumer will wait for a response to the request made before resending the request |
| session.timeout.ms | Consumer sends heartbeat (within configured period) to the consumer group coordinator to tell it that it is alive and restrict triggering the rebalancer. |
| max.partition.fetch. bytes | the maximum amount of data that the server will return per partition. |



## Common messaging consuming patterns

* Consumer group - continuous data processing : In this pattern, once consumer is created and subscribes to a topic, it starts receiving messages from the current offset.
 * The consumer commits the latest offsets based on the count of
* messages received in a batch at a regular, configured interval.
 * The consumer checks whether it's time to commit, and if it is, it will commit the offsets.
* Consumer group - discrete data processing : In this, consumers fetch data based on the offset provided and they commit specific offsets that are set as per their specific application requirements.
 * Commit can happen synchronously or asynchronously.
 * The consumer API allows call to commitSync() and commitAsync() and pass a map of partitions and offsets that we wish to commit.

(Ref: https://kafka.apache.org/documentation/)



## Event Processing

* Event processing takes one or more events from an event stream and applies actions to those events.
* Common enterprise services
 * Event handling
 * Data transformation
 * Data mapping
 * Protocol conversion
* Event Operations
 * An event stream to filter some events from the stream
 * Event validation against an event schema
 * Event enrichment with additional data
 * Event composition (aggregation) to produce a new event from two or more events



## Reader and Writer

import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; public class Reader implements Consumer {

private final KafkaConsumer<String, String> consumer; // 1 private final String topic;

public Reader(String servers, String groupId, String topic) {

this.consumer = new KafkaConsumer<String, String>(Consumer.createConfig(servers, groupId)); this.topic = topic;

}

@Override

public ConsumerRecords<String, String> consume() { this.consumer.subscribe(java.util.Arrays.asList(this.topic)); // 2 ConsumerRecords<String, String> records = consumer.poll(100); // 3 return records;

}

}

import org.apache.kafka.clients.producer.*; public class Writer implements Producer {

private final KafkaProducer<String, String> producer; private final String topic;

public Writer(String servers, String topic) {

this.producer = new KafkaProducer<String, String>(Producer.createConfig(servers)); // 1 this.topic = topic;

}

@Override

public void produce(String message) { //2

ProducerRecord<String, String> pr = new ProducerRecord<String, String>(topic, message); producer.send(pr);

}

}



## Message Validation

The processing application reads events from the raw-messages topic, validates the messages, and routes the errors to the invalid-

messages topic and the correct ones to the valid-messages topic.



## Example Validator - 1

import java.io.IOException;

import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerRecord; import com.fasterxml.jackson.databind.JsonNode;

import com.fasterxml.jackson.databind.ObjectMapper; public class Validator implements Producer {

private final KafkaProducer<String, String> producer; private final String goodTopic;

private final String badTopic;

protected static final ObjectMapper MAPPER = new ObjectMapper();

public Validator(String servers, String goodTopic, String badTopic) { // 1 this.producer = new KafkaProducer<String, String>(Producer.createConfig(servers)); this.goodTopic = goodTopic;

this.badTopic = badTopic;

}

@Override

public void produce(String message) { //2 ProducerRecord<String, String> pr = null; try {

JsonNode root = MAPPER.readTree(message); String error = "";

error = error.concat(validate(root, "event")); error = error.concat(validate(root, "customer")); error = error.concat(validate(root, "currency")); error = error.concat(validate(root, "timestamp")); if (error.length() > 0) {

pr = new ProducerRecord<String, String>(this.badTopic, "{"error": " " + error + ""}"); // 3

} else {

pr = new ProducerRecord<String, String>(this.goodTopic, MAPPER.writeValueAsString(root));// 4

}

} catch (IOException e) {



} catch (IOException e) {

pr = new ProducerRecord<String, String>(this.badTopic,

"{"error": "" + e.getClass().getSimpleName() + ": " + e.getMessage() + ""}"); // 5

} finally {

if (null != pr) { producer.send(pr);

}

}

}

private String validate(JsonNode root, String path) { if (!root.has(path)) {

return path.concat(" is missing. ");

}

JsonNode node = root.path(path); if (node.isMissingNode()) {

return path.concat(" is missing. ");

}

return "";

}

}

In 1, the constructor takes two topics: the good and the bad message topics

In 2, the produce method validates that the message is in JSON format, and the existence of the event, customer, currency, and timestamp fields

In 3, if the message doesn't have any required fields, an error message is sent to the bad messages topic In 4, if the message is correct, the message is sent to the good messages topic

In 5, if the message is not in JSON format, an error message is sent to the bad messages topic



## Message Enrichment

The processing application reads events from the raw-messages topic, validates the messages, routes the errors to the invalid-messages topic, enriches the messages with geolocation and prices, and finally, writes them to the valid-messages topic.

Taxi Monitoring Event

TMDT ID

Driver ID Taxi Number

Current Position

Availability Status

From Backend Databases

Taxi Number Taxi Type

Taxi Passenger Capacity Taxi Make Model Driver Name

Driver Phone Number







Kafka Streams and ksqlDB

A n i n t r o d u ct i o n to K a f k a S t r ea m s P roces s i n g

Stateful vs Stateless Processing Wi n d o w s a n d T ime

A n i n t r o d u c t ion to k s q lDB



## Kafka Streaming platform



## Homogeneous, or Heterogeneous Topic



## Kafka Streams

Kafka Streams is the “brain” of the Kafka ecosystem, consuming records from the event stream, processing the data, and optionally writing enriched or transformed records back to Kafka

Reference: Mastering Kafka Streams and ksqlDB



## Secured Streaming

This may be over the public internet and require that

the data be secured.

Browser, IoT Device, Machine, IoE etc

Browser, IoT Device, Machine, IoE etc

Collection (Ingestion) Tier

Message Queueing Tier

In Memory Data Store

Data Access Tier

Long Term Storage



## Data Processing Patterns

Bounded data processing with a classic batch engine . A finite pool of unstructured data on the left is run through a data processing engine, resulting in corresponding structured data on the right.

Unbounded data processing via ad hoc fixed windows with a classic batch engine . An unbounded dataset is collected up front into finite, fixed-size windows of bounded data that are then processed via successive runs a of classic batch engine.

Unbounded data processing into sessions via ad hoc fixed windows with a classic batch engine. An unbounded dataset is collected up front into finite, fixed-size windows of bounded data that are then subdivided into dynamic session windows via successive runs a of classic batch engine.



## Micro-batching

Micro-batching involves grouping records into small batches and emitting them to downstream processors at a fixed interval; event-at-a-time processing allows each event to be processed at soon as it comes in, instead of waiting for a batch to materialize



## Bounded and Unbounded

Unbounded streams have a start but no definite end. Unbounded streams do not terminate and provide the data as it is generated. It must be continuously processed, i.e., events must be immediately handled after they have been taken. Input is unbounded and will not be complete at any point of time, so it is not possible to wait for all input data to arrive.

[https://www.tutorialandexample.com/apache-flink-tutorial/](https://www.tutorialandexample.com/apache-flink-tutorial/)



## Time Agnostic Processing

Time-agnostic processing is used for cases in which time is essentially irrelevant; that is, all relevant logic is data driven. Because everything about such use cases is dictated by the arrival of more data, there’s really nothing special a streaming engine has to support other than basic data delivery.

Example 1 - Filtering unbounded data. A collection of data (flowing left to right) of varying types is filtered into a homogeneous collection containing a single type.

Example 2 - Performing an inner join on unbounded data. Joins are produced when matching elements from both sources are observed.



## Approximate Algorithms

Approximation algorithms, such as approximate Top-N, streaming k-means, and so on, take an unbounded source of input and provide output. The upside of approximation algorithms is that, by design, they are low overhead and designed for unbounded data. The downsides are that a limited set of them exist, the algorithms themselves are often complicated.

Computing approximations on unbounded data. Data are run through a complex algorithm, yielding output data that look more or less like the desired result on the other side.



## Windowing Patterns - 1

Windowing strategies. Each example is shown for three different keys, highlighting the difference between aligned windows (which apply across all the data) and unaligned windows (which apply across a subset of the data).



## Windowing Patterns – 2

* Three commonly used Windowing Pattern :
 * A fixed/tumbling window basically divides the incoming stream of data into fixed-size segments, where each one has a window length, a start time, and an end time.
 * With this small batch of data in each window, it is easy to reason about when performing aggregations such as sum, max, or average.
 * A sliding window is another way of dividing the incoming stream of data into fixed-size segments, where each one has a window length and a sliding interval.
 * Because of the overlapping of the windows, the aggregation will produce a smoother result than in the fixed/tumbling window.
 * The session window type has no predetermined window length. Rather, it is determined usually by a gap of inactivity that is greater than some threshold.
 * For example, the length of a session window on Facebook is determined by the duration of activities that a user does, such as browsing the user feeds, sending messages, and so on.



Windowing into fixed windows by processing time . Data are collected into windows based on the order they arrive in the pipeline.

Windowing into fixed windows by event time . Data are collected into windows based on the times at which they occurred. The black arrows call out example data that arrived in processing-time windows that differed from the event-time windows to which they belonged.



Windowing into session windows by event time. Data are collected into session windows capturing bursts of activity based on the times that the corresponding events occurred. The black arrows again call out the temporal shuffle necessary to put the data into their correct event-time locations.



## Kafka Streams

Kafka Streams lives in the stream processing layer of the Kafka ecosystem. This is where sophisticated data processing, transformation, and enrichment happen.

Kafka Streams was built to simplify the development of stream processing applications with a simple, functional API and a set of stream processing primitives that can be reused across projects. When more control is needed, a lower-level Processor API can also be used to define our topology.

Kafka Streams has a friendlier learning curve and a simpler deployment model than cluster-based solutions like Apache Flink and Apache Spark Streaming. It also supports event-at-a-time processing, which is considered true streaming.

Kafka Streams is great for solving problems that require or benefit from real-time decision making and data processing. Furthermore, it is reliable, maintainable, scalable, and elastic.



## Example:
Four Kafka Streams tasks running in four threads



## APIs and Data Structure

* KStream
 * A KStream is an abstraction of a partitioned record stream, in which data is represented using insert semantics (i.e., each event is considered to be independent of other events).
* KTable
 * A KTable is an abstraction of a partitioned table (i.e., changelog stream), in which data is represented using update semantics (the latest representation of a given key is tracked by the application). Since KTables are partitioned, each Kafka Streams task contains only a subset of the full table.
* GlobalKTable
 * This is similar to a KTable, except each GlobalKTable contains a complete
* (i.e., unpartitioned) copy of the underlying data.



## Stateless Versus Stateful Processing

* Stateless Applications
 * Each event handled by Kafka Streams application is processed independently of other events, and only stream views are needed by our application.
 * Our application treats each event as a self-contained insert and requires no memory of previously seen events.
* Stateful Applications
 * Applications need to remember information about previously seen events in one or more steps of our processor topology, usually for the purpose of aggregating, windowing, or joining event streams.
 * These applications are more complex under the hood since they need to track additional data, or state .



## Example - Twitter

* Tweets that mention certain hashtags are
* consumed from a source topic called tweets:
 * Each record is JSON-encoded.
 * Unneeded fields should be removed during the deserialization.
* Retweets should be excluded (filtered) from processing.
* Tweets that aren’t written in English should be branched into a separate stream for translating.
* Non-English tweets need to be translated to English. The newly translated tweets should be merged with the English tweets stream to create one unified stream.
* Each tweet should be enriched with a sentiment score.
* The enriched tweets should be serialized using Avro, and written to an output topic.

(Such insights can guide us in business decisions.)

Reference: Mastering Kafka Streams and ksqlDB



## Remote Queries

* Stateful processing helps to understand the relationships between events and leverage these relationships for more advanced stream processing use cases.
* Kafka Streams captures information about the events it consumes into Ktable and GlobalKTable.
* Using stateful processing we can:
 * Recognize patterns and behaviors in our event streams
 * Perform aggregations
 * Enrich data in more sophisticated ways
* using joins







## Practices and Tips

Comm o n Be s t P ractic e s



## Producer Common Best Practices

* Data validation :
 * Perform basic validation; examples could be conformity to schema, not null values for Key fields, and so on.
* Exception handling:
 * Define different exception classes and as per your business requirements, decide on the actions that need to be taken
* Design Parameters :
 * Number of retries, Number of bootstrap URLs are to be designed
* Design Partitioning :
 * Choose an appropriate partitioning strategy to ensure that messages are distributed uniformly across all topic partitions.
 * For highly reliable systems, persist messages that are passing through producer applications.
 * Avoid adding partitions to existing topics when using key-based partitioning for message distribution



## Consumer Common Best Practices

Exception handling : Just like producers, it is the sole responsibility of consumer programs to decide on program flows with respect to exceptions.

Handling rebalances : Whenever any new consumer joins consumer groups or any old consumer shuts down, a partition rebalance is triggered.

Commit offsets at the right time : If you are choosing to commit offset

for messages, you need to do it at the right time.

Automatic offset commits: Choosing an auto-commit is also an option to go with where we do not care about processing duplicate records or want consumer to take care of the offset commit automatically.

Keeping the auto-commit interval low will always result in avoiding less processing of duplicate messages.



## Direct Approach to Spark Kafka Integration

(Ref: https://kafka.apache.org/documentation/)



## Direct Approach Features

Parallelism and throughput : Spark Streaming creates RDD partition equal to the number of Kafka partitions available to consume data in parallel which increases throughput.

No write-ahead log : Spark reads data directly from Kafka and commits the offset of processed messages to checkpoint. In case of failure, Spark knows where to start.

No Zookeeper : Spark uses a checkpoint mechanism to deal with data loss and to start execution from the last execution point in case of failure.

Exactly one processing : Direct approach provides opportunity to achieve exactly one processing, which means that no data is processed twice and no data is lost.



## Summary



## Concluding Remarks

* We discussed about the Kafka architecture and its component in detail. We can put Kafka architecture in two viewpoints :
 * The logical viewpoint is from the perspective of establishing data flows and
* seeing how different components depend on each other.
 * The technical viewpoint will help in technically designing producer/consumer applications and understanding the Kafka physical design.
* Every component in Kafka has some specific role to play, and, even if one of these is missing overall Kafka functionality cannot be achieved.
* We discussed Kafka Producer APIs, Kafka Consumer APIs and
* different components around.



## References



Apache Kafka Quick Start Guide by Raul Estrada Published by Packt Publishing, 2018

Kafka: The Definitive Guide, 2nd Edition by Neha Narkhede; Gwen Shapira; Rajini Sivaram; Todd Palino, Published by O'Reilly Media, Inc., 2021

Mastering Kafka Streams and ksqlDB by Mitch Seymour, Publisher: O'Reilly Media, Inc., February 2021.

Kafka Documentation [https://kafka.apache.org/](https://kafka.apache.org/)



## Master Of Technology

Spark Stream Processing

Suria R Asai

[suria@nus.edu.sg](mailto:suria@nus.edu.sg)

Institute of Systems Science National University of Singapore

© 2019-24 NUS. The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied.

Total Slides : 82

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Learning Objectives

* Design robust Streaming Applications .
* Understand and apply transformation functions and performance tuning techniques on moving data.
* Design for ingestion , sources and sinks based on processing requirements.
* Apply data processing patterns, as well as technologies that help
* to speed up the development of stream processing systems.
 * Recognize the anti-patterns to avoid while designing a streaming applications

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Agenda

* Stream-Processing Model
* Spark Structured Streaming
 * Structured Streaming Sources
 * Structured Streaming Sinks
 * Event Based Stream Processing
 * State Management
* Discrete Stream Processing

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Stream Processing Model

F ro m a li ttle s p ark ma y b u r s t a fl a me.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Highlights

Purely declarative API based on automatically incrementalizing some static relational queries expressed through Spark SQL / DataFrame API.

Support end-to-end real-time applications that integrate streaming with batch and interactive analysis, with exactly-once guarantee and various operational features.

Run in a microbatch execution mode by default but also support a low- latency continuous operators for some streaming queries, with the tradeoff of operational flexibility and latency.

Hybrid batch and streaming execution saves significant cost for low- volume applications and ease operations in some scenarios.

Compared to Apache Flink and Apache Kafka Streams, achieve high performance in throughput by leveraging Spark SQL code generation engine.

Native integration with Apache Spark and automatic leverage of execution runtime / optimization / operational support / ecosystem.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Pure Streaming System vs Continuous Application

Pure Streaming Systems

Continuous Application

Output

Sink

(Transactions handles by engine)

Output Sink

( T r a nsa cti on s o f t en up to the user)

Streaming Computation

Continuous Application

(Interaction with other systems left to the user)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Declarative API for Real-Time Apps

https:/[/w](http://www.linkedin.com/pulse/15-minutes-read-structured-streaming-declarative-api-real-time-zhang/)w[w.linkedin.com/pulse/15-minutes-read-structured-streaming-declarative-api-real-time-zhang/](http://www.linkedin.com/pulse/15-minutes-read-structured-streaming-declarative-api-real-time-zhang/)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Recollection Slide

Bounded and Unbounded Stream

Unbounded streams have a start but no definite end. Unbounded streams do not terminate and provide the data as it is generated. It must be continuously processed, i.e., events must be immediately handled after they have been taken. Input is unbounded and will not be complete at any point of time, so it is not possible to wait for all input data to arrive.

[https://www.tutorialandexample.com/apache-flink-tutorial/](https://www.tutorialandexample.com/apache-flink-tutorial/)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Unbounded Table

Reference: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.htm](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) [l](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Programming Model

A query on the input will generate the “ Result Table ”.

Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table.

Whenever the result table gets updated, we would want to write the changed result rows to an external sink.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Output Modes

* Output modes are a way to tell Structure Streaming how the output data should be written to a sink. There are three options.
 * Append mode : This is the default mode if output mode is not specified. In this mode, only new rows that were appended to the result table will be sent to the specified output sink.
 * Complete mode : The entire result table will be written to the
* output sink.
 * Update mode : Only the rows that were updated in the result table will be written to the output sink. For the rows that were not changed, they will not be written out.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Trigger Types

* The trigger information determines when to run the provided streaming
* computation logic in the streaming application.
 * Not specified(default)
 * For this default type, Spark will use the micro-batch mode and process the next batch of data as soon
* as the previous batch of data has completed processing.
 * Fixed interval
 * For this type, Spark will use the micro-batch mode and process the batch of data based on the user- provided interval.
 * If for whatever reason the processing of the previous batch of data takes longer than the interval, then the next batch of data is processed immediately after the previous one is completed. In other words, Spark will not wait until the next interval boundary.
 * One-time
 * This trigger type is meant to be used for one-time processing of the available batch of data, and Spark will immediately stop the streaming application once the processing is completed.
 * This trigger type is useful when the data volume is extremely low, and therefore it is more cost effective to spin up a cluster and process the data only a few times a day.
 * Continuous
 * This trigger type invokes the new continuous processing mode that is designed for a certain streaming applications that require very low latency.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Processing Sample

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Recollection Slide

## Window Patterns

A fixed/tumbling window basically divides the incoming stream of data into fixed- size segments, where each one has a window length, a start time, and an end time.

A sliding window is another way of dividing the incoming stream of data into fixed-

size segments, where each one has a window length and a sliding interval.

The session window type has no predetermined window length. Rather, it is determined usually by a gap of inactivity that is greater than some threshold.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Word Count Example

$ nc -lk 9999

$ ./bin/run-example StructuredWordCount

localhost 9999

// Create DataFrame representing the

// stream of input lines from

// connection to localhost:9999 val lines = spark. readStream

.format("socket")

.option("host", "localhost")

.option("port", 9999)

.load()

// Split the lines into words val words = lines.as[String]

.flatMap(.split(" "))

// Generate running word count val wordCounts = words

.groupBy("value").count()

// Start running the query that prints

//	the running counts to the console

val query = wordCounts. writeStream

.outputMode("complete")

.format("console")

.start()

query.awaitTermination()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Examples of Real Time Processing - 1

Bank ATMs : Receive an input from the user and instantly reflect the transactions (withdrawal or any other request) to the centralized account.

Real-time monitoring : Capturing and analyzing data emitted from various

data sources like sensors, logs live feeds, and so on in real time.

Real-time business intelligence : Process of delivering business

intelligence (BI) or information about business operations as they occur. For [more](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[ ](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[information,](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[ ](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[refer](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[ ](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[to](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)[ ](https://en.wikipedia.org/wiki/Real-timebusinessintelligence) [https://en.wikipedia.org/wiki/Real-](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)

[timebusinessintelligence](https://en.wikipedia.org/wiki/Real-timebusinessintelligence) [.](https://en.wikipedia.org/wiki/Real-timebusinessintelligence)

Operational intelligence ( OI ): It uses real-time data processing and complex event processing systems to gain insight into operations by running query analyses against live feeds and event data. ( [http://en.wikipedia.org/wiki/Complexeventprocessing](http://en.wikipedia.org/wiki/Complexeventprocessing) )

For more information, refer

to [http://en.wikipedia.org/wiki/Operationalintelligence](http://en.wikipedia.org/wiki/Operationalintelligence) .

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Point of Sale (POS) systems : Update inventory, provide inventory history,

and sales of a particular item allowing an organization to run payments in real time.

Assembly lines : Process data in real time to reduce time, cost and errors. Errors are instantly captured and appropriate actions are taken without any delays which could otherwise have produced low quality or faulty products.

Fraud detection : Financial organizations use Spark Streaming to detect fraudulent transactions in real time. This helps organizations to proactively take a decision on a transaction and to avoid damage in a timely manner.

Recommendations : Domains including e-commerce, media, and others, use Spark Streaming to recommend the next set of products to purchase or the next set of stories to read based on current activities on their platform.

Risk avoidance : Service providers use Spark Streaming to expand their scope of due diligence before offering any service or product. This helps to reduce the risk of providing service or products to non-eligible consumers.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Complexity in Real Time systems

* System responsiveness :
 * The un-said expectations from real-time data processing systems are that they should process data as it arrives within milliseconds or microseconds , and should not introduce any delays in producing results.
* Fault-tolerant :
 * Failures do happen but real-time systems cannot afford to lose a single event.
* Scalable :
 * This need to have a scale-out architecture so that it can meet the demands of growing data by adding more computational resources without re-architecting the complete solution.
* In memory :
 * Real-time systems cannot afford to read/write from disks, so data processing needs to be handled within the memory itself. So the systems should ensure sufficient memory for storing the input data in the system's memory.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Spark Structured Streaming

Always in motion is the future.

— Grand Jedi Master Yoda, Star Wars Episode V: The Empire Strikes Back

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Spark Streaming Applications

A Spark Streaming application deployed in a cluster such as Hadoop YARN, Mesos or Spark Standalone mode has two main components very similar to any other type of Spark application:

Spark driver : This contains the application code written by the user

Executors : The executors that execute the jobs submitted by the Spark driver

Data Block	Data Block	Data Block

Cluster Infrastructure (YARN/Mesos/Standalone)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## High-level architecture

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming architecture

The obtained streamed information is sent to backend administrations so that it can be sorted out and making it accessible to business experts, application engineers, and machine learning calculations.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Major Aspects

Fast recovery from failures and stragglers

Better load balancing and resource usage

Combining of streaming data with static datasets and interactive queries

Native integration with advanced processing libraries (SQL, machine learning, graph processing)

Reference: Older Databricks Documentation

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Micro Batches

Spark Streaming uses a “micro-batch” architecture, where the streaming computation is treated as a continuous series of batch computations on small batches of data.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Streaming within Spark’s components

Reference: Execution of Spark Streaming within Spark’s components @ Learning Spark by Holden Karau; Patrick Wendell; Andy Konwinski; Matei Zaharia

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Data Sources

* Spark Streaming enables developer/analysts to analyze data from a variety of data sources.
 * Kafka source : require Apache Kafka with version 0.10 or higher.
 * File source : Files are located on either the local file system, HDFS, or S3.
 * Socket source : This is for testing purposes only.
 * Rate source : This is for testing and benchmark purposes only.
* Official Spark connectors exist for the following external systems and protocols:
 * Kafka (https://kafka.apache.org)—A distributed, fast, scalable publish-subscribe messaging system. It persists all messages and is capable of acting as a	replay queue.
 * Flume (https://flume.apache.org)—A distributed, reliable system for collecting, aggregating, and
* transferring large amounts of log data.
 * Amazon Kinesis (https://aws.amazon.com/en/kinesis)—An AWS streaming platform similar to Kafka.
 * Twitter (https://dev.twitter.com/overview/documentation)—The popular social network’s API
* service.
 * ZeroMQ (http://zeromq.org)—A distributed messaging system
 * MQTT (http://mqtt.org)—A lightweight publish-subscribe messaging protocol.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Data Delivery Semantics

At most once : This implies that a streaming processing engine guarantees that a piece of data will be delivered to an application no more than one time, but it could be zero times.

* For some use cases, this is acceptable, but it is not for some other use cases. One of those use cases is a financial transaction processing application.
* Losing data can result in not charging customers and therefore a reduction in revenue.
* At least once : This implies that a streaming processing engine guarantees that
* a piece of data will be delivered to an application one or more times.
 * There is no data lost in this case; however, there is a potential for double or triple counting.
 * In the example of financial transaction processing applications, it means that a transaction is applied multiple times, which results in complaints from customers. This guarantee is stronger than at most once because no data will be lost.
* Exactly once : This implies that a streaming processing engine guarantees that a piece of data will be delivered to an application exactly one time only, no more and no less.
 * In this case, there is no data loss and no double counting. Most modern and popular streaming processing engines provide this kind of guarantee.

Of the three guarantees, this one is the most desirable for building critical business streaming applications.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Processing streaming data in Apache Spark

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Spark Streaming Application Data Flow - 1

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

* Starting with the Spark Streaming Context (SSC) , the driver will execute long-running tasks on the executors (that is, the Spark workers).
* The code defined within the driver (starting ssc.start()), the Receiver on the executors ( Executor 1 in this diagram) receives a data stream from the Streaming Sources .
 * Spark Streaming can receive Kafka or Twitter , and/or we can build our own custom receiver. With the incoming data stream, the receiver divides the stream into blocks and keeps these blocks in memory.
* These data blocks are replicated to another executor for high availability.
* The block ID information is transmitted to the block manager master on the driver, thus ensuring that each block of data in memory is tracked and accounted for.
* For every batch interval configured within SSC (commonly, this is every 1 second), the driver will launch Spark tasks to process the blocks.

Those blocks are then persisted to any number of target data stores, including cloud storage (for example, S3, WASB), relational data stores (for example, MySQL, PostgreSQL, and so on), and NoSQL stores.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Processing Mode in Structured Streaming

Example: Latencies in 100s of ms

Example:

Latencies in

<10s of ms

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Anatomy of a Streaming Query

| Source: Specify where to read data from Built-in support for Files / Kafka/Sockets/Pluggable/ Kinesis* Can include multiple sources of different types using join() / union() |
| :-: |
| Transformations Using Data Frames or Datasets or SQL. Internal processing - always exactly once, optimized SQL functions like fromjson user- defined functions, lambdas, function literals with map, flatMap… |
| Sink Write transformed output to external storage systems Built-in support for Files/Sockets/Parquet<br />/Avro /Kafka Use foreach to execute arbitrary code with the output data Some sinks are transactional and exactly once (e.g. files). |
| Trigger When to output. Specified as time, eventually supports data size. No trigger means as fast as possible. Output Mode can be complete (whole answer) or update (changed) or append (new). |

from pyspark.sql import Trigger

spark.readStream

.format(“kafka”)

.option(“subscribe”, “sometopic”)

.load()

.groupBy(“value.cast(“string”) as key)

.agg(count(“*”) as ‘value’)

.writeStream()

.formal(“kafka”)

.option(“topic”, “output”)

.trigger(“1 minute”)

.outputMode(“update”)

(chekpointlocation”, “…”)

.start()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Simple netcat based Word Count - 1

from pyspark.sql import SparkSession

from pyspark.sql.functions import explode

from pyspark.sql.functions import split

spark = SparkSession 

.builder 

.appName( "StructuredNetworkWordCount" ) 

.getOrCreate()

## Create DataFrame collecting stream of input lines from connection to localhost:9999

lines = spark 

.readStream 

.format( "socket" ) 

.option( "host" , "localhost" ) 

.option( "port" , 9999) 

.load()

## Split the lines into words

words = lines.select( explode(

split(lines.value, " " )

).alias( "word" )

)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

| ## Generate running word count<br />wordCounts = words.groupBy("word").count()<br /><br />## Start running the query that prints the running counts to the console<br />query = wordCounts <br />.writeStream  | |
| :-: | :-: |
| .outputMode("complete") <br />.format("console") <br />.start()<br /><br />query.awaitTermination() | |
| | |

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Structured Streaming

A stream seen as an indexed sequence of events

Offsets are used to request data from the external source and

to indicate what data has already been consumed.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Structured Stream Processing Model - 1

Reference: [https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.htm](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html) [l](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Reference: [https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.htm](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html) [l](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Available Sources

* The following are the sources currently available in the Spark distribution of Structured Streaming:
* File
 * Allows the ingestion of data stored as files. In most cases, the data is transformed in records that are further processed in streaming mode. This supports these formats: JSON, CSV, Parquet, ORC, and plain text.
* Kafka
 * Allows the consumption of streaming data from Apache Kafka.
* Socket
 * A TCP socket client able to connect to a TCP server and consume a text-based data stream. The stream must be encoded in the UTF-8 character set.
* Rate
 * Produces an internally generated stream of (timestamp, value) records with a configurable production rate. This is normally used for learning and testing purposes.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Source Classification

* Reliability:
 * Sources are considered reliable when they provide replay capabilities from an offset, even when the structured streaming process fails.
* Classification based on Reliability
 * Reliable
 * File source, Kafka source
 * Unreliable
 * Socket source, Rate source

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## File Source

* The File source is a simple streaming data source that reads files from a monitored directory in a file system.
 * A file-based handover is a commonly used method to bridge a batch-based
* process with a streaming system.

// Use format and path options val fileStream = spark.readStream

.format("parquet")

.option("path", "hdfs://data/exchange")

.schema(schema)

.load()

// Use format and path options fileStream = spark.readStream

.format("parquet")

.option("path", "hdfs://data/exchange")

.schema(schema)

.load()

Formats Supported: CSV, JSON, Parquet, ORC, text and textFile

Options: Parameters (maxFilesPerTrigger, latestFirst , maxFileAge, fileNameOnly etc), Common Text Parsing Options (CSV, JSON), Error Handling, Schema Inference and Parsing.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Defining Schema - 1

In Structured Streaming, we reuse the Spark SQL API for creating

schema definitions.

There are several different methods that we can use to define the schema that defines the content of the stream:

import pyspark

from pyspark.sql import SparkSession

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

. . .

schema = StructType([ 

StructField("id",StringType(),True),  StructField("type",StringType(),True),  StructField("location",StringType(),True),  StructField("latitude", DoubleType(), True),  StructField("longtitute", DoubleType(), True), 

])

df = spark.createDataFrame(data=data,schema=schema)

df.printSchema() df.show(truncate=False)

import org.apache.spark.sql.{StructType, StructField} import org.apache.spark.sql.types.

val schema = StructType( List(

StructField("id", StringType, true), StructField("type", StringType, true), StructField("location", StructType(List ( StructField("latitude", DoubleType, false), StructField("longitude", DoubleType, false)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Defining Schema – 2

import org.apache.spark.sql.Encoders

// Define the case class hierarchy

case class Coordinates(latitude: Double, longitude: Double)

case class Vehicle(id: String, `type`: String, location: Coordinates )

// Obtain the Encoder, and the schema from the Encoder val schema = Encoders.product[Vehicle].schema

3.	Extracted from dataset

val sample = spark.read.parquet(<path-to-sample>) val schema = sample.schema

The programmatic way of defining schemas is powerful but requires effort

and is complex to maintain, often leading to errors. Loading a dataset might

be practical at the prototyping stage. Inference method is mostly preferred when working with Scala.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

To load the SQL StructType schema from JSON file

import json

schemaFromJson = StructType.fromJson(json.loads(schema.json))

dfsample = spark.createDataFrame( spark.sparkContext.parallelize(structureData),schemaFromJson)

dfsample.printSchema()

Creating StructType object struct from DDL String

ddlSchemaStr = "`fullName` STRUCT<`first`: STRING, `last`: STRING,

`middle`: STRING>,`age` INT,`gender` STRING" ddlSchema = StructType.fromDDL(ddlSchemaStr) ddlSchema.printTreeString()

Checking if a Column Exists in a DataFrame

print(df.schema.fieldNames.contains("firstname")) print(df.schema.contains(StructField("firstname",StringType,true)))

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Sink Sources

* Sinks are the abstraction that represents how to produce data to an external system
* Sinks serve as output adaptors between the internal data representation in Structured Streaming and external systems.
 * They provide a write path for the data resulting from the stream processing.
 * They must also close the loop of reliable data delivery.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Output Sinks

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Reliable Sinks

* The sinks considered reliable or production ready provide well- defined data delivery semantics and are resilient to total failure of the streaming process.
* The File sink
 * This writes data to files in a directory in the filesystem. It supports the same file formats as the File source: JSON, Parquet, comma-separated values (CSV), and Text.
* The Kafka sink
 * This writes data to Kafka, effectively keeping the data “on the move.” This is an interesting option to integrate the results of our process with other streaming frameworks that rely on Kafka as the data backbone.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Experimental Sinks

* Experimental sinks are provided to support interaction and experimentation with Structured Streaming.
 * They do not provide failure recovery and therefore their use in production is
* discouraged because it can result in data loss.
* The Memory sink
 * This creates a temporary table with the results of the streaming query. The resulting table can be queried within the same Java virtual machine (JVM) process, which allows in-cluster queries to access the results of the streaming process.
* The Console sink
 * This prints the results of the query to the console. This is useful at development time to visually inspect the results of the stream process.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Tumbling Windows

A tumbling window of 10 seconds over a stream of elements. This illustration demonstrates the tumbling nature of tumbling windows.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Sliding Window

* A sliding window with a window size of 30 seconds and a reporting frequency of 10 seconds.
 * Not defined for periods of time smaller than the size of the window
 * No windows reported for time 00:10 and 00:20

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Stateful vs Stateless Computation (Scala)

scala> val ints = Stream.from(0)

ints: scala.collection.immutable.Stream[Int] = Stream(0, ?)

scala> val fibs = (ints. scanLeft((0, 1) ){ case ((previous, current), index) => (current, (previous + current))})

fibs: scala.collection.immutable.Stream[(Int, Int)] = Stream((0,1), ?) scala> fibs.take(8).print

(0,1), (1,1), (1,2), (2,3), (3,5), (5,8), (8,13), (13,21), empty

scala> fibs.map{ case (x, y) => x}.take(8).print

0, 1, 1, 2, 3, 5, 8, 13, empty

scala> import scala.math.{pow, sqrt} import scala.math.{pow, sqrt}

scala> val phi = (sqrt(5)+1) / 2 phi: Double = 1.618033988749895

scala> def fibonacciNumber(x: Int): Int =

((pow(phi,x) - pow(-phi,-x))/sqrt(5)).toInt

fibonacciNumber: (x: Int)Int

scala> val integers = Stream.from(0)

integers: scala.collection.immutable.Stream[Int] = Stream(0, ?) scala> integers.take(10).print 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, empty scala> val fibonacciSequence = integers.map(fibonacciNumber)

fibonacciSequence: scala.collection.immutable.Stream[Int] = Stream(0, ?) scala>fibonacciSequence.take(8).print

0, 1, 1, 2, 3, 5, 8, 13, empty

Binet formula to compute

the nth element of the sequence directly

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Advanced Spark Structured Streaming

A lot of things look cooler in slow motion. Eating isn't one of them.

Demetri Martin

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Structured Streaming ETL

Ref: https://databricks.com/blog/2017/09/01/streaming-etl-scale-apache-sparks-structured-streaming.html

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Selections and Filtering

All select and filter transformations are supported in Structured Streaming, as are all DataFrame functions and individual column manipulations.

| import org.apache.spark.sql.functions.expr<br />val simpleTransform = streaming.withColumn("stairs", expr("gt like '%stairs%'"))<br />.where("stairs")<br />.where("gt is not null")<br />.select("gt", "model", "arrivaltime", "creationtime") | | |
| :-: | :-: | :-: |
| .writeStream<br />.queryName("simpletransform")<br />.format("memory")<br />.outputMode("append")<br />.start() | dsraw = spark <br />.readStream <br />.format("kafka") <br />.option("kafka.bootstrap.servers", "kafka:9092") | <br /><br /><br /> |
| | .option("subscribe", "my-stream") <br />.option("startingOffsets", "earliest") <br />.load()<br />ds = dsraw.selectExpr("CAST(value AS STRING)") alertQuery = ds <br />.writeStream <br />.queryName("qalerts")<br />.format("memory")<br />.start() | |

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Aggregation

* Structured Streaming has excellent support for arbitrary aggregations
 * Example a cube on the phone model vs activity and the average x, y, z
* accelerations of some sensors

| // in Scala<br />val deviceModelStats = streaming.cube("gt", "model").avg()<br />.drop("avg(Arrivaltime)")<br />.drop("avg(CreationTime)")<br />.drop("avg(Index)")<br />.writeStream.queryName("devicecounts").format("memory").outputMode("complete")<br />.start() | | |
| :-: | :-: | :-: |
| | | |
| | aggDF <br />.writeStream <br />.queryName("aggregates") <br />.outputMode("complete") <br />.("memory") <br />.start() | |

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Joins

Structured Streaming supports joining streaming DataFrames to static DataFrames.

// in Scala

val historicalAgg = static.groupBy("gt", "model").avg() val deviceModelStats = streaming.drop("ArrivalTime",

"CreationTime", "Index")

.cube("gt", "model").avg()

.join(historicalAgg, Seq("gt", "model"))

.writeStream.queryName("devicecounts").format("memory").outputMode("complete")

.start()

* These operations are not directly supported on Streams
 * Count, show, describe, limit, take(n), distinct, foreach, sort and multiple stacked
* aggregations

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Event Time in Structured Stream

* The notion of time is ruled by the internal clock of the computers running any given application
 * Data coming from external devices, such as sensor networks, other datacenters, mobile phones, or
* connected cars, have no guarantees that their clocks are aligned with our cluster of machines

X-axis - the processing time, the clock time of the processing system.

Y-axis represents the internal representation of the event time timeline.

Events are represented with a circle with their corresponding event time label next to them. The event arrival time corresponds to the time on the x-axis.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Window Operations on Event Time

spark.readStream

.format("socket")

.option("host", "localhost")

.option("port", 9999)

.load()

// Split the lines into words

val words = lines.as[String].flatMap(.split(" "))

// Generate running word count

val wordCounts = words.groupBy("value").count()

## Group the data by window and word and compute the count of each group windowedCounts = words 

.withWatermark("timestamp", "10 minutes") 

.groupBy( window(words.timestamp, "10 minutes", "5 minutes"), words.word) 

.count()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Watermarks

* Watermarking lets spark engine automatically track the current event time in the data and attempts to clean up old state accordingly.
 * Watermarks are computed as a threshold based on the internal time representation. They
* are shifted line from the event-time timeline inferred from the event’s time information.

val timeStampEvents = raw.withColumn("timestamp", $"ts".cast(TimestampType))

.withWatermak("timestamp", "5 minutes")

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Handling Late Data and Watermarking

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

import spark.implicits. val words = ...

// streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group

val windowedCounts = words

.withWatermark("timestamp", "10 minutes")

.groupBy(

window($"timestamp", "10 minutes", "5 minutes"),

$"word")

.count()

val timeStampEvents = raw.withColumn("timestamp", $"ts".cast(TimestampType))

.withWatermak("timestamp", "5 minutes")

words = ... ## streaming DataFrame of schema { timestamp: Timestamp, word: String } ## Group the data by window and word and compute the count of each group windowedCounts = words 

.withWatermark("timestamp", "10 minutes") 

.groupBy( window(words.timestamp, "10 minutes", "5 minutes"), words.word) 

.count()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Handling Watermarking and Late Data

Reference: [https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.htm](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html) [l](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)[ ](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)and Spark Documentation

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Time-Based Window Aggregations

* As streams are potentially never-ending, instead of asking “how many X are there?” in a stream-processing context, we are more interested in knowing “how many X were there in 15-minute intervals.”
* Event-time support makes it easy to define and use tumbling and sliding
* window-based operations. A window functions as grouping criteria.
 * val perMinuteAvg = timeStampEvents
* .withWatermak("timestamp", "5 minutes")
* .groupBy(window($"timestamp", "1 minute"))
* .agg(avg($"pressure"))
 * perMinuteAvg.printSchema // let's inspect the schema of our window aggregation
* root
* |-- window: struct (nullable = true)

|-- start: timestamp (nullable = true)

|-- end: timestamp (nullable = true)

|-- pressureAvg: double (nullable = true)

|-- tempAvg: double (nullable = true)

perMinuteAvg.writeStream.outputMode("append").format("console").start()

// after few minutes

// Console Outputs . . .

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Tumbling and Sliding Windows

Window is a SQL function that takes a timeColumn of TimestampType type and additional parameters to specify the duration of the window:

window(timeColumn: Column,

windowDuration: String, slideDuration: String, startTime: String)

window($"timestamp", "5 minutes")

window($"timestamp", "10 minutes", "1 minute")

window($"timestamp", "10 minutes",

"5 minute", "2 minutes")

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Record Deduplication

* Structured Streaming offers a built-in function that removes duplicate records in the stream.
 * Simple Form

val deduplicatedStream = stream.dropDuplicates(<field> , <field>, ...)

Alternative with watermark

val deduplicatedStream = stream

.withWatermark(<event-time-field>, <delay-threshold>)

.dropDuplicates(<field> , <field>, ...)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Twitter Consumer Example

from tweepy.streaming import StreamListener

from tweepy import OAuthHandler

from tweepy import Stream

from kafka import SimpleProducer, KafkaClient accesstoken = “<YOUR TOKEN>” accesstokensecret = “<YOUR TOKEN>” consumerkey =	 “<YOUR TOKEN>” consumersecret =	 “<YOUR TOKEN>”

class StdOutListener(StreamListener):

def ondata(self, data):

producer.sendmessages( “tweets" , data.encode( 'utf-8' ))

print (data)

return True

def onerror(self, status): print (status)

kafka = KafkaClient( "localhost:9092" ) producer = SimpleProducer(kafka)

l = StdOutListener()

auth = OAuthHandler(consumerkey, consumersecret) auth.setaccesstoken(accesstoken, accesstokensecret) stream = Stream(auth, l)

stream.filter(track=[ "GoodFriday" ])

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Stream Machine Learning

Model Building : We build a Logistic Regression Model pipeline to classify whether the tweet contains hate speech or not.

Initialize Spark Streaming Context : Once the model is built, we define the hostname and port number from where we get the streaming data

Stream Data: Next, we add the tweets from the netcat server from the defined port, and the Spark Streaming API will receive the data after a specified duration

Predict and Return Results: Once we receive the tweet text, we pass the data into the machine learning pipeline and return the predicted sentiment from the model

Ref: https[://w](http://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/)ww[.analyti](http://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/)c[svidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/](http://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Logistic Regression Model for Twitter 1

## importing required libraries from pyspark import SparkContext

from pyspark.sql.session import SparkSession from pyspark.streaming import StreamingContext

import pyspark.sql.types as tp from pyspark.ml import Pipeline

from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler

from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer from pyspark.ml.classification import LogisticRegression

from pyspark.sql import Row ## initializing spark session

sc = SparkContext(appName="PySparkShell")

spark = SparkSession(sc) ## define the schema

myschema = tp.StructType([

tp.StructField(name= 'id', tp.StructField(name= 'label', tp.StructField(name= 'tweet',

])

## read the dataset

dataType= tp.IntegerType(), dataType= tp.IntegerType(), dataType= tp.StringType(),

nullable= True), nullable= True), nullable= True)

mydata = spark.read.csv('twittersentiments.csv',

schema=myschema, header=True)

## view the data mydata.show(5)

## print the schema of the file mydata.printSchema()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## define stage 1: tokenize the tweet text

stage1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= 'W') ## define stage 2: remove the stop words

stage2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filteredwords') ## define stage 3: create a word vector of the size 100

stage3 = Word2Vec(inputCol= 'filteredwords', outputCol= 'vector', vectorSize= 100)

## define stage 4: Logistic Regression Model

model = LogisticRegression(featuresCol= 'vector', labelCol= 'label') ## setup the pipeline

pipeline = Pipeline(stages= [stage1, stage2, stage3, model]) ## fit the pipeline model with the training data

pipelineFit = pipeline.fit(mydata)

## define a function to compute sentiments of the received tweets

def getprediction(tweettext):

try:

## filter the tweets whose length is greater than 0

tweettext = tweettext.filter(lambda x: len(x) > 0)

## create a dataframe with column name 'tweet' and each row will contain the tweet rowRdd = tweettext.map(lambda w: Row(tweet=w))

## create a spark dataframe

wordsDataFrame = spark.createDataFrame(rowRdd)

## transform the data using the pipeline and get the predicted sentiment

pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()

except :

print('No data')

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## initialize the streaming context

ssc = StreamingContext(sc, batchDuration= 3)

## Create a DStream that will connect to hostname:port, like localhost:9991 lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))

## split the tweet text by a keyword 'TWEETAPP' so that we can identify which set of words is from a single tweet

words = lines.flatMap(lambda line : line.split('TWEETAPP'))

## get the predicted sentiments for the tweets received words.foreachRDD(getprediction)

## Start the computation ssc.start()

## Wait for the computation to terminate ssc.awaitTermination()

A task is a combination of blocks that are generally ordered, as in any learning process setup:

A StreamReader (reads and parses Examples and creates a stream)

A Learner (provides the train method from an input stream)

A Model (data structure and set of methods used for Learner)

An Evaluator (evaluation of predictions)

A StreamWriter (manages the output of streams)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Wide Landscape of ML . . .

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Mllib and Streaming Algorithms

[The](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[streaming](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[machine learning](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[algorithms (e.g.](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [Streaming ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [Linear](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [Regression](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [,](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression) [Streaming](https://spark.apache.org/docs/2.4.7/mllib-clustering.html#streaming-k-means) [ ](https://spark.apache.org/docs/2.4.7/mllib-clustering.html#streaming-k-means) [KMeans](https://spark.apache.org/docs/2.4.7/mllib-clustering.html#streaming-k-means) [,](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[etc.)](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[can](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[ ](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)[simultaneo](https://spark.apache.org/docs/2.4.7/mllib-linear-methods.html#streaming-linear-regression)usly learn from the streaming data as well as apply the model on the streaming data.

Beyond these, for a much larger class of machine learning algorithms, we can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Final words on challenges. . .

St r eaming Analytics

Challenge #1: Historical Queries? λ-arch Challenge #2: Messy Data? validation

Challenge #3: Mistakes and Failures? reprocessing Challenge #4: Query Performance? compaction

Partition

reprocessing

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Discrete Stream Processing

The key to good decision making is not knowledge... It's whether our work fulfills

us.

Malcolm Gladwell

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## DStreams

* The Spark Streaming Context will create, fill and process DStreams in the Spark cluster.
* DStreams require the declaration of output operations in order to trigger the scheduling of the execution of the DStream transformations.
* The logic of that windowing operation is a logic of grouping over Spark’s
* batches, and it depends on two quantities:
 * The window length: dictates how many total batches, there should be in the amount of data the computation is performed on.
 * The slide interval: indicates how frequently the window should be refreshed with new elements.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Data Sinks

* Data sinks are meant for storing the output of streaming applications.
 * It is important to recognize which sinks can support which output mode and
* whether they are fault tolerant.
 * Kafka sink : require Apache Kafka with version 0.10 or higher. There is a specific set of settings to connect to a Kafka cluster.
 * File sink : This is a destination on a file system, HDFS, or S3. Commonly used file formats are supported, such as text, CSV, JSON, ORC, and Parquet.
 * Foreach sink : This is meant for running arbitrary computations on the rows in the output.
 * Console sink : This is for testing and debugging purposes only and when working with low-volume data. The output is printed to the console on every trigger.
 * Memory sink : This is for testing and debugging purposes only when working with low-volume data. It uses the memory of the driver to store the output.

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Time is on the y-axis, and per-batch execution is on the x-axis.

Reference: [https://spark.apache.org/docs/latest/streaming-programming-guide.htm](https://spark.apache.org/docs/latest/streaming-programming-guide.html) [l](https://spark.apache.org/docs/latest/streaming-programming-guide.html)

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Creating a Streaming Application

Any Streaming application in Spark needs to do four things:

Create a Spark Streaming Context

Define one or several DStreams from Data Sources or other

DStreams

Define one or more output operations to materialize the results of

these DStrea m operations.

Start the Spark Streaming Context

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Sample DStreams App

## Create a local SparkContext and Streaming Contexts

from pyspark import SparkContext

from pyspark.streaming import StreamingContext

## Create sc with two working threads

sc = SparkContext("local[2]", "NetworkWordCount")

## Create local StreamingContextwith batch interval of 1 second

ssc = StreamingContext(sc, 1)

## Create DStream that connects to localhost:9999

lines = ssc.socketTextStream("localhost", 9999)

## Split lines into words

words = lines.flatMap(lambda line: line.split(" "))

## Count each word in each batch

pairs = words.map(lambda word: (word, 1))

wordCounts = pairs.reduceByKey(lambda x, y: x + y)

## Print the first ten elements of each RDD in this DStream

wordCounts.pprint()

## Start the computation

ssc.start()

## Wait for the computation to terminate

ssc.awaitTermination()

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

The art of good decision making is looking forward to and celebrating the tradeoffs,

not pretending they don't exist.

Seth Godin

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## Concluding Remarks

* We explored how Structured Streaming implements the concept of event time and the facilities the API offers to make use of time embedded in the event data:
 * We learned how to use event time and how to fall back on processing time, when needed.
 * We explored watermarks, an important concept that lets us determine which events are too late and when state-related data might be evicted from the store.
 * We saw the different configuration for window operations and their link with event time.
 * Finally, we learned about the deduplication function in how it uses watermarks to keep its state bounded.
* We explored how Discrete Streaming works in Spark

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

Actions are the seed of fate deeds grow into destiny

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

## References

Spark Streaming Guide (Official)

Spark Documentation (Official)

Pro Spark Streaming: The Zen of Real-Time Analytics Using Apache Spark By Zubair Nabi

Learning Real-time Processing with Spark Streaming By Sumit

Gupta

Spark: The Definitive Guide By Bill Chambers and Matei Zaharia

High Performance Spark By Holden Karau and Rachel Warren

Apache Spark 2: Data Processing and Real-Time Analytics By Romeo Kienzler, Md. Rezaul Karim, Sridhar Alla and Siamak Amirghodsi

ATA/S-ARTS/Course Conduct/Real Time Processing

via Apache Spark

Architecting Systems for Real Time Data Processing

© 2019-24, NUS. All Rights Reserved

