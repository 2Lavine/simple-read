## RDD
Resilient Distributed Dataset)叫做弹性分布式数据集
Dataset:一个数据集合，用于存放数据的。 
Distributed:RDD中的数据是分布式存储的，可用于分布式计算。
Resilient:RDD中的数据可以存储在内存中或者磁盘中
## RDD 五大特性
- 可分区、一份 RDD 被分成多个分区
	- 分区规划靠近服务器
	- RDD 可以根据键对数据进行分区
- RDD 的方法作用在所有分区上
	- 元素可并行计算的集合
- RDD直接具有依赖关系


RDD是分布式的列表List或数组Array，但他不可变
抽象的数据结构
- 抽象类Abstract Class和泛型Generic Type
## RDD 编程
入口对象是SparkContext对象
- 他的主要功能就是创建第一个RDD
## RDD创建方式
• 通过并行化集合创建 
- 本地 对象 转 分布式RDD 
- 分布式的开始
• 读取外部数据源 ( 读取文件 )

## RDD算子
方法函数：本地对象的API,叫做方法\函数
算子：分布式对象的API,叫做算子

Transformation算子
- 定义：RDD的算子，返回值仍旧是一个RDD的，称之为转换算子
- 特性：这类算子是Lazy懒加载的.如果没有action算子，Transformation算子是不工作的.
Action算子
- 定义：返回值不是rdd的就是action算子.
- collect()就是一个典型的 action 算子

## RDD 持久化
RDD的数据是过程数据，只在处理的过程中存在，一旦处理完成，就不见了.
这个特性可以最大化的利用资源，老旧RDD没用了就从内存中清理，给后续的计算腾出内存空间.

当我们需要对同一个 RDD 同时做不同操作的时候，为了防止重复计算就需要缓存
## RDD缓存特点
分区自行保存到 内存硬盘上
- 缓存的数据在设计上是认为有丢失风险的.
- 所以缓存需要其保留RDD之间的血缘（依赖）关系
	- 一旦缓存丢失，可以基于血缘关系的记录，重新计算这个RDD的数据
 checkPoint 保存
 - CheckPoint存储RDD数据是集中收集各个分区数据进行存储.而缓存是分散存储
 - 用的时候直接使用rdd.checkpoint()

## 分布式变量问题
Driver 进程上运行一些本地代码
- 他把 RDD 运算交给其他进程
- 其他代码运算本地运行
Executor 进程主要运行 RDD 的一些分布式的运算
- RDD会被分为不同的分区，RDD 的运算是基于分区的
- Exec会把管理多个不同的分区(分为多个线程)

分区线程运算的时候可能会需要 Driver 变量
- 而我们知道进程之间的数据是共享的，但是分区可能会同时需要获取一些外部的数据来进行比较运算，此时同一份进程就有多份数据

## 解决分布式变量问题
1. 广播变量
	1. 将分区需要的变量标记为广播变量对象。
	2. 主动调用这个变量来表示避免重复传递


## SPARK 内核调度
Spark 的任务调度就是如何组织任务去处理RDD中每个分区的数据，
- DAG 调度器
	- 将逻辑的DAG图进行处理，最终得到逻辑上的Task划分
- Task 调度器
	- Task应该在哪些物理的executor上运行，
	- 监控管理TASK的运行

## DAG
有向无环图
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402221717823.png)

 Action是不返回 RDD 的算子，他会把一串 RDD 执行起来
 - 因此一个action 就会产生一个 DAG
 - 一个 DAG 就对应会产生一个 JOB
所以每一个 JOB 会有一个 DAG 图，他是由一个 Action 产生的
RDD总体运算是基于分区的

## DAG 的宽窄依赖
窄依赖
- Input and output stays in same partition
- 父RDD的一个分区，
	- 分区将所有数据发给子RDD的一个分区
宽依赖
- 父RDD的一个分区，
	- 分区将数据发给子RDD的多个分区
- 宽依赖还有一个别名：shuffle
即数据传递时
- 只给一个分区时窄依赖
- 给多个分区时宽依赖

## DAG 的阶段划分
DAG 会从后向前，遇到宽依赖就划分出一个阶段.
- 称之为stage
- stage 内部一定都是窄依赖
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402221728055.png)

## 内存迭代计算：窄依赖与运算
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402221736686.png)

由于窄依赖是线性的，所以我们可以把这一条运算由一个task(线程完成）减少通信之间的消耗
- 这一阶段的这一条线，就是内存计算管道，是纯内存计算.
- 如上图，task1 task2 task3,就形成了三个并行的内存计算管道.
因此我们可以得到
- 一个 task来负责一个分区，他是一个线程
- 就是一条窄依赖链，
- 我们根据窄依赖链划分内存计算管道

## SPARK 并行度
Spark的并行：在同一时间内，有多少个task在同时运行
- 在有了6个task并行的前提下，说明我们至多有 6 个窄依赖链
- rdd的分区就被规划成6个分区了.

SPARK 并行度和 exector 数量是两个不同的概念
- 一个 exector 可以有多个 task 同时运行
- 为了避免 task 线程之间通信损耗，建议一个服务器一个 exector
	- 不然同一个服务器有多个 exector 进程，他们之间的 task 走的不是进程内存交换而是本地网络通信
## 集群中如何规划并行度
结论：设置为CPU总核心的2~10倍
比如集群可用CPU核心是100个，我们建议并行度是200~1000
- 过低会导致一些 task执行时间过长
	- 概率上导致 各个task 之间时间波动大
	- 导致 cpu核心空闲时间大
	- task 时间变短可以让空闲时间变小
- 过高会导致 各个线程之间管理通信资源变大
## 为什么不在 RDD 上再设置并行度
回导致增加宽依赖，从而导致窄依赖链变短
## SPARK为什么 比 MAPreduce 块
- 编程模型上Spark占优（算子够多）
- 算子交互上，和计算上可以尽量多的内存计算而非磁盘迭代