> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.zhihu.com](https://www.zhihu.com/question/318091919/answer/2376806633?utm_id=0) 


**5. 哪些地方会使用缓存（Where）**
-----------------------
**二、内容分发**
在 DNS 服务器的协助下，无论是对用户还是服务器，内容分发网络都可以是完全透明的，在两者都不知情的情况下，由 CDN 的缓存节点接管了用户向服务器发出的资源请求。

问题是缓存节点中必须有用户想要请求的资源副本，才可能代替源站来响应用户请求。CDN 该“如何获取源站资源” 和 “如何管理（更新）资源”。

CDN 获取源站资源的过程被称为 “内容分发”，“内容分发网络” 的名字正是由此而来，可见这是 CDN 的核心价值。目前主要有以下两种主流的内容分发方式：
*  主动分发（Push）：分发由源站主动发起，将内容从源站或者其他资源库推送到用户边缘的各个 CDN 缓存节点上。这个推送的操作没有什么业界标准可循，可以采用任何传输方式（HTTP、FTP、P2P，等等）、任何推送策略（满足特定条件、定时、人工，等等）、任何推送时间，只要与后面说的更新策略相匹配即可。由于主动分发通常需要源站、CDN 服务双方提供程序 API 接口层面的配合，所以它对源站并不是透明的，只对用户一侧单向透明。主动分发一般用于网站要预载大量资源的场景。比如双十一之前一段时间内，淘宝、京东等各个网络商城就会开始把未来活动中所需用到的资源推送到 CDN 缓存节点中，特别常用的资源甚至会直接缓存到你的手机 APP 的存储空间或者浏览器的 [localStorage](https://en.wikipedia.org/wiki/Web_storage#localStorage) 上。
*   被动回源（Pull）：被动回源由用户访问所触发全自动、双向透明的资源缓存过程。当某个资源首次被用户请求的时候，CDN 缓存节点发现自己没有该资源，就会实时从源站中获取，这时资源的响应时间可粗略认为是资源从源站到 CDN 缓存节点的时间，再加上资源从 CDN 发送到用户的时间之和。因此，被动回源的首次访问通常是比较慢的（但由于 CDN 的网络条件一般远高于普通用户，并不一定就会比用户直接访问源站更慢），不适合应用于数据量较大的资源。被动回源的优点是可以做到完全的双向透明，不需要源站在程序上做任何的配合，使用起来非常方便。这种分发方式是小型站点使用 CDN 服务的主流选择，如果不是自建 CDN，而是购买阿里云、腾讯云的 CDN 服务的站点，多数采用的就是这种方式。

对于 “CDN 如何管理（更新）资源” 这个问题，同样没有统一的标准可言，尽管在 HTTP 协议中，关于缓存的 Header 定义中确实是有对 CDN 这类共享缓存的一些指引性参数，比如 “浏览器” 小节 HTTP header 参数 Cache-Control 的 s-maxage，但是否要遵循，完全取决于 CDN 本身的实现策略。
现在，最常见的做法是超时被动失效与手工主动失效相结合。超时失效是指给予缓存资源一定的生存期，超过了生存期就在下次请求时重新被动回源一次。而手工失效是指 CDN 服务商一般会提供给程序调用来失效缓存的接口，在网站更新时，由持续集成的流水线自动调用该接口来实现缓存更新。

**四、CDN 的应用场景**
*   加速静态资源：这是 CDN 最普遍的应用场景。
*   安全防御：CDN 在广义上可以视作网站的堡垒机，源站只对 CDN 提供服务，由 CDN 来对外界其他用户服务，这样恶意攻击者就不容易直接威胁源站。CDN 对某些攻击手段的防御，如对 [DDoS 攻击](https://zh.wikipedia.org/zh-tw/%E9%98%BB%E6%96%B7%E6%9C%8D%E5%8B%99%E6%94%BB%E6%93%8A)的防御尤其有效。但需注意，将安全都寄托在 CDN 上本身是不安全的，一旦源站真实 IP 被泄漏，就会面临很高的风险。
*   协议升级：不少 CDN 提供商都同时对接（代售 CA 的）SSL 证书服务，可以实现源站是 HTTP 协议的，而对外开放的网站是基于 HTTPS 的。同理，可以实现源站到 CDN 是 HTTP/1.x 协议，CDN 提供的外部服务是 HTTP/2 或 HTTP/3 协议、实现源站是基于 IPv4 网络的，CDN 提供的外部服务支持 IPv6 网络，等等。
*   状态缓存：CDN 不仅可以缓存源站的资源，还可以缓存源站的状态，比如源站的 301/302 转向就可以缓存起来让客户端直接跳转、还可以通过 CDN 开启 [HSTS](https://es.wikipedia.org/wiki/HTTP_Strict_Transport_Security)、可以通过 CDN 进行 [OCSP 装订](https://zh.wikipedia.org/wiki/OCSP%E8%A3%85%E8%AE%A2)加速 SSL 证书访问等。有一些情况下甚至可以配置 CDN 对任意状态码（比如 404）进行一定时间的缓存，以减轻源站压力，但这个操作应当慎重，在网站状态发生改变时去及时刷新缓存。
*   修改资源：CDN 可以在返回资源给用户的时候修改它的任何内容，以实现不同的目的。比如，可以对源站未压缩的资源自动压缩并修改 Content-Encoding，以节省用户的网络带宽消耗、可以对源站未启用客户端缓存的内容加上缓存 Header，自动启用客户端缓存，可以修改 [CORS](https://developer.mozilla.org/zh-CN/docs/Glossary/CORS) 的相关 Header，将源站不支持跨域的资源提供跨域能力等。
*   访问控制：CDN 可以实现 IP 黑 / 白名单功能，根据不同的来访 IP 提供不同的响应结果，根据 IP 的访问流量来实现 QoS 控制、根据 HTTP 的 Referer 来实现防盗链等。
*   注入功能：CDN 可以在不修改源站代码的前提下，为源站注入各种功能。
### **3. 服务端缓存**

服务端缓存主要是为了**减少 CPU/IO、数据库以及下游服务接口的压力**，这也是实际编程中最常用的手段。

除减少数据库的压力外，缓存返回数据的响应速度比数据库要快。另外，尽可能不调用外部接口，因为外部接口无论 WebSocket、WebService，还是 HTTP，其响应速度都是不可控的。如果外部接口响应时间过长，也会影响自身性能。

服务端缓存大致分为以下几种：

*   容器缓存，如 Nginx、Tomcat 等。
*   中间件缓存，如 MongoDB、Elasticsearch、Redis、ZooKeeper、Kafka 等。
*   页面静态化缓存，如 FreeMaker、Thymeleaf 等。
*   文件管理，如 FastDFS 等。

**6. 缓存必知必会**
-------------

### **6.1 缓存击穿**

缓存击穿是一个**失效**的热点 Key 被并发集中访问，导致请求全部打在数据库上。图如下

![](_resources/v2-2e645e6edc776a92cde10b9f49138cd0_r.jpg)

**解决方案：**

1.  **热点 key 缓存不失效：**对热点 key 可以设置**永不过期**。
2.  **使用互斥锁或堵塞队列：**这样可以控制数据库的线程访问数，减小数据库的压力，但也会让系统吞吐率有所下降。实现流程如下：

*   阻塞当前 Key 的请求
*   从后端存储恢复数据
*   在 Key 对应的 Value 还未恢复的过程中， 如果有其他请求继续获取该 Key， 同样阻塞该请求
*   当 key 从后端恢复后，依次唤醒该 Key 对应阻塞的请求

用堵塞队列来实现的话可以参考 Golang 官方库 **[singleflight](https://pkg.go.dev/golang.org/x/sync@v0.0.0-20210220032951-036812b2e83c/singleflight)**

1.  热点数据由代码来手动管理，缓存击穿是仅针对热点数据被自动失效才引发的问题，对于这类数据，可以直接由开发者通过代码来有计划地完成更新、失效，避免由缓存策略来自动管理。

### **6.2 缓存雪崩**
某个时刻热点数据出现大规模的[缓存失效](https://www.zhihu.com/search?q=%E7%BC%93%E5%AD%98%E5%A4%B1%E6%95%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A346651831%7D)，大量的请求全部打到数据库，导致数据库瞬时压力过载从而拒绝服务甚至是宕机。

![](_resources/v2-6829f56cb97efe528bf3561f71239218_r.jpg)

**分析：**造成缓存雪崩的关键在于在同一时间大规模的 key 失效。诱因可能是

1.  大量热点数据设置了相同或相近的过期时间。
2.  缓存组件不可用，比如 redis 宕机了。

**解决方案：**

1.  **打散缓存失效时间**：主要是通过对 key 的 TTL 增加随机数去尽量规避，过期时间则需要根据业务场景去设置。
2.  **使用多级缓存**：这里有个 [github 开源库: hybridcache](https://github.com/cshum/hybridcache) 可以借鉴下
3.  **兜底逻辑使用熔断机制**。防止过多请求同时打到 DB。  
    在触发熔断机制后返回预先配置好的兜底数据，减少过多请求压倒 DB 导致服务不可用。  
    

![](_resources/v2-454260dc4252446e4c7e5ab2532484e5_r.jpg)

1.  **组件高可用：**  
*   对于 redis 这样的缓存组件，可以搭建 Redis 集群（集群模式或哨兵模式），提高 Redis 的可用性，尽量规避单点故障导致缓存雪崩。  
    

![](_resources/v2-e13578b291a8d11c650ee752250680d4_r.jpg)

  
`Redis`基于一个`Master`主节点多`Slave`从节点的模式和`Redis`持久化机制，将一份数据保持在多个实例中，从而实现**增加副本冗余量**，又使用哨兵机制实现主备切换， 在`master`故障时，自动检测，将某个`slave`切换为`master`，最终实现`Redis`高可用  
*   提高数据库的容灾能力，可以使用[分库分表](https://www.zhihu.com/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A346651831%7D)，读写分离的策略。  

### **6.3 缓存穿透**

缓存穿透是指用户查询**数据库没有的数据**，缓存中自然也不会有。那先查缓存再查数据相当于进行了两次无效操作。大量的无效请求将给数据库带来极大的访问压力，甚至导致其过载拒绝服务。下图中**红色箭头**标出了每一次用户请求到来都会经过的两次无效查询。

![](_resources/v2-a0607ef2f161478d729b84060ac0098e_r.jpg)

**分析：**造成缓存穿透的原因可能是

1.  空数据查询 (黑客攻击)，空数据查询通常指攻击者伪造大量不存在的数据进行访问（比如不存在的商品信息、用户信息）
2.  缓存污染（网络爬虫），缓存污染通常指在遍历数据等情况下冷数据把热数据驱逐出内存，导致**缓存了大量冷数据而热数据被驱逐**。

**解决方案：**

1.  对于空数据查询：  
    **1）使用布隆过滤器高效判断 key 是否存在**，流程如如下  
    

![](_resources/v2-fbcad9b72dcb9f6dfd6028723fbad1ce_r.jpg)

  
虽然不能完全避免数据穿透的现象，但已经可以将 99% 的穿透查询给屏蔽在 Redis 层了，极大的降低了底层数据库的压力，减少了资源浪费（布隆过滤器用 bitmap 实现）。如果想拥有更高的空间利用率和更小的误判，替代的方案是使用布谷鸟过滤器。如果想了解这两种过滤器的相关细节，强烈推荐看这篇文章：[作为一名后台开发，你必须知道的两种过滤器](https://mp.weixin.qq.com/s/PR0tuLY_Bgd5rujDBQR_Xg?st=F4732020A51AE1BEB4D0C5B530A1B6A1221CB70F4610E1C82A2DDEF08D8D3F4ACAAFED5D9B4CA7A0214E39DE4D0C8953BAAB715A05F0180D46EA1DB4EBC63D5388C295B048BCE5177B14C93245B696480D4F0175DC2E8128F239C2F157BF215F5EB5AEE069D53A387C5160E77E916232F9457EBDF513A1542FCDC218DC559BDD823E36A139C82BB8F36FB9ECDEA1088459BC065556C8CCC8BB7B528F3EDAC17A7EB9D3B1BDE4A1A8CAED64F9CDB15549B9A0E852FA388AA2985B9FF6314FD226AD65BDDD0163FF4CA0E0B929C9B148DB&vid=1688850571937661&cst=1A883B3F08702C0F9258E961CD103DBFFA1538968A07443A551F1F6FB8D037792DE0CE95437EF42AC8CCD8D064FD2EB3&deviceid=afebe3a4-2eec-4bc0-9bec-ef903925e222&version=4.0.0.90428&platform=mac)  
由于布隆过滤器存在 “误报” 和“漏报”，而且实现也比较复杂，对于空数据查询其实还有一种简单粗暴的策略：  
**2）缓存空值**，流程图如下：  

![](_resources/v2-aa5406d37b61e8c176266108983c9a97_r.jpg)

  
当碰到查询结果为空的 key 时，放一个空值到缓存中，下次再访问就知道此 key 是无效的，避免无效查询数据库。但这样花费额外的空间来存储空值。  

1.  对于[缓存污染](http://118.25.23.115/redis/27-%E7%BC%93%E5%AD%98%E8%A2%AB%E6%B1%A1%E6%9F%93%E4%BA%86%EF%BC%8C%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F.html)：关键点是**能识别出只访问一次或者访问次数很少的数据**。然后使用淘汰策略去删除冷数据，下面以 redis 为例  
    

<table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th>缓存淘汰策略</th><th>解决缓存污染</th></tr></tbody></table>

*   noeviction 策略：不会淘汰数据，解决不了。
*   volatile-ttl 策略：给数据设置合理的过期时间。当缓存写满时，会淘汰剩余存活时间最短的数据，避免滞留在缓存中从而造成污染。
*   volatile-random 策略：随机选择数据，无法把不再访问的数据筛选出来，会造成缓存污染。
*   volatile-lru 策略：LRU 策略只考虑数据的访问时效，对只访问一次的数据，不能很快筛选出来。
*   volatile-lfu 策略：LFU 策略在 LRU 策略基础上进行了优化，筛选数据时优先筛选并淘汰访问次数少的数据。
*   allkeys-random 策略：随机选择数据，无法把不再访问的数据筛选出来，会造成缓存污染。
*   allkeys-lru 策略：LRU 策略只考虑数据的访问时效，对只访问一次的数据，不能很快筛选出来。
*   allkeys-lfu 策略：LFU 策略在 LRU 策略基础上进行了优化，筛选数据时优先筛选并淘汰访问次数少的数据。

### **6.4 缓存预热**

缓存预热是指系统上线后，提前将热点数据加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，在线上高并发访问时可以提高数据访问速度和减小数据库压力。

**解决方案：**

1.  对于单点缓存

*   写个缓存刷新页面，上线时手工操作。
*   数据量不大时，可以在程序启动时加载。
*   用定时器定时刷新缓存，或者模拟用户触发。

1.  对于分布式缓存系统，如 Redis

*   写程序或脚本往缓存中加载热点数据。
*   使用缓存预热框架。

### **6.5 缓存降级**

缓存降级是指当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，即使是有损部分其他服务，仍然需要保证主服务可用。可以将其他次要服务的数据进行缓存降级，从而提升主服务的稳定性。降级的目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。

以参考日志级别设置预案：

（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；

（2）警告：有些服务在一段时间内成功率有波动（如在 95~100% 之间），可以自动降级或人工降级， 并发送告警；

（3）错误：比如可用率低于 90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；

（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。

服务降级的目的，是为了防止 Redis 服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis 出现问题，不去数据库查询，而是直接返回默认值给用户。

### **6.6 缓存更新模式**

### **1. Cache-Aside 模式**

Cache-Aside 是最常用的一种缓存更新模式，其逻辑如下：

*   **失效**：应用程序先从 cache 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。（图 1）
*   **命中**：应用程序从 cache 中取数据，取到后返回。（图 1）
*   **更新**：先把数据存到数据库中，成功后，再让缓存失效。（图 2）

![](_resources/v2-a6edb9c66aa778cb2c75be9474d85a60_r.jpg)

**注意，Cache-Aside 更新操作是先更新数据库，成功后，让缓存失效**。

如果是**先删除缓存，然后再更新数据库**，会有什么影响呢？

我们可以思考一下，假设有两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。

那么 Cache-Aside 是否会出现上面提到的问题呢？

我们可以脑补一下，假如有两个并发操作：查询和更新，首先，没有了删除 cache 数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。就不会出现后续的查询操作一直都在取老的数据的问题。

这是标准的 design pattern，包括 Facebook 的论文《[Scaling Memcache at Facebook](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下 Quora 上的这个问答《[Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?](https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend)》，主要是怕两个并发的写操作导致脏数据。

那么，是不是 Cache Aside 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

这种 case 理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

**所以，这也就是 Quora 上的那个答案里说的，要么通过 2PC 或是 Paxos 协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而 Facebook 使用了这个降低概率的玩法，因为 2PC 太慢，而 Paxos 太复杂。当然，最好还是为缓存设置上过期时间。**

### **2. Read/Write Through 模式**

在上面的 Cache Aside 中，应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序代码比较复杂。而 Read/Write Through 是把更新数据库（Repository）的操作由缓存服务自己代理，对于应用层来说，就简单很多了。**可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 Cache。**

### **2.1 Read-Through 模式**

Read-Through 策略是当缓存失效时（过期或 LRU 换出），**缓存服务自己**从数据库加载丢失的数据，填充缓存并将其返回给应用程序。

![](_resources/v2-c84087a092d1404bfbf01619df34b9d6_r.jpg)

Cache-Aside 和 Read-Through 策略都是**延迟加载数据**，也就是说，只有在第一次读取时才加载数据。

Read-Though 和 Cache-Aside 的区别：

1.  Cache-Aside 策略是应用程序负责从数据库中获取数据并填充到缓存。此逻辑在 Read-Though 中通常由库或独立缓存服务提供支持
2.  与 Cache-Aside 不同，Read-Through cache 中的数据模型不能与数据库中的数据模型不同。

### **2.2 Write-Through 模式**

Write Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作）。

![](_resources/v2-c6449b24cf35066e4bb2df3240a8b87c_r.jpg)

就其本身而言，Write-Through 似乎没有太大作用，而且它会引入额外的写入延迟，因为数据首先写入缓存，然后写入主数据库。它一般和 Read-Though 配对使用，这样可以获得数据一致性保证而且免于应用程序和缓存失效、更新、集群管理等复杂的问题。

[DynamoDB Accelerator (DAX)](https://aws.amazon.com/cn/dynamodb/dax/) 是通读 / 直写缓存的一个很好的例子。 它连接 DynamoDB 和你的应用程序。 对 DynamoDB 的读取和写入可以通过 DAX 完成。

### **3. Write Behind Caching 模式**

Write Behind 也叫 Write Back。**其实 Linux 文件系统的 Page Cache 算法用的也是这个，所以说底层的东西很多时候是相通。**

Write Behind 的思想是：在更新数据时，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。

![](_resources/v2-63c0d0e1b83a9a26955064f58a35f831_r.jpg)

Write Behind 的优势有：

1.  **直接操作内存，数据的 I/O 操作速度极快。**
2.  **更新数据库时可以异步更新，可以合并对同一数据的多次操作，性能强悍**。

事物总有两面性，Write Behind 的不足有：

1.  **数据不能保证强一直性，而且可能会丢失。**比如在 Linux/Unix 系统下突然关机会导致数据丢失。
2.  **实现逻辑比较复杂。因为需要监视哪些数据发生被更新了，并且在适当的时机持久化这些数据。**比如操作系统的 Write Behind 会在仅当这个 cache 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write

没有完美的方案，只有合适的方案。我们基本上不可能做出一个没有缺陷的设计，软件设计从来都是取舍 Trade-Off。比如算法设计中的时间换空间、空间换时间，有时候软件的强一致性和高性能不可兼得，高可用和高性能会有冲突等。

缓存更新这个章节，我们都没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新 Cache 成功，更新数据库失败了怎么吗？对于事务、分布式事务这个庞大的话题，大家如果有兴趣的话欢迎私信我，后续我再出几篇文章来分析、总结和归纳下。

### **6.7 缓存数据库更新一致性问题**

1.  概念介绍

*   双写一致性: 更新数据库后保证 redis 缓存同步更新

1.  读操作执行过程:

*   命中: 读操作先查询 cache, 命中则直接返回缓存数据
*   失效: 读操作先查询 cache,cache 失效或过期导致未命中, 则从数据库中读, 写到缓存后返回

1.  写操作执行过程:

3.1 写操作更新缓存 (更新缓存: 先更新数据库还是缓存?)

*   先更新数据库后更新缓存

A 线程更新数据库后还没更新缓存时, B 线程覆盖掉 A 的数据库值, 然后 A 后写入缓存覆盖掉 B 的缓存值, 导致数据不一致

*   先更新缓存后更新数据库

更新缓存成功, 但是更新数据库失败导致数据不一致

3.2 写操作删除缓存 (删除缓存: 先删还是后删缓存?)

*   先写数据库后删缓存 (facebook 的缓存)

写入数据库未删缓存之前的缓存命中会是旧值 (数据不一致时间很短)

A 线程的缓存未命中读取数据库旧值后 B 线程写入数据库并删除缓存 (这里没有删除掉缓存, 因为缓存不存在, 如果存在 A 线程不可能缓存未命中), 之后 A 线程才把旧值写入缓存导致数据不一致 (概率小: 读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存, 但是读操作时间比写操作时间快很多)

*   先删缓存后写数据库

A 线程删掉缓存后未写入数据库之前, B 线程读取数据旧值, A 线程写入数据后, B 更新缓存, 导致数据不一致性

解决办法: **延时双删，延时双删策略会在写库前后删除缓存中的数据，并且给缓存数据设置合理的过期时间，从而可以保证最终一致性**。**写流程具体如下**：

1.  先删除缓存，再写数据库
2.  休眠一段时间（比如 500ms）
3.  再次删除缓存

![](_resources/v2-0c0c2c2550a1f56b6275172a1213f891_r.jpg)

这里为什么会休眠一段时间呢？这里主要是防止：在写请求删除缓存但还未成功写入数据库后，读请求可能将旧值加载到缓存。

**读流程：**先读缓存，当缓存未命中，再从数据库中读取，然后再写入缓存。

延时双删虽然解决了上述讨论的缓存对数据库不一致的问题，但这种策略存在以下问题：

1.  休眠一段时间可能会对性能造成影响
2.  在第二次缓存删除失败后，会导致数据不一致，需要业务方实现重试机制。

**休眠时间需要业务方设置 (难点在时间怎么设置), 并且要大于一次读操作时间, 才能在旧值加载到就缓存之后二次顺利删掉缓存。**

### **6.8 缓存过期**

缓存过期是个很复杂的问题。没有解决这一问题的 “灵丹妙药”，但有几个简单的策略可供参考：

*   **始终对所有缓存键设置生存时间 (TTL)**，但 Write-Through 缓存更新策略 (6.6 节中有讲到) 除外。可以将生存时间设置成很长的时间，例如数小时，甚至数天。这种方法能够捕获应用程序错误，例如在更新底层数据库时，忘记更新或删除给定的缓存键。最终，缓存键会自动过期并刷新。
*   **对于频繁更改的数据，只需设置较短的 TTL (几秒钟) 即可**。例如评论、排行榜、活动流等，不要添加 Write-Through 缓存或复杂的过期逻辑。如果某条数据库查询在生产环境中被大量访问，只需改动几行代码就能为此查询添加 TTL 为 5 秒的缓存键。在你评估更优雅的解决方案时让你的应用程序保持正常运行。
*   Ruby on Rails 团队研究出了一种更新的模式 - 俄罗斯套娃缓存。在这种模式下，嵌套记录通过其自有缓存键进行管理，顶层资源就是这些缓存键的集合。假设您有一个包含用户、文章和评论的新闻网页。在这种方法中，他们中的每个都是自己的缓存键，页面则分别查询每个键。
*   **若不确定缓存键是否会在数据库更新时受到影响，只需删除此缓存键**。如果你使用的是延迟更新策略 (Cache-Aside、Write-Through) 会在需要时刷新此键。

有关缓存过期和俄罗斯套娃缓存的详细介绍，请参阅 Basecamp Signal vs Noise 博客文章 [“俄罗斯套娃” 缓存的性能影响](https://signalvnoise.com/posts/3690-the-performance-impact-of-russian-doll-caching)。

还可以使用另一种模式以在下游服务不可用时一定程度提高服务的弹性，也就是使用两个 TTL：一个软 TTL 和一个硬 TTL。客户端将尝试根据软 TTL 刷新缓存项，但如果下游服务不可用或因其他原因未响应请求，则将继续使用现有的缓存数据，直至达到硬 TTL

### **6.9 缓存淘汰**

缓存回收策略可以分为：

*   **基于时间**：当某缓存超过生存时间时，则进行缓存回收。或者当某缓存最后被访问后超过某时间仍然没有被访问，则进行缓存回收。
*   **基于空间**：当缓存超过某大小时，则进行缓存回收。
*   **基于容量**：当缓存超过某存储条数时，则进行缓存回收。

缓存淘汰算法有：

*   **FIFO**（First In First Out）：优先淘汰最早进入被缓存的数据。FIFO 实现十分简单，但一般来说它并不是优秀的淘汰策略，越是频繁被用到的数据，往往会越早被存入缓存之中。如果采用这种淘汰策略，很可能会大幅降低缓存的命中率。
*   **LRU**（Least Recent Used）：优先淘汰最久未被使用访问过的数据。LRU 通常会采用 HashMap 加双端链表的双重结构来实现（如 LinkedHashMap），以 HashMap 来提供访问接口，保证常量时间复杂度的读取性能，以 LinkedList 的链表元素顺序来表示数据的时间顺序，每次缓存命中时把返回对象调整到 LinkedList 开头，每次缓存淘汰时从链表末端开始清理数据。对大多数的缓存场景来说，LRU 都明显要比 FIFO 策略合理，**尤其适合用来处理短时间内频繁访问的热点对象**。但相反，它的问题是如果一些热点数据在系统中经常被频繁访问，但最近一段时间因为某种原因未被访问过，此时这些热点数据依然要面临淘汰的命运，LRU 依然可能错误淘汰价值更高的数据。
*   **LFU**（Least Frequently Used）：优先淘汰最不经常使用的数据。LFU 会给每个数据添加一个访问计数器，每访问一次就加 1，需要淘汰时就清理计数器数值最小的那批数据。LFU 可以解决上面 LRU 中热点数据间隔一段时间不访问就被淘汰的问题，但同时它又引入了两个新的问题，首先是需要对每个缓存的数据专门去维护一个计数器，每次访问都要更新，多线程并发更新要加锁就会带来高昂的开销；另一个问题是不便于处理随时间变化的热度变化，譬如某个曾经频繁访问的数据现在不需要了，它也很难自动被清理出缓存。

缓存淘汰策略直接影响缓存的命中率，没有一种策略是完美的、能够满足全部系统所需的。不过，随着淘汰算法的发展，近年来的确出现了许多相对性能要更好的，也更为复杂的新算法。以 LFU 分支为例，针对它存在的两个问题，近年来提出的 TinyLFU 和 W-TinyLFU 算法就往往会有更好的效果。

*   **TinyLFU**（Tiny Least Frequently Used）：TinyLFU 是 LFU 的改进版本。为了缓解 LFU 每次访问都要修改计数器所带来的性能负担，TinyLFU 会首先采用 Sketch 对访问数据进行分析，所谓 Sketch 是统计学上的概念，指用少量的样本数据来估计全体数据的特征，这种做法显然牺牲了一定程度的准确性，但是只要样本数据与全体数据具有相同的概率分布，Sketch 得出的结论仍不失为一种高效与准确之间权衡的有效结论。借助 [Count–Min Sketch](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch) 算法（可视为[布隆过滤器](https://en.wikipedia.org/wiki/Bloom_filter)的一种等价变种结构），TinyLFU 可以用相对小得多的记录频率和空间来近似地找出缓存中的低价值数据。为了解决 LFU 不便于处理随时间变化的热度变化问题，TinyLFU 采用了基于 “滑动时间窗” 的热度衰减算法，简单理解就是每隔一段时间，便会把计数器的数值减半，以此解决 “旧热点” 数据难以清除的问题。
*   **[W-TinyLFU](https://arxiv.org/pdf/1512.00727.pdf)**（Windows-TinyLFU）：W-TinyLFU 又是 TinyLFU 的改进版本。TinyLFU 在实现减少计数器维护频率的同时，也带来了无法很好地应对稀疏突发访问的问题，所谓稀疏突发访问是指有一些绝对频率较小，但突发访问频率很高的数据，譬如某些运维性质的任务，也许一天、一周只会在特定时间运行一次，其余时间都不会用到，此时 TinyLFU 就很难让这类元素通过 Sketch 的过滤，因为它们无法在运行期间积累到足够高的频率。应对短时间的突发访问是 LRU 的强项，W-TinyLFU 就结合了 LRU 和 LFU 两者的优点，从整体上看是它是 LFU 策略，从局部实现上看又是 LRU 策略。具体做法是将新记录暂时放入一个名为 Window Cache 的前端 LRU 缓存里面，让这些对象可以在 Window Cache 中累积热度，如果能通过 TinyLFU 的过滤器，再进入名为 Main Cache 的主缓存中存储，主缓存根据数据的访问频繁程度分为不同的段（LFU 策略，实际上 W-TinyLFU 只分了两段），但单独某一段局部来看又是基于 LRU 策略去实现的（称为 Segmented LRU）。每当前一段缓存满了之后，会将低价值数据淘汰到后一段中去存储，直至最后一段也满了之后，该数据就彻底清理出缓存。

另外还有两种高级淘汰策略 [ARC](https://en.wikipedia.org/wiki/Adaptive_replacement_cache)（Adaptive Replacement Cache）、[LIRS](https://en.wikipedia.org/wiki/LIRS_caching_algorithm)（Low Inter-Reference Recency Set）, 大家有兴趣可以再深入阅读，对其他缓存淘汰策略感兴趣的读者也可以参考维基百科中对 [Cache Replacement Policies](https://en.wikipedia.org/wiki/Cache_replacement_policies) 的介绍。

### **6.10 分布式缓存**

相比起缓存数据在进程内存中读写的速度，一旦涉及网络访问，由网络传输、数据复制、序列化和反序列化等操作所导致的延迟要比内存访问高得多，所以对分布式缓存来说，处理与网络有相关的操作是对吞吐量影响更大的因素，往往也是比淘汰策略、扩展功能更重要的关注点，这决定了尽管也有 Ehcache、Infinispan 这类能同时支持分布式部署和进程内嵌部署的缓存方案，但通常进程内缓存和分布式缓存选型时会有完全不同的候选对象及考察点。我们决定使用哪种分布式缓存前，首先必须确认自己需求是什么？

*   从访问的角度来说，如果是频繁更新但甚少读取的数据，通常是不会有人把它拿去做缓存的，因为这样做没有收益。对于甚少更新但频繁读取的数据，理论上更适合做复制式缓存；对于更新和读取都较为频繁的数据，理论上就更适合做集中式缓存。笔者简要介绍这两种分布式缓存形式的差别与代表性产品：  
    

*   **复制式缓存**：复制式缓存可以看作是 “能够支持分布式的进程内缓存”，它的工作原理与 Session 复制类似。缓存中所有数据在分布式集群的每个节点里面都存在有一份副本，读取数据时无须网络访问，直接从当前节点的进程内存中返回，理论上可以做到与进程内缓存一样高的读取性能；当数据发生变化时，就必须遵循复制协议，将变更同步到集群的每个节点中，复制性能随着节点的增加呈现平方级下降，变更数据的代价十分高昂。 复制式缓存的代表是 [JBossCache](https://jbosscache.jboss.org/)，这是 JBoss 针对企业级集群设计的缓存方案，支持 JTA 事务，依靠 JGroup 进行集群节点间数据同步。以 JBossCache 为典型的复制式缓存曾有一段短暂的兴盛期，但今天基本上已经很难再见到使用这种缓存形式的大型信息系统了，JBossCache 被淘汰的主要原因是写入性能实在差到不堪入目的程度，它在小规模集群中同步数据尚算差强人意，但在大规模集群下，很容易就因网络同步的速度跟不上写入速度，进而导致在内存中累计大量待重发对象，最终引发 OutOfMemory 崩溃。如果对 JBossCache 没有足够了解的话，稍有不慎就要被埋进坑里。 为了缓解复制式同步的写入效率问题，JBossCache 的继任者 [Infinispan](https://infinispan.org/) 提供了另一种分布式同步模式（这种同步模式的名字就叫做 “分布式”），允许用户配置数据需要复制的副本数量，譬如集群中有八个节点，可以要求每个数据只保存四份副本，此时，缓存的总容量相当于是传统复制模式的一倍，如果要访问的数据在本地缓存中没有存储，Infinispan 完全有能力感知网络的拓扑结构，知道应该到哪些节点中寻找数据。
*   **集中式缓存**：集中式缓存是目前分布式缓存的主流形式，集中式缓存的读、写都需要网络访问，其好处是不会随着集群节点数量的增加而产生额外的负担，其坏处自然是读、写都不再可能达到进程内缓存那样的高性能。 集中式缓存还有一个必须提到的关键特点，它与使用缓存的应用分处在独立的进程空间中，其好处是它能够为异构语言提供服务，譬如用 C 语言编写的 [Memcached](https://memcached.org/) 完全可以毫无障碍地为 Java 语言编写的应用提供缓存服务；但其坏处是如果要缓存对象等复杂类型的话，基本上就只能靠序列化来支撑具体语言的类型系统（支持 Hash 类型的缓存，可以部分模拟对象类型），不仅有序列化的成本，还很容易导致传输成本也显著增加。举个例子，假设某个有 100 个字段的大对象变更了其中 1 个字段的值，通常缓存也不得不把整个对象所有内容重新序列化传输出去才能实现更新，因此，一般集中式缓存更提倡直接缓存原始数据类型而不是对象。相比之下，JBossCache 通过它的字节码自审（Introspection）功能和树状存储结构（TreeCache），做到了自动跟踪、处理对象的部分变动，用户修改了对象中哪些字段的数据，缓存就只会同步对象中真正变更那部分数据。 如今 [Redis](https://redis.io/) 广为流行，基本上已经打败了 Memcached 及其他集中式缓存框架，成为集中式缓存的首选，甚至可以说成为了分布式缓存的实质上的首选，几乎到了不必管读取、写入哪种操作更频繁，都可以无脑上 Redis 的程度。也因如此，之前说到哪些数据适合用复制式缓存、哪些数据适合集中式缓存时，笔者都在开头加了个拗口的 “理论上”。尽管 Redis 最初设计的本意是 NoSQL 数据库而不是专门用来做缓存的，可今天它确实已经成为许多分布式系统中无可或缺的基础设施，广泛用作缓存的实现方案。

*   从数据一致性角度说，缓存本身也有集群部署的需求，理论上你应该认真考虑一下是否能接受不同节点取到的缓存数据有可能存在差异。譬如刚刚放入缓存中的数据，另外一个节点马上访问发现未能读到；刚刚更新缓存中的数据，另外一个节点访问在短时间内读取到的仍是旧的数据，等等。根据分布式缓存集群是否能保证数据一致性，可以将它分为 AP 和 CP 两种类型。此处又一次出现了 “理论上”，是因为我们实际开发中通常不太会把追求强一致性的数据使用缓存来处理，可以这样做，但是没必要（可类比 MESI 等缓存一致性协议）。譬如，Redis 集群就是典型的 AP 式，有着高性能高可用等特点，却并不保证强一致性。而能够保证强一致性的 ZooKeeper、Doozerd、Etcd 等分布式协调框架，通常不会有人将它们当为“缓存框架” 来使用，这些分布式协调框架的吞吐量相对 Redis 来说是非常有限的。不过 ZooKeeper、Doozerd、Etcd 倒是常与 Redis 和其他分布式缓存搭配工作，用来实现其中的通知、协调、队列、分布式锁等功能。

分布式缓存与进程内缓存各有所长，也有各有局限，它们是互补而非竞争的关系，如有需要，完全可以同时把进程内缓存和分布式缓存互相搭配，构成透明多级缓存（Transparent Multilevel Cache，TMC），如图 4-14 所示。先不考虑 “透明” 的话，多级缓存是很好理解的，使用进程内缓存做一级缓存，分布式缓存做二级缓存，如果能在一级缓存中查询到结果就直接返回，否则便到二级缓存中去查询，再将二级缓存中的结果回填到一级缓存，以后再访问该数据就没有网络请求了。如果二级缓存也查询不到，就发起对最终数据源的查询，将结果回填到一、二级缓存中去。

![](_resources/v2-4f0ba9d32cc8c5d5b194635fd8c15bd8_r.jpg)

尽管多级缓存结合了进程内缓存和分布式缓存的优点，但它的代码侵入性较大，需要由开发者承担多次查询、多次回填的工作，也不便于管理，如超时、刷新等策略都要设置多遍，数据更新更是麻烦，很容易会出现各个节点的一级缓存、以及二级缓存里数据互相不一致的问题。必须 “透明” 地解决以上问题，多级缓存才具有实用的价值。一种常见的设计原则是变更以分布式缓存中的数据为准，访问以进程内缓存的数据优先。大致做法是当数据发生变动时，在集群内发送推送通知（简单点的话可采用 Redis 的 PUB/SUB，求严谨的话引入 ZooKeeper 或 Etcd 来处理），让各个节点的一级缓存自动失效掉相应数据。当访问缓存时，提供统一封装好的一、二级缓存联合查询接口，接口外部是只查询一次，接口内部自动实现优先查询一级缓存，未获取到数据再自动查询二级缓存的逻辑。

**附加解释下 CAP 定理：**CAP 是分布式计算领域所公认的著名定理。其描述了一个分布式的系统中，涉及共享数据问题时，以下三个特性最多只能同时满足其中两个：

*   **一致性**（**C**onsistency）：代表数据在任何时刻、任何分布式节点中所看到的都是符合预期的。
*   **可用性**（**A**vailability）：代表系统不间断地提供服务的能力，理解可用性要先理解与其密切相关两个指标：可靠性（Reliability）和可维护性（Serviceability）。可靠性使用平均无故障时间（Mean Time Between Failure，MTBF）来度量；可维护性使用平均可修复时间（Mean Time To Repair，MTTR）来度量。可用性衡量系统可以正常使用的时间与总时间之比，其表征为：A=MTBF/（MTBF+MTTR），即可用性是由可靠性和可维护性计算得出的比例值，譬如 99.9999% 可用，即代表平均年故障修复时间为 32 秒。
*   **分区容忍性**（**P**artition Tolerance）：代表分布式环境中部分节点因网络原因而彼此失联后，即与其他节点形成 “网络分区” 时，系统仍能正确地提供服务的能力。

**7. 如何设计缓存（How）**
------------------

在进行缓存结构设计的时候，需要考虑的点有很多：

**1）对缓存带来的价值持怀疑态度：**从成本、延迟和可用性等方面来评估引入缓存的合理性，并仔细评估引入缓存带来的额外风险和收益。

**2）业务流量量级评估：**对于低并发低流量的应用而言，引入缓存并不会带来性能的显著提升，反而会带来应用的复杂度以及极高的运维成本。也不是任何数据都需要使用缓存，比如图片视频等文件使用分布式文件系统更合适而不是缓存。因此，在引入缓存前，需要对当前业务的流量进行评估，在**高并发大流量的业务场景中引入缓存相对而言收益会更高**；

**3）缓存组件选型：**缓存应用有很多如 Redis、Memcached 以及 tair 等等，针对每一种分布式缓存应用的优缺点以及适用范围、内存效率、运维成本甚至团队开发人员的知识结构都需要了解，才能做好技术选型；

**4）缓存相关指标评估：**在引入缓存前，需要着重评估 value 大小、峰值 QPS、缓存内存空间、缓存命中率、过期时间、过期淘汰策略、读写更新策略、key 值分布路由策略、数据一致性方案等多个因素，要做到心中有数；

**5）缓存高可用架构：**分布式缓存要高可用，比如缓存的集群设计、主从同步方案的设计等等，只有缓存足够可靠，才能服务于业务系统，为业务带来价值；

**6）完善的监控平台：**当缓存投入生产环境后，需要有一套监控系统能够显式的观测缓存系统的运行情况，才能更早的发现问题，同时对于预估不足的非预期热点数据，也需要热点发现系统去解决非预期的热点数据缓存问题。

**7）缓存最近访问原则：**将缓存数据放在离用户最近的地方，无疑会极大的提升响应的速度，这也是多级缓存设计的核心思想。

**8）考虑缓存数据的安全问题**。包括加密、与外部缓存队列通信时的传输安全性以及缓存投毒攻击和侧信道攻击的影响。

**引用**
------

coolshell：缓存更新的套路 亚马逊缓存最佳实践 wikipedia：Cache (computing) 凤凰架构：透明多级分流系统 FaceBook 论文：《[Scaling Memcache at Facebook](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)》 Golang 官方库: singleflight Github 开源库: hybridcache wikipedia：Adaptive Replacement Cache wikipedia：Low Inter-Reference Recency Set wikipedia：Cache Replacement Policies

其实缓存无处不在，由于篇章有限，所以不能一一道来！如果你对本文有疑惑、建议、指正或对某个技术点感兴趣，**欢迎关注我**，给我留言、催更哦！

![](_resources/d81d8017b8f0280056c43f2900bd4271_l.jpg)netero​

谢邀，就比如中国人和人类的关系。CDN 属于缓存服务的一种。缓存服务其实是比较泛的一个概念。当然 cdn 也不全是缓存服务。他只是用到了缓存技术。其他用到的技术例如: 智能 DNS, 作用是就近原则，把用户流量引导到最近的缓存服务器上。

![](_resources/v2-e338719528d19acf81d7e2780fb5d8b1_l.jpg)泰海 - 美玲

CDN 代表内容交付网络，是一个高度分布式的平台，其中许多服务器在全球范围内分开。Web 内容根据用户的区域位置传递给最终用户并且还基于原始网页 CDN 服务器。

CDN 服务可以加快网站内容的交付速度。CDN 还增加了访问量，从而将网站引向全球，用户获得最快的内容以由最近的 CDN 服务器传送。CDN 还提供安全性和保护以防流量很大。