## **Vision** and Mission

Vision

Enabling a digital economy that is always learning and always leading

Mission

Developing digital talent through education, applied research, consulting and career services

Executive Educ a t i on

Industry Engage m e nt

Graduate P r o g r a mm es

S e r v i ces P o rt f o li o

Applied R esea r ch

Career S e r v i ces

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## NUS-ISS Learning Pathways

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Our Practice-based Education

Education Research

Graduate Diploma in Systems Analysis

Artificial Intelligence

StackUp – Startup Tech Talent Development

Master of Technology (Enterprise Business Analytics)

Stackable Certificate Programme

Software Systems

Graduate Programme

Executive Education

Master of Technology (Software Engineering)

Master of Technology (Digital Leadership)

Digital Products and Platforms

Digital Strategy and Leadership

Master of Technology (Intelligent Systems)

S ma rt H e al th Leadership Centre

e-Government Leadership Centre

Digital Innovation & Design

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Our Key Customers

Entertainment & Hospitality

Healthcare & Pharmaceutical

Trade, A ss o ci a t i o n & Chamber

Logistics & T r a n s po r t a t i on

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Our Partners

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

B ig Da t a

Engineering Fo r An a ly t ics

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Course Introduction

- This course will teach the participants to engineer Big Data solutions on a distributed computing platform
- The key motivation is to provide the participants with necessary engineering skills to deal with a range of real- world Big Data solutions
  - This will be achieved by constructing an architectural framework for big data storage and processing, using appropriate languages to manage polyglot programming scripting persistence
  - Construction will foster implementation of the best fitting analytics model for the given requirements
- Illustrate the above discussed implementation processes using Spark Framework

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Learning Objectives

Understand the growth of Big Data and need for a scalable processing framework

Understand the various data storage options , choose an appropriate storage model based on the application requirements

Perform data manipulation and querying on Big Data solutions dealing with high volume using NoSQL

Understand the big data computing essentials, storage needs, and relevant architectural mechanism in processing both structured and unstructured data

Understand various in memory, batch processing and Spark query engine to perform analytics

Understand and use the rdd, data frame, machine learning, statistics and other related packages that come with Spark

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Course Design

Big Data Solutions & Applications

Machine Learning

Other Applications

Stream Processing

Batch Processing

Workload Management

Data Storage

Distributed File System

Data Integration

Compute Cluster

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Agenda

| Day |                                                                                      Topic                                                                                      |       Dates       |
| :-: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------: |
|  1  | Course Introduction<br >Introduction to Big Data Engineering Big Data Architecture<br >Introducing Apache Spark Framework<br >Lab: Installation or Workshop Tools and Framework | FT: 19th Feb 2024 |
|  2  |                   Functional Thinking and Programming<br >Lab: Writing Scalable Functions in Python Workshop Big Data Ingestion<br >Big Data Ingestion Tools                    | FT: 20th Feb 2024 |
|  3  |   Spark Query Language with Python and Scala Spark SQL Demo<br >Lab: Spark SQL Workshop<br >Spark and Kubernetes Deployment Introduction to Spark RDD<br >Spark RDD Workshop    | FT: 21st Feb 2024 |
|  4  |                                              Spark Data Formats Spark Best Practices<br >Introduction to Machine Language Spark ML                                              | FT: 22nd Feb 2024 |
|  5  |                                            Spark ML Workshop Managing Big Data Projects<br >Case Study and Closing Session Mock Exam                                            | FT: 23rd Feb 2024 |

Practicing the workshops and c o m pl e ti n g t h e mock exam helps!!!

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Our Wish is that . . .

You will be equipped to engineer from development of Big Data

Solutions that targets solving family of business analytics

problems

You will be the change agents who implements scale large scale data engineering solutions in the industry

You will be solving such problems in your jobs by employing

sound distributed Big Data Engineering principles and design

instead of random approaches

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

[wwwissnusedusg](http:www.iss.nus.edu.sg)

[wwwfacebookcomISSNUS](http:www.facebook.comISS.NUS)

twittercomISSNUS @issnus

[wwwlinkedincomcompanyissnus ](http:www.linkedin.comcompanyissnus) youtubecomuser TheISSNUS

ATABA-BEADCourse Conduct Module Introduction

All rights reserved

## Introduction to Big Data Engineering

Dr Venkat Ramanathan ( [rvenkat@nusedusg](mailto:rvenkat@nus.edu.sg) [ ](mailto:rvenkat@nus.edu.sg)) NUS-ISS

© 2016-2023 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Learning Objectives

Understand the role of data in performing Business Analytics

Analyze, classify and characterize the “Big Data”

Analyze critically the design challenges in Big Data Solutions and understand the need for a distributed computing platform

Discuss the various data storage formats necessitated in Big Data Solutions landscape

Appraise the needful components by analyzing key characteristic requirements as demanded by the Big Data Solution

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Agenda

Introduction to Big Data

Introduction to Big Data Analytics

Design Challenges in Building Big Data Engineered Solutions

Components of Big Data Engineered Solutions

Use Cases for Data Science

Summary

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Introduction to Big Data

There sat that beautiful big machine whose sole job was to copy things and do addition Why not make the computer do it? That’s why I sat down and wrote the first compiler It was very stupid What I did was watch myself put together a program and make the computer do what I did

Admiral Grace Hopper

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

The Course… BEAD

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## DATA:

What is data?

Why do we need data?

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Need for sourcing & storing data

Design Model A r c h i t e ct u re Process Analytics

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Analytics

What is Information (step towards Analytics)?

Derivative from DIKW Model

No Rain, cloudy so outdoor activities ok

Moderate haze, so restrict to healthy participants and no physically strenuous games

Annual Day event (out door and indoor activity)

Singapore Island, West, Time 13:01:05PM, PSI 210 PM25  gm 3 , Temperature 32 0 C, Humidity 62%, Cloudy

SG-W:13:01:05:210:32:62:C

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## The Analytics Continuum

Source: https:[wwwibmcomdeveloperworkscommunityblogsjfpentryCognitiveComputingvsAnalytics?lang=en](http:www.ibm.comdeveloperworkscommunityblogsjfpentryCognitiveComputingvsAnalytics?lang=en)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Business Analytics

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Business Drivers for Analytics

|               Business Driver               |                                 Examples                                 |
| :-----------------------------------------: | :----------------------------------------------------------------------: |
|        Optimize business operations         |                Sales, pricing, profitability, efficiency                 |
|           Identify business risk            |                      Customer churn, fraud, default                      |
|     Predict new business opportunities      |             Upsell, cross-sell, best new customer prospects              |
| Comply with laws or regulatory requirements | Anti-Money Laundering, Fair Lending, Basel II- III, Sarbanes-Oxley (SOX) |

MONITIZE TRANSFORM NEW BUSINESS

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Business Questions

- Bank (eg: a large bank such as DBS)
  - Loans
- Who should I campaign to offer loan?
- Will this customer default his loan?
- What kind of financial activities does the customer involve in?
- Credit Card
- Risk management – eg: Is the current transaction a fraudulent usage?
- Usage patterns for campaigns, eg: What products should we sell to this client?
- To answer the above first we determine what or wherefrom the data is sourced?
- Existing records with banks (eg: Transaction records in savings, credit & other ac)
- Point of sales systems where cards are used
- Other external systems like credit rating organisations, Govt organisations etc
- Profiling of customer based on wealth distribution & distribution

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- How can we answer those questions?
  - Need Data
  - Need to analyse data
  - Need to infer data
  - Need to predict outcomes
- Using data for decisions always existed; what is new?
  - Look at the cases described
- Large amounts of Data
- Data is from heterogamous sources and heterogeneous types
- Data is growing rapidly
- etc…

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## 1. Descriptive Analytics

Descriptive analytics are carried out to answer questions about events that have already occurred This form of analytics contextualizes data to generate information

Examples:

What was the sales volume over the past 12 months?

What is the number of support calls received as categorized by severity and geographic location?

What is the monthly commission earned by each sales agent?

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## 2. Diagnostic Analytics

Diagnostic analytics aims to determine the cause of a phenomenon that occurred in the past using questions that focus on the reason behind the event

The goal of this type of analytics is to determine what information is related to the phenomenon in order to enable answering questions that seek to determine why something has occurred

Examples:

Why were Q2 sales less than Q1 sales?

Why have there been more support calls originating from the Eastern region than from the Western region?

Why was there an increase in patient re- admission rates over the past three months?

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## 3. Predictive Analytics

Predictive analytics are carried out in an attempt to determine the outcome of an event that might occur in the future With predictive analytics, information is enhanced with meaning to generate knowledge that conveys how that information is related

Models use strength and magnitude of the associations (of past events)

Models depends on the conditions under which the past events occurred which should be changed if the conditions change

Examples:

What are the chances that a customer will default on a loan if they have missed a monthly payment?

What will be the patient survival rate if Drug B is administered instead of Drug A?

If a customer has purchased Products A and B, what are the chances that they will also purchase Product C?

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## 4. Prescriptive Analytics

Prescriptive analytics build upon the results of predictive analytics by prescribing actions that should be taken The focus is not only on which prescribed option is best to follow, but why

Provide results that can be reasoned about because they embed elements of situational understanding that can be used to gain an advantage or mitigate a risk

Provides more value than any other type of analytics and correspondingly require the most advanced skillset, specialized software and tools

Computes various outcomes and suggests the best course of action for each each

Examples:

Among three drugs, which one provides the best results?

When is the best time to trade a particular stock

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## 5. Cognitive Analytics

- Cognitive computing combines artificial intelligence and machine- learning algorithms, in an approach which attempts to reproduce the behavior of the human brain
- Cognitive analytics applies intelligent technologies to bring all of these data sources within reach of analytics processes for decision making and business intelligence
  - Cognitive analytics is a data forward approach that starts and ends with what’s contained in information This unique way of approaching the entirety of information (all types and at any scale) reveals connections, patterns and collocations that enable unprecedented, even unexpected insight
  - A cognitive system can provide real-time answers to questions posed in natural language by searching through massive amounts of information that have been entered into its knowledge base, making sense of context, and computing the most likely answer As developers and users “train” the system, answers become more reliable and increasingly precise over time

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Categories of Analytics

Cognitive Analytics

Prescriptive Analytics

Predictive Analytics

Diagnostic Analytics

Descriptive Analytics

Inspiration: Adapted & Enhanced from Big Data Fundamentals: Concepts, Drivers & Techniques by Thomas Erl et al

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## What is Big Data

Let’s contextualise the V’s with a case scenario – eg: Google Maps!

Lněnička, Martin & Máchová, Renáta & Komárková, Jitka & Čermáková, Ivana (2017) Components of Big Data Analytics for Strategic Management of Enterprise Architecture

Big Data Engineering deals with advanced techniques that harness independent resources for building scalable data systems when the above mentioned characteristics of the datasets require new architectures for efficient storage, manipulation, and analysis

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Data Never Sleeps….. !!!!

Data is constantly pouring out of our smartphones, smart watches, smart TVs, and countless other devices that are all connected—and it continues to proliferate at an astounding rate But just how much data is generated every minute in 2022?

Reference:

[https:wwwdomocomblogdata-never-sleeps-10](https:www.domo.comblogdata-never-sleeps-10)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Big Data Defined

Big Data defines a situation in which data sets have grown to such enormous sizes and consists of a variety of structures and are obtained from, stored at or available in various locations that conventional information technologies can no longer effectively handle either the size of the data set or the scale and growth of the data set

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Engineering the Big Data Systems

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## What is Data Engineering

External Sources

Internal Applications

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Data Engineering vs Big Data Engineering

Creation of Data

Traditional Systems

|       Operation       |   Engineering Aspects    | TechnologyTools<br >(Examples only) |
| :-------------------: | :----------------------: | :---------------------------------: |
|     Manual Entry      | Data Validation at Entry |                Java                 |
|       API Calls       |   Traditional Adapters   |           JDBC, JN Bridge           |
| Limited B2B transfers |       Data Formats       |                 XML                 |
|                       | Communication Protocols  |              HTTP, FTP              |
|                       |  Communication Pattern   |          RR, Asynchronous           |

Big Data Systems

|       Source        |   Engineering Aspects    | TechnologyTools (Examples only) |
| :-----------------: | :----------------------: | :-----------------------------: |
| Traditional Systems | Data Validation at Entry |        Databases, Files         |
|       Devices       |  Real Time accumulation  |              Kafka              |
|   Social Networks   |       Data Formats       |    Diverse (discussed later)    |
|                     | Communication Protocols  |     HTTP, FTP and many more     |
|                     |  Communication Pattern   |             Diverse             |

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Preparation of Data

Traditional Systems

|   Operation    | Engineering Aspects  |        Example        |
| :------------: | :------------------: | :-------------------: |
|   Validation   |      UI checks       |  Mandatory data etc.  |
|                |    Business Rules    |     Age negative      |
| Transformation | Nil or SimpleMinimal | Case sensitivity etc. |

Big Data Systems

|   Operation    |               Engineering Aspects                |                                           Example                                            |
| :------------: | :----------------------------------------------: | :------------------------------------------------------------------------------------------: |
|   Validation   | Cannot Directly Enforce Use cleansingelimination |                           Absence of data or content needs fixing                            |
| Transformation |                    Extensive                     | Dates in different format Null value fixes<br >Mapping changes from input to storage formats |

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Storage of Data

Traditional Systems

|    Persistence     |   Engineering Aspects    | TechnologyTools<br >(Examples only) |
| :----------------: | :----------------------: | :---------------------------------: |
|     Databases      |    Usually Relational    |               Oracle                |
| Data Store Formats |        Structured        |               Tables                |
|      Servers       | In premise Single Server |           Database server           |
|      Security      |       Well managed       |         Firewall Protected          |

Big Data Systems

|    Persistence     |        Engineering Aspects        | TechnologyTools<br >(Examples only) |
| :----------------: | :-------------------------------: | :---------------------------------: |
|     Databases      |             Multiple              |           MongoDB, RDBMS            |
| Data Store Formats | Structured, Un or Semi structured |            JSON, Parquet            |
|      Servers       | Multiple, Distributed, Replicated |           Server Clusters           |
|      Security      |            Challenging            |        Governance Standards         |

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Data Processing

Traditional Systems

|          Output          | Engineering Aspects | TechnologyTools<br >(Examples only) |
| :----------------------: | :-----------------: | :---------------------------------: |
| <br >Information Systems |   Simple Reports    |                 SQL                 |
|                          |       Graphs        |           Crystal Reports           |
|     Decision Systems     |        OLAP         |             IBM Cognos              |

Big Data Systems

|       Output        | Engineering Aspects | TechnologyTools<br >(Examples only) |
| :-----------------: | :-----------------: | :---------------------------------: |
| Information Systems |       Diverse       |       Visualisation Software        |
|      Analytics      |       Complex       |          Power BI, Tableau          |
|  Real Time Alerts   |       Dynamic       |             Amazon SNS              |

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Big Data System Characteristics

- There are three basic states of data: data at rest, data in motion, and data in use
  - Data at rest
- Data at rest is a term that refers to data stored on a device or backup medium in any form
- It can be data stored on hard drives, backup tapes, in offsite cloud backup, or even on mobile devices
- Data at rest is a term that refers to data stored on a device or backup medium in any form It can be data stored on hard drives, backup tapes, in offsite cloud backup, or even on mobile devices
- Data in motion
- Data in motion is data that is currently traveling across a network or sitting in a computer’s RAM ready to be read, updated, or processed
- This data in motion (usually encrypted) includes data moving across a cables and wireless transmission It can be emails or files transferred over FTP or SSH
- Data in use
- Data in use is data that is not just being stored passively on a hard drive or external storage media This is data that is being processed by one or more applications
- This is data currently in the process of being generated, updated, appended, or erased
- Reference: [http:aspgcomthree-states-digital-data](http:aspg.comthree-states-digital-data)
- Three states of Data [https:enwikipediaorgwikiDigitaldata](https:en.wikipedia.orgwikiDigitaldata)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- Data can take different forms
  - Structured data refers to data that has a defined length and format
- Computer- or machine-generated: Machine-generated data generally refers to data that is created by a machine without human intervention
- Human-generated: This is data that humans, in interaction with computers, supply
- Unstructured data is data that does not follow a specified format
- Semi-structured data refers to data that falls between structured and unstructured data
- Semi-structured data does not necessarily conform to a fixed schema (that is, structure) but can be interpreted (mainly by self-describing data like having simple labelvalue pairs)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Structured vs Unstructured Data vs Semi-structured

Information with a high degree of organization

The structure facilitates data processing

Typically has a data model

Easily entered, stored, queried, analyse

The storage understand the structure

List of students Employee details

Unstructured Data

Data that does not have a schema

Cannot be queried or processed in typical sense

For inference may require NLP, Image Processing, ML etc

PDF files Audio Video

Has a schema but is not rigid

Easy to adapt to changes

Easy to split the dataset and distribute

Employee data stored in XML JSon

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Meeting the Technology Challenge posed by Big Data

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Big Data Processing –

Options and Approaches

- Can existing Platform, Architecture, Tools and Technologies handle this?
  - Option 1: Scale Up
  - Option 2: Scale Out

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- Merits of Scale up?
  - Well we live in existing computing models, methods and software
- Demerits
  - Cost
  - There is a limit to which we can scale up
  - Reliability
  - Performance
- We seem to Gravitate to Scale Out option!
- But… What does that mean? A whole new way of Computing!

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- Scale out essentially means Distributed Systems
- Technological Challenges
  - To process large data sets in multiple systems
- Well known RDBMS do not meet the need
- Nodes may fail – hence we need to have an approach to handle that
- Number of nodes may not be a constant – we need to cater to that
- Communication models between nodes need to be established
- For example data for same data set or query may be stored across multiple nodes
- We need to aggregate it (advantage is we may be able to parallelize)
- So what do we need!
- A new Distributed Storage and Analysis Technology – Infrastructure such as Cloud Platform A Processing Framework like Hadoop!

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Data Storage – Past, Present and the Future

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Structured Data Formats

- Text Data File – Fixed Length Records
  - Each Field in the record are of predefined size and each line is a record
  - Example: File Storing Employee Details (EmployeeDAT)
- 1111111111222222222233333333334444
- 1234567890123456789012345678901234567890123

| 123 | Venkat |        | ISS | 61  | 1200.50 |
| :-: | :----: | :----: | :-: | :-: | :-----: |
| 345 | Suriya |        | ISS | 45  | 2000.75 |
| 456 | Jacob  | Mathew | SOC | 35  | 1500.00 |

In the above example the fields and their lengths (in bracket) are as below: EmpID(8), Name(15), Dept(8), Age(4), Salary(8)

Record Length is 43 characters

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- Delimited Text Files
  - Fields are delimited by a character such as comma or tab stops etc
  - Example Comma Delimited Files (EmployeeCSV):

12 3 , Ven k at , ISS , 6 4 , 12005 0

34 5 , Sur i a , ISS , 4 5 , 2 0007 5

45 6 , Jac o b Mathew , S OC , 3 5 , 150 0 0 0

Files may have headers in first row as below:

Em p ID , N a me , Depart m ent , Age , S a lar y

12 3 , Ven k at , ISS , 6 4 , 12005 0

345,Sur i a , ISS , 4 5 , 2 0007 5

45 6 , Jac o b Mathew , S OC , 3 5 , 150 0 0 0

Note: CSV’s are classified Semi-Structured; I frown on this but have no authority to contest IT Industry idiosyncrasies

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

- RDBMS (SQL)

  - Employee Table

- Column Oriented Databases (NoSQL)
  - Product Example: Google Big Table, Apache Cassandra
  - Employee ‘Table’

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Semi Structured Formats

- JSON (NoSQL or File Based)
  - NoSQL product example: MongoDB, Apache CouchDB
  - Example: EmployeeJSON (one record per file)

Example: EmployeesJSON (multiple records in one file)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Unstructured Data Formats

- Storage
  - Stored in Distributed File System eg HDFS
  - May use Object Stores
  - Business Documents (PDF Files, Emails, DOC files)
  - Social Network Posts and Feeds
  - Image Files, Audio Files, Video Files etc
  - Uses Key Value store model

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Implementing the Big Data Solution

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

##

Big Data Solutions

Machine Learning

Other Applications

Stream Processing

Batch Processing

Workload Management

Data Storage

Distributed File System

Data Ingestion

Compute Cluster

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Processing the Big Data – Technologies

Hadoop Distributed File System (HDFS)

Machine Learning

Resilient Distributed Datasets

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

The most dangerous phrase in the language is, ‘We’ve always done it this way’

- Admiral Grace Hopper

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Key Points

- Autonomous computers that work together to give the appearance of a single coherent system:
  - Resource sharing is the main motivation for constructing distributed systems
  - Architectural styles reflect way in which both software and hardware are organized to communicate in a distributed system
- Big Data analysis blends traditional statistical data analysis approaches with computational ones
  - The overall goal of data analysis is to support better decision-making
  - Carrying out data analysis helps establish patterns and relationships among the data being analysed

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

We're flooding people with information We need to feed it through a processor A human must turn information into intelligence or knowledge We've tended to forget that no computer will ever ask a new question

- Admiral Mdm Grace Hopper

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## References

Coulouris, George F, Jean Dollimore, and Tim Kindberg Distributed systems: concepts and design pearson education, 2005

Tanenbaum, Andrew S Distributed operating systems Pearson Education India, 1995

Verissimo, Paulo, and Luis Rodrigues Distributed systems for system architects Vol 1 Springer Science & Business Media, 2012

Data Science with Hadoop and Spark: Designing and Building Effective Analytics at Scale, by Casey Stella et al, Published by Addison-Wesley Professional, 2016

Enterprise Big Data Engineering, Analytics, and Management, by Thomas Roth-Berghofer, Samia Oussena, Martin Atzmueller, Publisher: IGI Global, Release Date: June 2016

Magazines, newspapers, blogs, Wikipedia, websites and other online resources

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Appendix:

Non-Lecture slides (for reference)

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

Use Cases Big Data Analytics

How many millionaires do you know who have become wealthy by investing in savings accounts? I rest my case

~Robert G Allen

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Product Recommendation

Recommender systems have become rather common for online retailers and many other businesses with an online retail presence

We are familiar with various flavours of product recommendation techniques used by companies such as Amazon, Netflix, Facebook, LinkedIn, and, more recently, GoogleYouTube

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Customer Churn Analysis

- It is well known that keeping an existing customer is often much cheaper than finding a new one
  - Whether you are a bank, a retailer, a gaming company, an Internet service provider, a cell phone provider, an airline, or an insurance company, there is a strong desire in almost any business to actively pursue programs for customer retention and prevent customer churn (also known as “attrition”)
  - Churn models differ by industry, due to the different business models and specific customer engagement and lifetime value models
- Customer churn analysis uses machine learning to predict the likelihood of each customer “leaving”
- Businesses then use this data to drive and guide customer- retention programs (such as discounts or other incentive programs) to encourage these at-risk customers to stay

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Customer Segmentation

- Customer segmentation is a common technique used to identify segments of customers that behave similarly with regard to their interaction with the business
  - A grocery store may be interested in segmenting its customers by the type of food products they purchase For example, one segment of customers might be “people who favour meat,” while another might be “people who favour gourmet products”
  - Similarly, airlines and hotels are interested in segmenting customers into business travellers versus non-business travellers Airlines are also interested in “domestic passengers” versus “international passengers”
- An immediate benefit of such segmentation is the ability to increase marketing efficiency For example, airlines may customize email campaigns based on effective segmentation to achieve much higher response rates

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Sales Leads Prioritization

Many sales professionals enjoy a pipeline of sales leads that result from good, effective marketing

There are many ways by which a business can prioritize the sales efforts, but one of the most natural parameters is “likelihood to close within N days”

Applying data science, businesses can model each lead with various features (such as geographic location, customer type, website engagement, previous sales, etc), and build a predictive model to determine the likelihood of each lead to close within the desired time period

Based on such models, sales operations improve in efficiency and overall revenues increase

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Sentiment Analysis

- Sentiment analysis is an application of text analytics and natural language processing techniques, with the goal of understanding customer sentiment about a certain topic (eg, a product or service)
  - The Web has become an excellent source for assembling consumer opinions There are now several Web sites containing such opinions, eg, customer reviews of products, forums, discussion groups, and blogs
  - With the increased adoption of crowd-sourced feedback from customers in online forums and the growth of social networks such as Facebook and Twitter, there is a lot of information available about customer sentiment

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Fraud Detection

Big Data E n gi neer i ng

Fraud or payment abuse is a serious problem for many businesses as well as government organizations Every time money changes hands based on some criteria or set of rules, there is potential for fraud and abuse for monetary gain by malicious actors

Clearly fraud detection is a critical capability for companies involved in payments such as banks, PayPal, or Square But fraud detection is also highly effective in improving the bottom line for insurance companies, retailers, and many others

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Predictive Maintenance

- Equipment doesn’t operate forever and will ultimately fail at some point; it always fails sometime in the future
  - Unfortunately, such failure may have dire consequences given the binary nature of the failure There are many examples of this, in various industries Let’s look at a few:
- When a component in a cell tower fails, the cell tower may stop operating and many cell phone users in the nearby vicinity may not be able to use their mobile services until the component is fixed and the tower is fully functional again
- When an AC compressor fails in an office building, the employees working there may suffer from poor working conditions for a day or two until a technician is able to fix the problem
- If an engine in a helicopter or airplane fails, there could be dire consequences indeed Fortunately, this is not a common safety issue as testing of these engines is rather thorough before lift-off Nevertheless, if a failure is found on the ground, the vessel may need lengthy repair, which may result in flight delay or cancellation
- If a freezer in a fast-food restaurant fails, the restaurant will need to replace it However, that may take a few days What will it do in the meantime with all the frozen food kept in that freezer?

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Market Basket Analysis

- A common use case for retailers is known as market basket analysis (also known as affinity analysis or association mining)
  - In this type of analysis, we are trying to understand the purchasing behavior of the user More specifically, with market basket analysis, retailers hope to gain insights into which products tend to be purchased together
  - Market basket analysis often drives store layout design, where items with strong association are placed strategically close to each other, making it more likely that the customer will purchase the related item
  - Retailers can also use the results of market basket analysis for effective marketing campaigns to drive foot traffic into a physical store

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Predictive Medical Diagnosis

- Making decisions about a medical diagnosis is hard, partly because there is often a lot of uncertainty and not enough data
  - Furthermore, the implications of an incorrect decision can be dire
  - Arming medical professionals with computer-assisted, data-based medical diagnosis tools is a tremendous opportunity to improve healthcare Let’s explore a few specific applications:
- Machine learning algorithms can detect unknown patterns of diagnosis and, if validated clinically, can add to the existing repository of medical knowledge
- Electronic patient records use the ICD10 standard to record existing diagnoses for patients Sometimes the electronic record is missing a few key diagnoses, and automated diagnosis of disease can be used for identifying these coding gaps
- Various care quality measures such as HEDIS (Healthcare Effectiveness Data and Information Set) can be improved by, for example, screening based on automated diagnosis results

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Insurance Risk Analysis

- Insurance is a risk-based industry Products such as property, auto, or life insurance are always priced based on risk assessment and the application of the risk pooling principle
  - Insurance companies have been using predictive risk modeling for some time, modeling risk based on key indicators such as age, gender, geographic location, and historical data about the consumer
- For example, it is well known that younger drivers tend to be more accident-prone than experienced drivers Thus, auto insurance companies will typically charge a higher premium for drivers under the age of 25
- Since accurate risk analysis is so critical to the profitability of an insurance company, every attempt is made to improve this and gain a competitive edge
- For example, auto insurance companies are looking at sensor data coming from automobiles (GPS data, etc) as a new source of data to be used to improve accuracy of risk prediction By tracking driving behaviour, the insurance company can more accurately assess the risk of accident for that driver

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Predicting Oil and Gas Well Production Levels

- The basic asset of any oil and gas company is the well, from whence oil and natural gas are produced
  - Oil and gas companies such as Schlumberger, Haliburton, Noble Energy, and Chesapeake therefore invest heavily in research and development to maximize oil production levels, resulting in direct impact on the top line of the business
  - There are many variables that may impact the production levels of a given well
  - With sensor data, geophysics data about the well, and other data sources, models can be constructed to predict well production
  - With this predictive model, the oil and gas company can understand what impacts production levels and address issues negatively impacting this level ahead of time, resulting in streamlined production and increased revenues

ATABA-BEADIntroduction to Big Data Engineering

© 2016-2023 NUS All rights reserved

## Big Data Architecture

Suria R Asai

( [suria@nusedusg](mailto:rvenkat@nus.edu.sg) [ ](mailto:rvenkat@nus.edu.sg)) NUS-ISS

Total Slides: 66

© 2016-2024 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

All rights reserved

ATABA-BEADBig Data Architecture

## Learning Objectives

Understand data architecture by examining how data is managed from collection through to transformation , distribution and consumption in an organization

Learn about the data engineering lifecycle , a framework describing “ cradle to grave ” data engineering operating in terms of principles

Understand the various deployment options such as on-premise, collocated and public cloud processing platforms

All rights reserved

ATABA-BEADBig Data Architecture

## Agenda

The (Big) Data Architecture

(Big) Data Engineering Life Cycle

Hadoop Ecosystem

Kubernetes Orchestration

Commercial Cloud Products

Summary

All rights reserved

ATABA-BEADBig Data Architecture

The (Big) Data Architecture

I propose to consider this question “Can machines think?”

Alan Turning

All rights reserved

ATABA-BEADBig Data Architecture

## Different Systems . . .

Big data about individuals using an end device

All rights reserved

ATABA-BEADBig Data Architecture

## Gartner Hype Cycle

All rights reserved

ATABA-BEADBig Data Architecture

All rights reserved

ATABA-BEADBig Data Architecture

Big Data for

Server Side

## CLOUD OFFERINGS

Example: Google Cloud Dataproc, Amazon EMR, Azure HDInsight and IBM Analytics.

Processing Types:

Data Processing Pipeline

Batch

Interactive

Near Real Time

Real Time

Time Frame

Eg: Apache Hadoop YARN

INTELLIGENT Insight Discovery and Action

INSIGHTS

Big Data for End Device Computing

Intelligent Decisions

Slow Precise

Quick & Dirty

Constraints

Provenance

Mobility

Peer-to-Peer

Scheduling or Event

All rights reserved

ATABA-BEADBig Data Architecture

## Traditional Two Disparate Platforms

Data Science, ML and Data Streaming

Business Intelligence & SQL Analytics

I n c o m p l e t e suppor t f o r End-to-end Use Cases

Governance and Security

Files and Blobs

I n c o m p a t i b le se c ur i t y & governances models

Governance and Security

Table and ACL

Disjointed and Duplicative Data Silos

Data Lake

Unstructured files, logs, text, images, videos etc

Data Warehouse

Structured tables

All rights reserved

ATABA-BEADBig Data Architecture

## Components of Data Lakehouse Platforms

Data

Warehousing

Data

Engineering

Data Pipelines and Workflows

Platform Companies: Example DataBricks, Snowflake, Dremio etc, Managed Services: Example RedShift, Synapse, Big Query

Data Lake, Catalogue, Security, Management, Operations, & Governance

Public Cloud or On Prem: Example AWS, Azure, GCP, Oracle etc,

Big Data Storage

Hardware and Cluster Management

All rights reserved

ATABA-BEADBig Data Architecture

## The totality of corporate data.

Unstructured Data

Non Repetitive Data

Business Value Derived Varies

All rights reserved

ATABA-BEADBig Data Architecture

## On Premise or Colocation Services or Cloud Services?

Wh o o wn s t h e m a c h i n e s ?

Distributed Computing I n f r a s t r u c t ure

W h o d e p l o y s t h e s of t w a r e ?

H o w d o e s s c a l i n g h a pp e n?

All rights reserved

ATABA-BEADBig Data Architecture

## Spoiled for choices

Ref: [https:mattturckcomlandscapemad2023pdf](https:mattturck.comlandscapemad2023.pdf)

All rights reserved

ATABA-BEADBig Data Architecture

## Types of Processing - 1

Ba t c h P r o c e s s i n g

R e a l T i m e S t r e a m P r o c e s s i n g

All rights reserved

ATABA-BEADBig Data Architecture

Serving or QueryTransactional Processing

All rights reserved

ATABA-BEADBig Data Architecture

## Types of Processing – 3.

R e q u e s t R e sp o n s e P r o c e s s i n g

All rights reserved

ATABA-BEADBig Data Architecture

## Working of Layers

|    <br >Batch Layer     |                                                                  Stored immutable data<br >Constantly growing in size<br >Recomputed views all the time                                                                  |
| :---------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|    <br >Speed Layer     |                                         Constant stream of data<br >Stores mutable data<br >Less in size volume<br >Views live for a specified period and discarded at intervals                                         |
| <br ><br >Serving Layer | Responsible for indexing and making sure exposed batch views perform well<br >Exposes real-time views created incrementally by the speed layer<br >Merges result from both batch and speed views in a consistent fashion |

All rights reserved

ATABA-BEADBig Data Architecture

## The idea of Data Lake

A d ata l ake is a collection of data storage instances combined with one or more processing capabilities Most data assets are copied from diverse enterprise sources and are stored in their raw arrival state, so they can be refined and repurposed repeatedly for multiple use cases Ideally, a data lake will sto re an d p rocess data of any structure, latency or container (fil es, do cu m ents, resu lt sets, tab les, fo r m ats, B LO B s, m essages, etc )

© [https:vitalfluxcomdata-lake-design-principles-best-practices](https:vitalflux.comdata-lake-design-principles-best-practices)

All rights reserved

ATABA-BEADBig Data Architecture

## Consumption Patterns

All rights reserved

ATABA-BEADBig Data Architecture

Data Ingestion Type

Is the data being ingested in a batch process or in real time?

Data Ingestion Pattern How is the data being ingested? Does it involve transferring files, using messages, connecting to RDBMS or via machine events?

Data Ingestion Frequency How frequently is the data being ingested?

Data Format What data formats are involved?

Data Analytics Type What type of analytics is required on the ingested data? Do we require text, visual, sentimental, or predictive analytics?

Data Types

What type of data is involved?

All rights reserved

ATABA-BEADBig Data Architecture

## Data Lake Architecture

© Architecting Data Lakes Report by Alice La Plante and Ben Sharma

Zaloni has created a data lake reference architecture

All rights reserved

ATABA-BEADBig Data Architecture

## Recall The Processing layers . . .

Machine Learning

HDFS or S3 or GFS or AFS

Hadoop Cluster (Zookeeper, AirFlow, YARN, HCatalogue)

Analytics Engines

(Statistics, Text, Search, Streams)

All rights reserved

ATABA-BEADBig Data Architecture

## A Generic Big Data Architecture

Big Data E n gi neer i ng

For Analytics

Data Storage

…

RDBMS NoSQL Cloud Store In-Mem DFS

Structured

APP

ERP

DW, DB,OLTP, ODS

Batch Scheduled

Big Data Governance

Analytics Servicing

Semi-Structured

Real-Time Views

NoSQL

|     |                    |                      |                  |                  |                     |
| :-: | :----------------: | :------------------: | :--------------: | :--------------: | :-----------------: |
|     |                    | Statistical Analysis |                  | Machine Learning | Recommender Systems |
|     | Predictive Systems |                      | Knowledge Graphs |                  | Pre-computed Views  |
|     |                    |                      |                  |                  |                     |

Data Consumption

Reports Visuals Insights Alerts Search Store

All rights reserved

ATABA-BEADBig Data Architecture

All rights reserved

ATABA-BEADBig Data Architecture

(Big) Data Engineering Life Cycle

S t o r i e s a r e j u s t d a t a w i t h a s o u l

Dr Brené Brown

All rights reserved

ATABA-BEADBig Data Architecture

## Principles of Data Engineering

Choose common components wisely

Plan for failure

Architect for scalability

Architecture is leadership

Always be architecting

Build loosely coupled systems

Make reversible decisions

Prioritize security

Embrace Automation - DevOps, DataOps, MLOps, FinOps

All rights reserved

ATABA-BEADBig Data Architecture

## Data Engineering Life Cycle

Monitor and Test

Develop product

All rights reserved

ATABA-BEADBig Data Architecture

Data Orchestration

Data Engineering and DataOps

Software Engineering and DevOps

All rights reserved

ATABA-BEADBig Data Architecture

## Components and undercurrents of the data engineering lifecycle

(c) Fundamentals of Data Engineering by Joe Reis, Matt Housley Published by O'Reilly Media, Inc

All rights reserved

ATABA-BEADBig Data Architecture

## Processing Spectrum

- Storage
  - RDBMS
  - File Store
  - Object Store
  - NoSQL
- Column Family
- Document
- Key Value
- In Memory
- NewSQL

Cognitive

Prescriptive

Predictive

Diagnostic

Descriptive

- Machine Learning
  - Ensemble
  - Recommender
  - Predictors
  - Reinforced
- Deep learning
  - Vision
  - Speech
  - Cognitive
  - NLP
- Quantum
- Synthetic Media
- Horizontal AI
- Smart Systems

- Domain
  - Finance
  - Healthcare
  - Insurance
  - Commerce
  - Logistics
  - Marketing
  - Sales
  - CRM
  - Education
  - Human Capital
  - Transportation
  - Agriculture
  - Life Sciences
- B2B B2C
- Platforms

AI and Machine Learning Models

Analytics Algorithms

Applications Spectrum

All rights reserved

ATABA-BEADBig Data Architecture

## Generations of Data Engineering

Hadoop Era

Till 2018

Kubernetes Era

Since k8s graduated from CNCF (2018)

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Projects

U s e r d e f i n e s m a p an d r e d u c e tas k s u s i n g M a p R e d u ce A P I a n d su b m i t s i t t o t h e s y st e m

A jo b i s t r i g g e r e d o n t h e c l u s t e r

Y A R N f i g u r e s o u t w h e r e a nd

3 h o w t o r u n t h e jo b , sto r e s t h e r e s u l t i n H D F S

Co m m o n S to r a g e a c c e s s e d b y c l u s t e r

All rights reserved

ATABA-BEADBig Data Architecture

Commercial Cloud Products

Mathematical reasoning may be regarded rather schematically as the exercise of a combination of two facilities, which we may call intuition and ingenuity

Alan Turing

All rights reserved

ATABA-BEADBig Data Architecture

## Amazon Data Lake and Analytics Products

All rights reserved

ATABA-BEADBig Data Architecture

## Azure Data Lake and Analytics Products

All rights reserved

ATABA-BEADBig Data Architecture

## GCP Analytics Products

All rights reserved

ATABA-BEADBig Data Architecture

## IBM Analytics Products

All rights reserved

ATABA-BEADBig Data Architecture

## Kubernates : Kubeflow Architecture

https:[wwwkubefloworgdocsimageskubeflow-overview-platform-diagramsvg](http:www.kubeflow.orgdocsimageskubeflow-overview-platform-diagram.svg)

All rights reserved

ATABA-BEADBig Data Architecture

## Snowflake’s hybrid columnar architecture

All rights reserved

ATABA-BEADBig Data Architecture

## Delta Lakehouse Layered Architecture

All rights reserved

ATABA-BEADBig Data Architecture

## Dremio

All rights reserved

ATABA-BEADBig Data Architecture

Kubernetes Orchestration

One time I tried to explain Kubernetes to someone Then we both didn’t understand it!!

~Tweet by @SwiftOnSecurity

All rights reserved

ATABA-BEADBig Data Architecture

## Comparing containerization to virtualization

Virtual Images run a guest operating system per VM, with a hypervisor layer to implement system calls onto the underlying host operating system

Containers are designed to be disposable and replaceable, so they need to start quickly and use as few resources for overhead processing as possible For this reason, most container images are built from base images containing streamlined, Linux-based, open-source operating systems

All rights reserved

ATABA-BEADBig Data Architecture

## Benefits of Big Data on Kubernetes

Support multiple standby NameNodes

Supports multiple NameNodes for multiple namespaces

Storage overhead reduced

Support GPUs

Intra-node disk balancing

Support for Opportunistic Containers and Distributed Scheduling

Support for Data Lake and Object Storage System via file- system connectors

All rights reserved

ATABA-BEADBig Data Architecture

## Kubernetes Resources for Data Storage

Kubernetes provides to help manage the three commodities of cloud computing:

compute , network , and storage

Kubernetes resources that can be used to manage stateful workloads, including

Volumes, PersistentVolumes, PersistentVolumeClaims, and StorageClasses

Kubernetes also helps in managing persistence in containerized applications in general

All rights reserved

ATABA-BEADBig Data Architecture

## Kubernetes Separation of Concern

Kubernetes offers users a way to automate many of the manual tasks involved with operating containers such as autoscaling, resiliency management, metrics monitoring and more

All rights reserved

ATABA-BEADBig Data Architecture

https:[wwwvamsitalkstechcomarchitecturebig-data-kubernetes-a-reference-architecture-for-spark-with-kubernetes-2-4](http:www.vamsitalkstech.comarchitecturebig-data-kubernetes-a-reference-architecture-for-spark-with-kubernetes-2-4)

All rights reserved

ATABA-BEADBig Data Architecture

A very large part of space-time must be investigated, if reliable results are to be obtained

Alan Turing

All rights reserved

ATABA-BEADBig Data Architecture

## Summary of Essential Points

- Data architecture translates business needs into data and system requirements and seeks to manage data and its flow through the enterprise
- Data Engineering focuses on data preparation and management The data engineering lifecycle breaks data problems into key stages
- The Hadoop ecosystem comprises of HDFS for storage and YARN for cluster management Other tools include:
  - Data Ingestion (Flume, Sqoop, Kana); Data Storage (HDFS, HBase); Data Processing (Spark, Hadoop MapReduce, Pig); Data Modelling as tables for SQL access (Impala, Hive); Data Exploration (Hue, Search); Data Protection (Sentry); Service Programming (Avro, Zookeeper, Thrift); Data Visualization (Zeppelin, Tableau)
- New data systems are hosted and managed using Kubernetes
  - spark-submit can be directly used to submit a Spark application to a Kubernetes cluster
  - Kubernetes requires users to supply images that can be deployed into containers within pods

All rights reserved

ATABA-BEADBig Data Architecture

A very large part of space-time must be investigated, if reliable results are to be obtained

Alan Turing

All rights reserved

ATABA-BEADBig Data Architecture

## References

Big Data E n gi neer i ng

Frank J Ohlhorst, Big Data Analytics: Turning Big Data into Big Money, Published by John Wiley & Sons, 2012

David Feinleib, "Big Data Bootcamp: What Managers Need to Know to Profit from the Big Data Revolution", Published by Apress, 2014

L Bass, P Clements, R Kazman, Software Architecture in Practice 3rd edition Addison-Wesley; 2013

Software Architecture for Big Data and the Cloud by Bruce Maxim; Maritta Heisel; Rami Bahsoon; Nour Ali; Ivan Mistrik Published by Morgan Kaufmann, 2017

Eadline, Douglas Hadoop 2 Quick-Start Guide: Learn the Essentials of Big Data Computing in the Apache Hadoop 2 Ecosystem Addison-Wesley Professional, 2015

Swizec Teller, Hadoop Essentials, Publisher: Packt Publishing, Release Date: April 2015, ISBN: 9781784396688

All rights reserved

ATABA-BEADBig Data Architecture

Hadoop Eco System

It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers… They would be able to converse with each other to sharpen their wits At some stage therefore, we should have to expect the machines to take control

Alan Turing

All rights reserved

ATABA-BEADBig Data Architecture

## The Basic Hadoop Blocks

HDFS

A distributed file system designed to run on commodity hardware

Map Reduce

A framework to define a data processing task

YARN

Yet Another Resource Negotiator – A framework to run the data processing task

All rights reserved

ATABA-BEADBig Data Architecture

## Coordination between Hadoop Blocks

U s e r d e f i n e s m a p an d r e d u c e tas k s u s i n g M a p R e d u ce A P I a n d su b m i t s i t t o t h e s y st e m

A jo b i s t r i g g e r e d o n t h e c l u s t e r

Y A R N f i g u r e s o u t w h e r e a nd

3 h o w t o r u n t h e jo b , sto r e s t h e r e s u l t i n H D F S

Co m m o n S to r a g e a c c e s s e d b y c l u s t e r

All rights reserved

ATABA-BEADBig Data Architecture

## Apache Hadoop History

All rights reserved

ATABA-BEADBig Data Architecture

## Advantages of Apache Hadoop

- Low cost—Runs on commodity hardware:
  - Can run on average performing commodity hardware
  - helps in controlling cost at the same time achieve scalability and performance
  - Adding or removing nodes from the cluster is simple
- Storage flexibility :
  - Can store data in raw format, process unstructured and semi-structured data better
- Open source community:
  - Supported by many contributors with a growing network of developers worldwide
  - Organizations such as Yahoo, Facebook, Cloudera, Horton works etc have contributed
- Fault tolerant :
  - Massively scalable, reliable in terms of data availability and fault tolerant
  - Architecture assumes that nodes can go down and focusses on process of data
- Complex data analytics :
  - Data science has also grown leaps and bounds
  - Complex and heavy computation intensive algorithms need scalable algorithms for a very large-scale data for speed

All rights reserved

ATABA-BEADBig Data Architecture

## Big Data Processing with Hadoop

- Hadoop is composed of two primary components: HDFS and YARN
  - HDFS is the Hadoop Distributed File System, responsible for managing data stored on disks across the cluster
  - YARN acts as a cluster resource manager, allocating computational assets to applications that wish to perform a distributed computation

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Is. . .

- A generic software framework
  - For distributed Storage of Large Data Sets
  - And distributed processing of many concurrent tasks
  - On clusters built from commodity hardware
  - Designed to handle failures automatically
- Map Reduce Tez Spark Others
- YARN
- Processing
- HDFS
- Store any file

All rights reserved

ATABA-BEADBig Data Architecture

## Core Hadoop Platform

Hadoop Distributed File System (HDFS) : A distributed file system that provides high-throughput access to application data

Hadoop YARN : A framework for job scheduling and cluster resource management

Hadoop MapReduce : A YARN-based system for parallel processing of large datasets

In Memory

(Spark)

HPC MPI

(OpenMPI)

Streaming

(Storm, S4)

Interactive

(Tez)

Batch

(MapReduce)

YARN

Processing (Cluster Resource Management)

HDFS

Store any file (Redundant Reliable Storage)

Applications Run Natively IN Hadoop

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Eco System

| <br >R<br >Hadoop | Hive Crunch Mahout Pig<br >(DWH (Aggregate) (Machine (data-<br >+SQL) Learning) flow) | <br >Giraph<br >(Image) |
| :---------------: | :-----------------------------------------------------------------------------------: | :---------------------: |
|    Map Reduce     |                                                                                       |                         |

| Stream Processing |     |                 |     |                 | In Memory |                 |                 |
| :---------------: | :-: | :-------------: | :-: | :-------------: | :-------: | :-------------: | :-------------: |
|   <br ><br >S4    |     | <br ><br >Flume |     | <br ><br >Storm |           | <br ><br >Flink | <br ><br >Spark |

Kafka (Message Broker)

Thrift

(RPC

Serialize)

Sqoop (Hadoop  RDBMS)

Avro (Serilalize)

HDFS (Distributed File System)

Management and Configuration

Cluster Resource Management

Ambari (Manage YARN apps)

YARN (Hadoop Cluster Mngmt)

ZooKeeper

(Configurations, Nameing etc )

Mesos (Cluster Mngmt)

Ca s ca d i n g (Workflow for Map Reduce Jobs)

Oozie (Workflow for Hadoop Jobs)

All rights reserved

ATABA-BEADBig Data Architecture

## Eco System

Source: [https:mydataexperimentscom20170411hadoop-ecosystem-a-quick-glance](https:mydataexperiments.com20170411hadoop-ecosystem-a-quick-glance)

[http:hadoopecosystemtablegithubio](http:hadoopecosystemtable.github.io)

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Design Considerations

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Distributions Available

Big Data E n gi neer i ng

- For Analytics
- Cloudera :
  - The most widely used Hadoop distribution with the biggest customer base as it provides good support and has some good utility components such as Cloudera Manager, which can create, manage, and maintain a cluster, and manage job processing, and Impala is developed and contributed by Cloudera which has real-time processing capability
- Hortonworks :
  - Hortonworks uses an open source Hadoop Ambari was developed and contributed to Apache by Hortonworks Hortonworks offers a very good, easy-to-use sandbox for getting started Hortonworks contributed changes that made Apache Hadoop run natively on the Microsoft Windows platforms including Windows Server and Microsoft Azure
- DataBricks:
  - Databricks grew out of the AMPLab project at University of California, Berkeley that was involved in making Apache Spark, a distributed computing framework built atop Scala
- MapR :
  - MapR distribution of Hadoop uses different concepts than plain open source Hadoop and its competitors, especially support for a network file system (NFS) instead of HDFS for better performance and ease of use In NFS, Native Unix commands can be used instead of Hadoop commands MapR have high availability features such as snapshots, mirroring, or stateful failover
- Amazon Elastic MapReduce (EMR) :
  - AWS's Elastic MapReduce (EMR) leverages its comprehensive cloud services, such as Amazon EC2 for compute, Amazon S3 for storage, and other services, to offer a very strong Hadoop solution for customers who wish to implement Hadoop in the cloud EMR is much advisable to be used for infrequent Big Data processing It might save you a lot of money
- https:enwikipediaorgwikiCategory:Bigdatacompanies

All rights reserved

ATABA-BEADBig Data Architecture

## Cloudera Distribution

All rights reserved

ATABA-BEADBig Data Architecture

## Hortonworks Data Platform

All rights reserved

ATABA-BEADBig Data Architecture

## Hadoop Solution Examples

Log processing

Recommendation systems

Business intelligencedata warehousing

Video and image analysis

Archiving

ExtractTransformLoad (ETL)

Text mining

Index building

Graph creation and analysis

Pattern recognition

Collaborative filtering

Prediction models

Sentiment analysis

Risk assessment

All rights reserved

ATABA-BEADBig Data Architecture

## Introducing Apache Spark Framework

Suria R Asai (suria@nus.edu.sg) NUS-ISS

© 2016-2024 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

## Learning Objectives

Understand Apache Spark Architecture

Learn Apache Spark Ecosystem Components

Learn Various Types of Spark Cluster Managers

## Agenda

Apache Spark Architecture

Unified Solution Stack

Apache Spark Shell and Session

Spark Examples

Summary

Apache Spark Architecture

## Lightning-fast Unified Analytics Engine

- Apache Spark™ is a fast and general-purpose engine for large- scale data processing
  - Speed
- Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk
- Ease of Use, Modularity and Extensibility
- Offers high-level APIs to users, such as Java, Scala, Python, R
- Generality
- Combine SQL , streaming , and complex analytics
- Runs Everywhere
- Spark runs on Hadoop, Mesos, standalone, or in the cloud It can access diverse data sources including HDFS, Cassandra, HBase, and S3

## Design Goals of Apache Spark

- Distributed Computation Engine
  - Based on JVM and Functional Programming;
  - Supports Immutability Streams Transformations;
  - Concise Syntax
- Goals:
  - cluster computing platform designed to be fast and general-purpose
  - support more types of computations ; designed to cover workloads including batch applications, iterative algorithms, interactive queries, and streaming
  - process large datasets using ‘ In-Memory ’ methods; makes it easy and inexpensive to combine different processing types to create data analysis pipelines
  - designed to be highly accessible , offering simple APIs in Python, Java, Scala, and SQL, and rich built-in libraries to integrate with other data tools

## Distributed Vs single-machine analysis

## High-level architecture

Apache Spark Extensions

GraphX

Graph Computations

SparkR

R on Spark

Spark ML

Machine Learning

Apache Spark Core Libraries

Apache Spark Core API

Resource Manger Layer (YARN Mesos Standalone …)

Data Storage Layer

(File SystemHDFSHBaseCassandraS3…)

| NODE | NODE | NODE | NODE | NODE | NODE |
| :--: | :--: | :--: | :--: | :--: | :--: |

Unified Solution Stack

## Unified Solution Stack

Reference: Learning Spark, 2nd Edition

## Spark End User Libraries

## Spark Core

Provides services such as memory pool, scheduling of cluster, massively parallel processing constructs, basic IO constructs and abstractions

Comprises of basic components such as RDD (Resilient Distributed Data Sets), Dataframes, Datasets and Graph Frames

The core APIs available perform operations on ETL basic abstractions

Shared or distributed variables can be created and managed Example of such are broadcast variables and accumulators

## RDD (Resilient Distributed Dataset)

- RDD (Resilient Distributed Dataset)
  - Resilient – if data in memory is lost, it can be recreated
  - Distributed – processed across the cluster
  - Dataset – collection of rows and columns - data can come from a file or
- from any other source
  - RDDs are the fundamental unit of data in Spark
  - Most Spark programming consists of performing operations on RDDs
- Contained in an RDD
  - Set of dependencies on parent RDDs – lineage
  - Set of partitions – Atomic pieces of a dataset
- Ways to create an RDD
  - From a file or set of files
  - From data in memory
  - From another RDD
  - From a DataFrame or DataSet
  - From Local Collection
  - From DataSources

## RDD, DataFrame, DataSet

Big Data E n gi neer i ng

- Spark Release
  - RDD :The RDD APIs have been on Spark since the 10 release
  - DataFrames :Spark introduced DataFrames in Spark 13 release
  - DataSet :Spark introduced Dataset in Spark 16 release
- Data Formats
  - RDD: It can easily and efficiently process data which is structured as well as unstructured
  - DataFrame: It works only on structured and semi-structured data
  - DataSet: It also efficiently processes structured and unstructured data It represents data in the form of JVM objects of row or a collection of row object Which is represented in tabular forms through encoders
- We can move RDD to DataFrame (if RDD is in tabular format) by toDF() method or we can do the reverse by the rdd method
- RDD offers low-level functionality and control The DataFrame and Dataset allow custom view and structure It offers high-level domain-specific operations, saves space, and executes at high speed

## Spark SQL

- Work with structured and semi-structured data such as Hive tables, MySQL tables, Parquet files, AVRO files, JSON files, CSV files, and more
- Spark SQL is one of the most technically involved components of Apache Spark
  - It powers both SQL queries and the DataFrame API
  - At the core of Spark SQL is the Catalyst optimizer , which leverages advanced programming language features (eg Scala’s pattern matching and quasiquotes) in a novel way to build an extensible query optimizer
- Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:
  - Easily add new optimization techniques and features to Spark SQL
  - Enable external developers to extend the optimizer
- (eg adding data source specific rules, support for new data types, etc)

## Spark Machine Learning

- Spark MLlib and ML are the Spark’s packages to work with machine-learning algorithms They provide the following:
  - Inbuilt machine-learning algorithms such as Classification, Regression, Clustering, and more
  - Features such as pipelining, vector creation, and more
- ML algorithms include:
  - Classification: logistic regression and naive Bayes
  - Regression: generalized linear regression, survival regression
  - Decision trees, random forests, and gradient-boosted trees
  - Recommendation: alternating least squares (ALS)
  - Clustering: K-means, Gaussian mixtures (GMMs)
  - Frequent item sets, association rules, and sequential pattern mining
- ML workflow utilities include:
  - Feature transformations: standardization, normalization, and hashing
  - ML Pipeline construction
  - Model evaluation and hyper-parameter tuning
  - ML persistence: saving and loading models and pipelines

## Spark Streaming

- Spark Streaming is a Spark component that enables processing of live streams of data
- Stream processing involves:
  - Input and output operations, transformations, persistence, and check pointing
- Supports different types of stream processing:
  - Both batch and window stream configurations
  - Stream check pointing and processing using additional tools such as Kafka and Flume
- Three Ways of Stream Processing
  - Spark module functionality (for example, SQL, MLlib, and GraphX)
  - External Systems such as Kinesis or ZeroMQ
  - Create custom receivers for user-defined data sources

## SparkR (R on Spark)

- SparkR is an R package that provides a light-weight frontend to use Apache Spark from R
- The main idea behind SparkR was to explore different techniques to integrate the usability of R with the scalability of Spark
- SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc but on large datasets SparkR also supports distributed machine learning using MLlib
- There are various benefits of SparkR:
  - Data Sources API
  - Scalability to many cores and machines

https:sparkapacheorgdocslatestsparkrhtml

## Apache Spark’s ecosystem of connectors

## Apache Spark Shell and Spark Session

## Spark Deployment Modes

|     Mode     |    Spark Driver     |           Spark Executor            |    Cluster Manager    |
| :----------: | :-----------------: | :---------------------------------: | :-------------------: |
|    Local     |  JVM, Single Node   |                 JVM                 |   Runs on same host   |
|  Standalone  | Any node in cluster |  Each node launches JVM in cluster  |  Any host on cluster  |
| YARN Client  |       Client        |    YARN NodeManager’s container     | YARN Resource Manager |
| YARN Cluster |   YARN App Master   |    YARN NodeManager’s container     | YARN Resource Manager |
|  Kubernetes  |   Kubernetes pod    | Each worker runs within its own pod |   Kubernetes Master   |

## Spark Shell

The Spark Shell provides interactive data exploration
Writing Spark applications without the shell will be covered later
Python: PySpark

## Spark Session

- An Apache Spark session provides a single point of entry to interact
  - with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs
  * Unified Entry Point : It's the central point to access all Spark functionalities
  * Data Reading and Writing from various sources (like HDFS, S3, JDBC, Hive, etc) and write data to various sinks
  * Configuration Management
  * Creating DataFrames and Datasets . DataFrames and Datasets are the core data structures in Spark
  * Execution of SQL Queries
  * Managing Spark Services : Spark services like SparkContext, and
    - it's the main point of interaction when dealing with structured data

## Spark Components

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/20240220133741.png?token=AKMX3HTZZRYI4GDC6IAS5X3F2Q5OI)
In Spark 2.0, the SparkSession became a unified conduit

- to all Spark operations and data
- Subsumes the SparkContext, SQLContext, HiveContext, SparkConf, and StreamingContext

## Spark Job, Stage, Task

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402201339142.png?token=AKMX3HUF4EI3PCWYEKBWL7LF2Q5VU)

## How is the data distributed across physical machines?

Reference: Spark: The Definitive Guide, Feb 2018

## How does the core (Spark function) get a partition of data to work on?

Reference: Spark: The Definitive Guide, Feb 2018

## Terminologies of Spark Cluster

Application User program built on Spark Consists of a driver program and executors on the cluster

A jar containing the user's Spark application In some cases users will want to create an "user jar" containing their application along with its dependencies The user's jar should never include Hadoop or Spark libraries, however, these will be added at runtime

The process running the main() function of the application and creating the SparkContext

Cluster m ana g er

An external service for acquiring resources on the cluster (eg standalone manager, Mesos, YARN)

Distinguishes where the driver process runs In " cluster " mode, the framework launches the driver inside of the cluster In " client " mode, the submitter launches the driver outside of the cluster

Any node that can run application code in the cluster

A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them Each application has its own executors

A unit of work that will be sent to one executor

A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (eg save, collect); you'll see this term used in the driver's logs

Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs

## Simple Spark Apps: WordCount

count how often each word appears in a collection of text documents

- This simple program provides a good test case for parallel processing, since it:
  - serves as a “Hello World” for Big Data apps
- A distributed computing framework that can run WordCount efficiently in parallel at scale can likely handle much larger and more interesting computing problems
  - We start with a SparkSession , the entry point for Spark functionality
  - The textFile method reads the data from a text file
  - The flatMap transformation splits each line into words
  - The map transformation maps each word to a pair (word, 1)
  - The reduceByKey transformation sums up the counts for each word
  - Finally, collect is an action that triggers the execution of the job

## Word Count (Imperative vs Declarative)

| ## Initialize a SparkSession Imperative<br >from pyspark.sql import SparkSession<br ><br >spark = SparkSession.builder.appName("WordCountExample").getOrCreate() ## Read a text file<br >textFile = spark.sparkContext.textFile("pathtoyourtextfile.txt") ## Split each line into words and assign each word a count of 1 words = textFile.flatMap(lambda line: line.split(" "))<br >wordCounts = words.map(lambda word: (word, 1))<br >## Reduce by key to count the occurrences of each word wordCounts = wordCounts.reduceByKey(lambda a, b: a + b) ## Collect the results |                                                                                                                                                                                                                                                                                                                                             |                                                 |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------: |
|                                                                                                                                                                                                             results = wordCounts.collect() ## Print the word counts<br >for word, count in results:<br >print(word, count) ## Stop the SparkSession spark.stop()                                                                                                                                                                                                              |                                                                        from pyspark.sql import SparkSession spark = SparkSession.builder.appName(“ ## Chain transformations to count the w wordCounts = (<br >spark.sparkContext<br >.textFile("pathtoyourtextfile.t                                                                        | <br >WC").getOrCreate() ords<br ><br ><br >xt") |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | .flatMap(lambda line: line.split(" "))<br >.map(lambda word: (word, 1))<br >.reduceByKey(lambda a, b: a + b)<br >)<br >## Collect the results<br >results = wordCounts.collect() ## Print the word counts<br >for word, count in results: print(word, count)<br >## Stop the SparkSession<br >spark.stop() Declarative: Functional chaining |                                                 |

## More Application Examples

In-memory data mining on Hive data (Conviva)

Log analysis applications

Weather TimeSeries Data Application

Predictive analytics (Quantifind)

City traffic prediction (Mobile Millennium); Twitter spam

classification (Monarch); Collaborative filtering via matrix factorization

Game industry, processing and discovering patterns from the potential firehose of real-time in-game events

e-commerce industry, real-time transaction information could be

passed to a streaming clustering algorithm like k-means or collaborative filtering like ALS

Finance or security industry, the Spark stack could be applied to a fraud or intrusion detection system or risk-based authentication

https:githubcomdatabricksreference-apps [http:sparkapacheorgexampleshtml](http:spark.apache.orgexamples.html)

Spark Deployment

## Spark on YARN (Cluster Mode)

## The Spark Application

ATABA-BEADCourseConductIntroducing Apache Spark Framework

All rights reserved

## DAG visualization

Driver (Define RDD objects)

rdd1join(rdd2) groupby(…)filte r()…

DAG Scheduler (Like a queue planner)

Split graph into stages of tasks

Task Scheduler

Launch tasks via cluster manager

Worker

Executer tasks

Store and serve block

Cluster Manager

Threads

Block Manager

When creating RDDs, Spark generates two stage for each of RDD, shown as Stage 0 and Stage 1

Map is narrow operation, it includes in Stage 0 and Stage 1

Join is wide operation, Spark generates another stage – Stage 3

Filter and mapPartitions are narrow operations, both include in Stage 3

## Essential Points

- Spark provides a simple, efficient, and powerful programming model for a wide range of apps
  - There are a range of options available that allow the cluster creator to define attributes such as cluster size and storage type
  - Every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster
  - Spark has the concept of a big data pipeline — from ETL to Analytics

# Functionl Thinking

## Programming Languages Through The Years

ATABA-BEADFunctional Thinking

All rights reserved

## Imperative programming

Procedural vs Object Oriented Programming

Data and Code

Global Data (variables)

Global Data (variables)

Object

Properties

Methods

Object

Properties

Methods

|    General Function    |     |    General Function    |     |    General Function    |
| :--------------------: | :-: | :--------------------: | :-: | :--------------------: |
|           -            |     |           -            |     |           -            |
| Local data (variables) |     | Local data (variables) |     | Local data (variables) |

Object

Properties

Methods

One of the most important characteristics of procedural programming is that it relies on procedures that operate on data - these are two separate concepts In object - oriented programming , these two concepts are bundled into objects This makes it possible to create more complicated behavior with less code

ATABA-BEADFunctional Thinking

All rights reserved

Declarative programming

## Functional Programming

Data Data Data – Everywhere!!!!

Functional programming is a

programming paradigm is a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data

[https:mediumcom@richardengfp-is-for-nerds-6ed1ca43bb34](https:medium.com%40richardengfp-is-for-nerds-6ed1ca43bb34)

ATABA-BEADFunctional Thinking

All rights reserved

Object Oriented

ATABA-BEADFunctional Thinking

All rights reserved

## Principles of declarative programming

In declarative programming,

We rely on primitives that are much closer to our domain

We are able to create primitives as we go

The language itself can be regarded as a domain-specific language ( DSL )

We can identify patterns and make them reusable Don’t repeat yourself (DRY Principle) We can look for frequent operations and create patterns around the Example: Filtering is important

We can pass functions to other functions inline, without first defining them Treating functions as if they are ordinary variables allow us to achieve a new level of abstraction

We can create anonymous functions , functions as first-class citizens, and custom operator specifications

ATABA-BEADFunctional Thinking

All rights reserved

## Uniqueness of Functional Programming

- Currying: A single argument can translate multiple arguments in the series of functions
- Type Interference: The FP languages are intelligent to reduces the efforts made during the programming
  - We don’t have to mention the return type of function and data types in a clear and detailed manner these kind of stuffs will be accomplished by the programming itself
- Immutability: The already declared variables values can’t be modified, this feature is known as Immutability

ATABA-BEADFunctional Thinking

All rights reserved

## Functions have no side effects

Functional languages such as Standard ML, Scheme and Scala do not restrict side effects, but it is customary for programmers to avoid them The functional language Haskell expresses side effects such as IO and other stateful computations using monadic actions

What are side effects? A function has a side effect if it does something other than simply return a result, for example:

Modifying a variable

Modifying a data structure in place

Setting a field on an object

Throwing an exception or halting with an error

Printing to the console or reading user input

Reading from or writing to a file

Drawing on the screen

ATABA-BEADFunctional Thinking

All rights reserved

## Pure Functions

- A pure function depends only on (a) its declared input parameters and (b) its algorithm to produce its result A pure function has no “back doors,” which means:
  - Its result can’t depend on reading any hidden value outside of the function scope, such as another field in the same class or global variables
  - It cannot modify any hidden fields outside of the function scope, such as other mutable fields in the same class or global variables
  - It cannot depend on any external IO It can’t rely on input from files, databases, web services, UIs, etc; it can’t produce output, such as writing to a file, database, or web service, writing to a screen, etc
- A pure function does not modify its input parameters

Higher-Order Function (HOF) basically means that

you can treat a function as a value (val) — just like you can treat a String as a value

you can pass that value into other functions

“ Recursion is a requirement of functional programming”

ATABA-BEADFunctional Thinking

All rights reserved

## Functional Programming

FP prefers pure functions

FP avoids side effects

Functions are first class objects

FP prefers immutable objects

FP prefers iterators over lists

FP favours lazy evaluation

FP avoids traditional control constructs such as loops and if

statements

FP often uses recursion to avoid loops

FP uses higher order functions to define new functions

ATABA-BEADFunctional Thinking

All rights reserved

## Functional Constructs

- Writing pure functions means that we focus on functions that take immutable data and produce new immutable data as output
- An anonymous function (function literal, lambda abstraction,
- or lambda expression) is a function definition that is not bound to an identifier
- Type inference means that we have the option to decide whether to specify a type or not
- A closure is a programming abstraction that is able to capture all the free variables defined in its context
- A partial application consists in calling a function with fewer parameters than the ones declared in its signature
  - The return value is simply a function that takes as input the remaining parameters
- Tail recursion is a particular kind of recursion that can reuse the same stack frame for all the recursive function calls
- Pattern matching is a powerful facility that allows to construct and deconstruct data types to control execution flows

ATABA-BEADFunctional Thinking

All rights reserved

## Benefits of Functional Programming

- Pure functions are easier to reason about
- Testing is easier, and pure functions lend themselves well to techniques like property-based testing
- Debugging is easier
- Programs are more bulletproof
- Programs are written at a higher level, and are therefore easier to comprehend
- Function signatures are more meaningful
- Parallel concurrent programming is easier
- Additionally Scala is:
  - Concise, readable and also collection classes have functional APIs
  - Scala runs on JVM using the wealth of libraries and tools alongside

ATABA-BEADFunctional Thinking

All rights reserved

## Disadvantages of Functional Programming

Writing pure functions is easy, but combining them into a complete application is where things get hard

The advanced math terminology (monad, monoid, functor, etc) makes FP intimidating

For many people, recursion doesn’t feel natural

Because you can’t mutate existing data, you instead use a pattern that I call, “Update as you copy”

Pure functions and I O don’t really mix

Using only immutable values and recursion can potentially lead to performance problems, including RAM use and speed

GUIs and Pure FP are not a good fit

Note: “ReactiveX is a combination of the best ideas from the Observer pattern, the Iterator pattern, and Functional programming It is a library for composing asynchronous and event-based programs by using observable sequences”

ATABA-BEADFunctional Thinking

All rights reserved

## GitHub and Stack Overflow are used here because of their size and second because of their public exposure of the data necessary for the analysis.

[https:redmonkcomsogrady20221020language-rankings-6-22](https:redmonk.comsogrady20221020language-rankings-6-22)

ATABA-BEADFunctional Thinking

All rights reserved

## Ranking

1 JavaScript Big Data

Python

Java

PHP

C#

CSS

7 C++

TypeScript

Ruby

C

Swift

R

12 Objective-C

Shell

Scala

15 Go

17 PowerShell

17 Kotlin

19 Rust

19 Dart

ATABA-BEADFunctional Thinking

All rights reserved

## Why Scala?

- Scala is a type-safe language that relies on the JVM runtime
  - Scala is being used by numerous tech companies, including Netflix, Twitter, LinkedIn, AirBnB, AT&T, eBay, and even Apple It's also used in finance, by the likes of Bloomberg and UBS
- Scala supports lazy evaluation
  - Scala is not backward compatible
- Scala variables are by default immutable type
  - In Scala, all the operations on entities are done by using method calls (there is no static)

|                 |   Lightweight   |            |
| :-------------: | :-------------: | :--------: |
| Object Oriented | <br ><br >Scala | Functional |
|                 |   Performant    |            |

ATABA-BEADFunctional Thinking

All rights reserved

## Concise.. For example a class

public class Person { public final String name ; public final int age ;

Person(String name, int age) { this name = name;

this age = age;

}

}

class Person( val name: String,

val age: Int ) {}

ATABA-BEADFunctional Thinking

All rights reserved

## Pattern Matched … For example

import javautilArrayList;

Person[] people ; Person[] minors ; Person[] adults ;

in Java:

( people [i] age < 18 ? minorsList : adultsList)

add( people [i]);

|  {  | ArrayList<Person> | minorsList = new | ArrayList<Person>(); |
| :-: | :---------------: | :--------------: | :------------------: |
|     | ArrayList<Person> | adultsList = new | ArrayList<Person>(); |
|     | for (int i = 0; i | < people.length; |         i++)         |

minors = minorsListtoArray( people ); adults = adultsListtoArray( people );

An infix method call

A function value

val people: Array [Person]

val (minors, adults) = people partition (age < 18)

A simBig DpatalEenginpeeraingtFtoreAnralnyticsm© 20a16t-20c24hNUS All rights reserved

ATABA-BEADFunctional Thinking

## What People Say of Scala?

If I were to pick a language to use today other than Java, it would be Scala”

- James Gosling, creator of Java

“Scala, it must be stated, is the current heir apparent to the Java throne No other language on the JVM seems as capable of being a "replacement for Java" as Scala, and the momentum behind Scala is now unquestionable

While Scala is not a dynamic language, it has many of the characteristics of popular dynamic languages, through its rich and flexible type system, its sparse and clean syntax, and its marriage of functional and object paradigms”

Charles Nutter, creator of JRuby

“I can honestly say if someone had shown me the Programming in Scala book by Martin Odersky, Lex Spoon & Bill Venners back in 2003 I'd probably have never created Groovy”

James Strachan, creator of Groovy

ATABA-BEADFunctional Thinking

All rights reserved

## Understanding Python

Python is not a purely functional language, and a strict definition isn't helpful Instead, we'll identify some common features that are indisputably important in functional programming

- Functional programming language compilers can optimize these kinds of simple recursive functions Python can't do the same optimizations
  - Functional programming defines a computation using expressions and evaluation; often these are encapsulated in function definitions
  - It de-emphasizes or avoids the complexity of state change and mutable objects
  - Python is a hybrid of imperative and functional programming; some parts of Python don't allow purely functional programming ; however we can attempt to create hybridized functional programming in Python

ATABA-BEADFunctional Thinking

All rights reserved

## (Avoid) Flow Control

- Regular imperative constructs to be avoided
  - Example a block of code generally consists of some outside loops (for or while)
  - Example assignment of state variables within those loops, modification of data structures like dicts, lists, and sets (or various other structures, either from the standard library or from third-party packages)
  - Example some branch statements (ifelifelse or tryexceptfinally)

ATABA-BEADFunctional Thinking

All rights reserved

## In Summary . . .

“Functional code is characterised by one thing: the absence of side

- effects”
- Language features that aid functional programming are immutable data, first class functions and tail call optimizations
- Programming techniques used to write functional code are mapping, reducing, recursing, currying, and use of higher order functions
- Advantageous properties of functional programming are
- parallelization, lazy evaluation and determinism
- A pure function has no side effects – meaning it doesn’t rely on data outside the current function, and it doesn’t change data that exists outside the current function
  - Every other ‘functional’ thing can be derived from this property
- Reference: A Practical Introduction to Functional Programming by Mary Rose Cook

ATABA-BEADFunctional Thinking

All rights reserved

“People think that computer science is the art of geniuses but the actual reality is the opposite, just many people doing things that build on each other, like a wall of mini stones”

– Donald Knuth (computer scientist)

ATABA-BEADFunctional Thinking

All rights reserved

## Books You May Enjoy. . .

- Mastering Functional Programming by Anatolii Kmetiuk, Packt Publishing, August 2018, ISBN: 9781788620796
- Programming in Scala, 3rd ed, Updated for Scala 212, by Martin Odersky, Lex Spoon, Bill Benners
- Functional Programming in Scala, by Paul Chiusano, Rúnar Bjarnason, Manning
  - Scala for the Impatient, by Cay S Horstmann, Addison-Wesley
  - Programming Scala, Updated for Scala 211, by Alex Payne, Dean Wampler, O’Reilly
- Scala in Depth, by Joshua D Suereth, Manning
- Functional Python Programming - Second Edition, by Steven F Lott, Published by Packt Publishing, 2018
- Functional Programming in Python by Martin McBride Published by Packt Publishing, 2019
- Functional Programming in Scala By Paul Chiusano and Rúnar Bjarnason, 2014

ATABA-BEADFunctional Thinking

All rights reserved

Functional Thinking in Python

Suria R Asai ( [sur](mailto:suria@nus.edu.sg) [i](mailto:suria@nus.edu.sg) [a@nusedusg](mailto:suria@nus.edu.sg) ) NUS-ISS

Total Slides: 54

© 2016-2024 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Learning Objectives

- Understand that Python is a multi-paradigm language; it's a mixture of procedural and functional programming
- Understand pure functions, the concept of immutability, and referential transparency
  - First-class and higher-order functions , which are sometimes known as pure
- functions
  - Immu t able data
  - Strict and non-strict evaluation We can also call this eager versus lazy evaluation
  - Recursion instead of an explicit loop state
  - Functional type systems

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Agenda

- Basic Constructs
- Essential Functional Concepts
  - Immutable Data Set
  - Filter, Map, Currying and Reduce
  - Lambda
  - Recursion
- Higher Order Functions
- Data and Compute
  - Tuples and Strings
  - Generators, functtools and itertools
- PyCharm IDE
- Summary

ATABA-BEADFunctional Thinking in Python

All rights reserved

Basic Constructs

““Give someone a program, you frustrate them for a day; teach

them how to program, you frustrate them for a lifetime”

~ David Leinwebertis

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Literals and Keywords

- Every object has an identity, a type and a value
- Literals:
  - Numeric literals: integers , floating point, and imaginary numbers ( [numbersNumbe](https:docs.python.org3.6librarynumbers.html#numbers.Number) [r](https:docs.python.org3.6librarynumbers.html#numbers.Number) [ ](https:docs.python.org3.6librarynumbers.html#numbers.Number))
  - String, Byte and Formatted String Literals
- Keywords & Constants

| False  |  class   | finally |    is    | return |
| :----: | :------: | :-----: | :------: | :----: |
|  None  | continue |   for   |  lambda  |  try   |
|  True  |   def    |  from   | nonlocal | while  |
|  and   |   del    | global  |   not    |  with  |
|   as   |   elif   |   if    |    or    | yield  |
| assert |   else   | import  |   pass   |        |
| break  |  except  |   in    |  raise   |        |

False True None NotImplemented Ellipsis quit() exit()

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Data Types

Python Data Types

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Operators

- Arithmetic operators
  - addition (+), subtraction (-), multiplication (\*), division (), and remainder (%)
- Relational operators
- • ==, !=, >, <, >= and <=
- Logical operators
  - NOT, AND, OR

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Built in Functions

| Built-in Functions |             |              |            |                |
| :----------------: | :---------: | :----------: | :--------: | :------------: |
|       abs()        |   dict()    |    help()    |   min()    |   setattr()    |
|       all()        |    dir()    |    hex()     |   next()   |    slice()     |
|       any()        |  divmod()   |     id()     |  object()  |    sorted()    |
|      ascii()       | enumerate() |   input()    |   oct()    | staticmethod() |
|       bin()        |   eval()    |    int()     |   open()   |     str()      |
|       bool()       |   exec()    | isinstance() |   ord()    |     sum()      |
|    bytearray()     |  filter()   | issubclass() |   pow()    |    super()     |
|      bytes()       |   float()   |    iter()    |  print()   |    tuple()     |
|     callable()     |  format()   |    len()     | property() |     type()     |
|       chr()        | frozenset() |    list()    |  range()   |     vars()     |
|   classmethod()    |  getattr()  |   locals()   |   repr()   |     zip()      |
|     compile()      |  globals()  |    map()     | reversed() |   import ()    |
|     complex()      |  hasattr()  |    max()     |  round()   |                |
|     delattr()      |   hash()    | memoryview() |   set()    |                |

ATABA-BEADFunctional Thinking in Python

All rights reserved

def scopetest(): def dolocal():

spam = "local spam"

def dononlocal():

nonlocal spam

spam = "nonlocal spam"

def doglobal():

global spam

spam = "global spam"

spam = "test spam" dolocal()

print("After local assignment:", spam) dononlocal()

print("After nonlocal assignment:", spam)

doglobal()

print("After global assignment:", spam)

scopetest()

print("In global scope:", spam)

## Scope

After local assignment: test spam

After nonlocal assignment: nonlocal spam After global assignment: nonlocal spam In global scope: global spam

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Simple Statements

Expression statements

Assignment statements

assert statement

pass statement

del statement

return statement

yield statement

raise statement

break statement

continue statement

import statement

global statement

nonlocal statement

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Compound Statements – If

The [if](https:docs.python.org3.6referencecompoundstmts.html#if) , [while](https:docs.python.org3.6referencecompoundstmts.html#while) [ ](https:docs.python.org3.6referencecompoundstmts.html#while)and [for](https:docs.python.org3.6referencecompoundstmts.html#for) [ ](https:docs.python.org3.6referencecompoundstmts.html#for)statements implement traditional control flow constructs

> > > x = int(input("Please enter an integer: ")) Please enter an integer: 42

> > > if x < 0:

    x = 0

    print('Negative changed to zero')

elif x == 0:

    print('Zero')

elif x == 1:

    print('Single')

else :

    print('More')

More

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Compound Statements – Loops

> > > ## Measure some strings:

words = ['cat', 'window', 'defenestrate']

> > > for w in words:

> > > for i in range(5):

cat 3

window 6

print(w, len(w))

range(5, 10)

5, 6, 7, 8, 9

range(0, 10, 3)

0, 3, 6, 9

range(-10, -100, -30)

-10, -40, -70

> > > ## Fibonacci series:

## the sum of two elements defines the next

a, b = 0, 1

> > > while b < 10:

> > > for n in range(2, 10):

for x in range(2, n):

if n % x == 0:

print(n, 'equals', x, '\*', nx)

break

1

1

2

3

5

8

print(b)

a, b = b, a+b

## loop fell through without finding a factor print(n, 'is a prime number')

is a prime number

is a prime number

5 is a prime number

7 is a prime number

equals 2 \* 4

equals 3 \* 3

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Data Type Libraries

datetime — Basic date and time types

calendar — General calendar-related functions

collections — Container datatypes

collectionsabc — Abstract Base Classes for Containers

heapq — Heap queue algorithm

bisect — Array bisection algorithm

array — Efficient arrays of numeric values

weakref — Weak references

types — Dynamic type creation and names for built-in types

copy — Shallow and deep copy operations

pprint — Data pretty printer

reprlib — Alternate repr() implementation

enum — Support for enumerations

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Numeric Libraries and Others

- Numeric and Mathematical Modules
  - numbers — Numeric abstract base classes
  - math — Mathematical functions
  - cmath — Mathematical functions for complex numbers
  - decimal — Decimal fixed point and floating point arithmetic
  - fractions — Rational numbers
  - random — Generate pseudo-random numbers
  - statistics — Mathematical statistics functions
- File and Directory Access, OS, Development & Interpreter Services
- Data Persistence, Data Compression & Archive Services
- File Formats, Cryptographic Services
- Concurrency, IPC, Internet, Markup Support Services
- Multimedia, UI, I18n, Performance and Runtime Services

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Class and Instance Variables

instance variables are for data unique to each instance

class variables are shared by all instances

| class Dog:<br ><br >kind = 'canine' ## class var shared by all instances<br ><br >def init (self, name):<br >self.name = name<br >self.tricks = [] ## instance var creates a new empty list for each dog |                                                                                                                  |     |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------: | :-: |
|                                                                           def addtrick(self, trick): self.tricks.append(trick)                                                                           |                   >>> d = Dog('Fido')<br >>>> e = Dog(‘Dido')<br >>>> d.addtrick('roll over')                    |     |
|                                                                                                                                                                                                          | >>> e.addtrick('play dead')<br >>>> d.tricks ['roll over']<br >>>> e.tricks ['play dead']<br >>>>d.kind 'canine' |     |

ATABA-BEADFunctional Thinking in Python

All rights reserved

Essential Functional Concepts

“While functions being unable to change state is good because it helps us reason about our programs, there's one problem with that If a function can't change anything in the world, how is it supposed to tell us what it calculated?"

~Miran Lipovaca

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Lambda is simple

- Lambdas are used to emphasize succinct definitions of simple functions
  - Anything more complex than a one-line expression requires the def statement

mult35 = lambda x: x%3==0 or x%5==0

print(mult35(3)) print(mult35(4)) print(mult35(5))

Lambdas can only contain a single Python expression

A lambda expression can have any number of arguments (including none),

lambda: 1 ## No arguments

lambda x, y: x + y

lambda a, b, c, d: a*b + c*d

def add1():

return lambda x: x + 1

f = add1() print(f(2))

Function as return value

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Simple Pure Functions

- No Side effects, return value only, shallow copy
- Functional programming is a way of writing software applications using only pure functions and immutable values
- A pure function in programming is a function that has the following properties:
  - Deterministic : It returns the same result every time it's called with the same set of input values
  - No Side Effects : It does not cause any observable side effects, meaning it does not alter any external state (like global variables, inputoutput devices, etc)

def square(number):

"""Returns the square of the given number"""

return number \* number

## Example usage: result = square(5) print(result)

Does not modify any external state or have any side effects; it simply computes and returns the square of the given number

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Pure Functions on Collections

Similar principles of being deterministic with no side effects

One common operation on a list of tuples is to transform each tuple in some way and return a new list of tuples

def squaretuples(tupleslist):

#Returns a new list of tuples with each number squared

return [(x**2, y**2) for x, y in tupleslist]

## Example usage:

originallist = [(1, 2), (3, 4), (5, 6)] result = squaretuples(originallist) print(result)

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Pure Function on Numeric Data

The function sumnestednumbers handles integers, tuples, and lists

If the input is an integer, it simply returns the integer

If the input is a tuple or a list, it recursively processes each element

For any other type, it returns 0 (assuming we're only interested in

integers)

def sumnestednumbers(data):

#Recursively sums numbers in a nested structure of tuples, lists, and integers if isinstance(data, int):

return data

elif isinstance(data, (tuple, list)):

return sum(sumnestednumbers(item) for item in data) return 0

## Example usage:

nesteddata = (1, 2, [3, 4, (5, 6)], (7, (8, 9)))

result = sumnestednumbers(nesteddata)

print(result) ## Output: 45

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Pure Function on Categorical Data

This function takes a list of tuples where each tuple contains a category and some value, and it groups values by their categories

def groupbycategory(data):

"""Groups values by their categories given a list of (category, value) tuples""" groupeddata = {}

for category, value in data:

if category in groupeddata: groupeddata[category]append(value)

else:

groupeddata[category] = [value] return groupeddata

## Example usage:

data = [('fruit', 'apple'), ('fruit', 'banana'), ('vegetable', 'carrot'), ('fruit', 'orange')] grouped = groupbycategory(data)

print(grouped) ## Output: {'fruit': ['apple', 'banana', 'orange'], 'vegetable': ['carrot']}

They always produce the same output for the same input (deterministic)

They don't modify the input data or any external state (no side

effects)

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Type of Functions

- We need to distinguish between two broad species of functions, as follows:
  - Scalar functions : They apply to individual values and compute an individual result Functions such as abs(), pow(), and the entire math module are examples of scalar functions
  - Collection functions : They work with iterable collections
- We can further subdivide the collection functions into three subspecies:
  - Reduction : This uses a function to fold values in the collection together, resulting in a single final value
- For example, if we fold (+) operations into a sequence of integers, this will compute the sum This can be also be called an aggregate function, as it produces a single aggregate value for an input collection
- Mapping : This applies a scalar function to each individual item of a collection; the result is a collection of the same size
- Filter : This applies a scalar function to all items of a collection to reject some items and pass others The result is a subset of the input

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Imperative vs Functional Constructs

First-class and higher-order functions, aka pure functions

are succinct and expressive

- Practical Benefits:
  - Distributed(Big) Datasets
  - Parallel Processing
  - Formal provability
  - Modularity
  - Composability
  - Ease of debugging testing
- Prefers Immutable data

F u n ctional P i pe line

- Strict and non-strict evaluation We can also call this eager versus lazy evaluation
- Recursion instead of an explicit loop state
- Uses Functional type systems
- A Python lambda is a pure function
  - While this isn't a highly recommended style, it's possible to create pure functions through the lambda objects

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Filtering Example

Filter is a generator expression that returns an iterator from a sequence that meet a certain condition (predicate)

## Python program to demonstrate working of filter ## function that filters vowels

def fun(variable):

letters = ['a', 'e', 'i', 'o', 'u']

if (variable in letters):

return True

else:

return False

## sequence

sequence = ['g', 'e', 'e', 'j', 'k', 's', 'p', 'r']

## using filter function

filtered = filter(fun, sequence)

print('The filtered letters are:')

for s in filtered:

print(s)

Functional Pipeline

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Map Example

Map is a generator expression that returns an iterator over a sequence

| ## Python program to demonstrate working of map. ## Return double of n<br >def addition(n):<br >return n + n<br ><br >## We double all numbers using map() numbers = (1, 2, 3, 4)<br >results = map(addition, numbers) |     |     |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-: | :-: |
|                                            ## Does not print the value<br >print(results)<br >## For printing value<br >for result in results:<br >print(result, end = " ")                                            |     |     |
|                                                                                                                                                                                                                        |     |     |

Functional Pipeline

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Currying Example

Currying is a function transforms multiple function into execution of sequential functions

## Demonstrate Currying of composition of function def change(b, c, d):

def a(x):

return b(c(d(x)))

return a

def kilometer2meter (dist):

""" Function that converts km to m """ return dist \* 1000

def meter2centimeter (dist):

""" Function that converts m to cm """ return dist \* 100

def centimeter2feet (dist):

""" Function that converts cm to ft """ return dist 3048

if name == ' main ':

transform = change(centimeter2feet, meter2centimeter, kilometer2meter ) e = transform(565)

Functional Pipeline

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Reduce Example

Reduce is a function that accepts a function and a sequence and returns a single value calculated cumulatively

from functools import reduce def dosum(x1, x2):

return x1 + x2

print(reduce(dosum, [1, 2, 3, 4]))

Functional Pipeline

import functools import operator

## initializing list

lis = [ 1 , 3, 5, 6, 2, ]

## using reduce and operator

print ("The sum of the list elements is : ",end="") print (functoolsreduce(operatoradd,lis))

print ("The product of list elements is : ",end="") print (functoolsreduce(operatormul,lis))

print ("The concatenated product is : ",end="")

print (functoolsreduce(operatoradd,["geeks","for","geeks"]))

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Turtles - all the way down!!!

Turtles All the Way Down

- Our functional Python programs will rely on the
- following three stacks of abstractions:
  - Our applications will be functions—all the way down—until we hit the objects
  - The underlying Python runtime environment that supports our functional programming is objects—all the way down— until we hit the libraries
  - The libraries that support Python are a turtle on which Python stands

All programming languages rest on abstractions, libraries, frameworks and virtual machines These abstractions, in turn, may rely on other abstractions, libraries, frameworks and virtual machines

The most apt metaphor is this: the world is carried on the back of a giant turtle The turtle stands on the back of another giant turtle And that turtle, in turn, is standing on the back of yet another turtle

It's turtles all the way down

- Anonymous

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Factorial Example

A slightly less trivial example, factorial in recursive and iterative style:

| def factorialI(N):<br >"Iterative factorial function" assert isinstance(N, int) and N >= 1 product = 1 |                             |                                                                                                                             |
| :----------------------------------------------------------------------------------------------------: | :-------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |
|                       while N >= 1:<br >product \*= N N -= 1<br >return product                        | def fact<br >"Rec asse retu | orialR(N):<br >ursive factorial function"<br >rt isinstance(N, int) and N >= 1<br >rn 1 if N <= 1 else N \* factorialR(N-1) |
|                                                                                                        |                             |                                                                                                                             |

A higher order function implementation from libraries

from functools import reduce

from operator import mul

def factorialHOF(n):

return reduce(mul, range(1, n+1), 1)

ATABA-BEADFunctional Thinking in Python

All rights reserved

## OOP to Recursion

- In a functional sense, the sum of the multiples of three and five can be defined in two parts:
  - The sum of a sequence of numbers
  - A sequence of values that pass a simple test condition, for example, being multiples of three and five
- The sum of a sequence has a simple, recursive definition:

class SummableList(list):

def sum(self): s = 0

for v in self: s += v

return s

def sumr(seq):

if len(seq) == 0: return 0

return seq[0] + sumr(seq[1:])

Similarly, a sequence of values can have a simple, recursive definition, as follows:

def until(n, filterfunc, v):

if v == n: return []

if filterfunc(v): return [v] + until(n, filterfunc, v+1) else: return until(n, filterfunc, v+1)

ATABA-BEADFunctional Thinking in Python

All rights reserved

Higher Order Functions

"Object oriented programming makes code understandable by encapsulating moving parts Functional programming makes code understandable by minimizing moving parts"

~ Michael Feathers

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Functional Programming Techniques

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Functions are first class objects

Functions are objects that are created by def statements and can be manipulated by other Python functions We can also create a function as a callable object or by assigning a lambda object to a variable

- Python encourages focus on tuples, named tuples and
- immutable collections such as strings
- Python imposes a recursion limit , and doesn't automatically
- handle Tail Call Optimization (TCO)
  - We must optimize recursions manually using a generator expression
  - Use generator and generator expressions to work with collection of objects
- Generator expressions will perform the following tasks:
  - Conversions
  - Restructuring
  - Complex calculations

ATABA-BEADFunctional Thinking in Python

All rights reserved

## strings

- Python strings are immutable
  - str object has a number of methods to manipulate strings and they are pure functions
  - The syntax for str method functions is postfix, where most functions are prefix
  - For example, in this expression, len(variabletitle()) , the title() method is in postfix notation and the len() function is in prefix notation
- A simple prefix function to strip punctuation

def remove(str: Text, chars: Text) -> Text:

if chars:

return remove( strreplace(chars[0], ""), chars[1:]

)

return str

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Tuples and Named Tuples

- Python tuples are immutable objects, suitable for functional programming
  - A tuple has very few method functions, using prefix syntax
  - There are a number of use cases for tuples, particularly when working with list-of-tuple , tuple-of-tuple , and generator-of-tuple constructs
  - The namedtuple class adds an essential feature to a tuple: a name that we can use instead of an index

red = lambda color: color[0]green = lambda color:

color[1]blue = lambda color: color[2]

from typing import Tuple , Callable RGB = Tuple[int, int, int]

from typing import NamedTuple

class Color(NamedTuple):

red: Callable[[RGB], int] = lambda color: color[0]

"""An RGB color"""

red: int green: int blue: int name: str

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Function composition - 1

- Creating a new function by combining two or more functions, where the output of one function becomes the input of another
  - Let's define two functions, f and g, and then create a third function h which is the composition of f and g In this example, f(x) = x + 2 and g(x) = x \* 3 The composed function h will first apply g and then f to

def f(x) :

return x + 2

def g(x) :

return x \* 3

## Composing the functions

def h(x) :

return f(g(x))

## Test the composed function print(h(4))

## This will first multiply 4 by 3, and then add 2 to the result

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Function composition – 2.

Alternatively, use the functool s module

from functools import reduce

def compose(\*functions): def composetwo(f, g):

return lambda x: f(g(x))

return reduce(composetwo, functions, lambda x: x)

## Example usage

composedfunction = compose(f, g)

print(composedfunction(4)) ## Output will be the same as h(4) above

In this more general approach, compos e takes any number of functions as arguments and returns a new function that is their composition

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Functional Currying

- A technique of transforming a function that takes multiple arguments into a sequence of functions, each with a single argument
  - This is achieved by returning a new function that expects the next
- argument of the sequence
  - Currying is useful for creating more specific functions from general ones and for function composition

When you call multiply(5), it returns a new function innermultiply that takes the second argument When you later call curriedmultiply(3), it's equivalent to multiply(5)(3)

def multiply(x):

def innermultiply(y): return x \* y

return innermultiply

## Curried function curriedmultiply = multiply(5)

## Now you can use the curried function

print(curriedmultiply(3)) ## Output: 15

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Higher Order Functions

A higher-order function is simply a function that takes one or more functions as arguments andor produces a function as a result

- higher-order functions provide building blocks to express complex concepts by combining simpler functions into new functions
  - They allow chaining and combining higher-order functions

def compose(\*funcs):

"""Return a new function compose(f,g,)(x) == f(g((x)))""" def inner(data, funcs=funcs):

result = data

for f in reversed(funcs): result = f(result)

return result return inner

times2 = lambda x: x\*2 minus3 = lambda x: x-3 mod6 = lambda x: x%6

f = compose(mod6, times2, minus3)

print(all(f(i)==((i-3)\*2)%6 for i in range(1000000)))

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Types of Higher order functions

- There are three varieties of higher-order functions as follows:
  - Functions that accept functions as one (or more) of their arguments
  - Functions that return a function
  - Functions that accept a function and return a function, a combination of the preceding two features
- The max() and min() functions each have a dual life
  - They are simple functions that apply to collections They are also higher-

order functions

from tripdata import (

floatfrompair, latlonkml, limits, haversine, legs

)

path = floatfrompair(floatlatlon(rowiterkml(source))) trip = tuple(

(start, end, round(haversine(start, end), 4)) for start, end in legs(iter(path)))

long = max (dist for start, end, dist in trip) short = min (dist for start, end, dist in trip)

print(long) print(short)

print( max (1, 2, 3))

print( max ((1,2,3,4)))

t = """2 3

29 31 37 41

73 79 83 89

127 131 137 139

179 181 191 193

7 11 13 17 19 23

43 47 53 59 61 67 71

97 101 103 107 109 113

149 151 157 163 167 173

197 199 211 223 227 229"""

data = list(v for line in tsplitlines() for v in linesplit())

print(list( map (int, data)))

ATABA-BEADFunctional Thinking in Python

All rights reserved

Advanced Data & Compute

"In order to understand recursion, one must first understand

recursion (Anonymous)"

ATABA-BEADFunctional Thinking in Python

All rights reserved

## itertools — Functions creating iterators for efficient looping - 1

Iterator building blocks inspired by Haskell, recast to Python

Infinite Iterators

| Iterator |   Arguments   |                    Results                     |                Example                |
| :------: | :-----------: | :--------------------------------------------: | :-----------------------------------: |
| count()  | start, [step] |      start, start+step, start+2\*step, …       |   count(10) --> 10 11 12 13 14 ...    |
| cycle()  |       p       |           p0, p1, … plast, p0, p1, …           | cycle('ABCD') --> A B C D A B C D ... |
| repeat() |   elem [,n]   | elem, elem, elem, … endlessly or up to n times |      repeat(10, 3) --> 10 10 10       |

Combinatorics Iterators

|            Iterator            |        Arguments        |                                                                  Results                                                                  |
| :----------------------------: | :---------------------: | :---------------------------------------------------------------------------------------------------------------------------------------: |
|         <br >product()         | <br >p, q, … [repeat=1] | cartesian product, equivalent to a nested for-loop Example product('ABCD', repeat=2)<br >AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD  |
|      <br >permutations()       |       <br >p[, r]       | r-length tuples, all possible orderings, no repeated elements<br >Example permutations('ABCD', 2)<br >AB AC AD BA BC BD CA CB CD DA DB DC |
|      <br >combinations()       |        <br >p, r        |                   r-length tuples, in sorted order, no repeated elements combinations('ABCD', 2)<br >AB AC AD BC BD CD                    |
| combinationswithre placement() |        <br >p, r        |     r-length tuples, in sorted order, with repeated elements combinationswithreplacement('ABCD', 2)<br >AA AB AC AD BB BC BD CC CD DD     |

ATABA-BEADFunctional Thinking in Python

All rights reserved

Terminating Iterators

|       Iterator       |          Arguments          |                  Results                   |                         Example                         |
| :------------------: | :-------------------------: | :----------------------------------------: | :-----------------------------------------------------: |
|     accumulate()     |          p [,func]          |           p0, p0+p1, p0+p1+p2, …           |         accumulate([1,2,3,4,5]) --> 1 3 6 10 15         |
|       chain()        |           p, q, …           |         p0, p1, … plast, q0, q1, …         |           chain('ABC', 'DEF') --> A B C D E F           |
| chain.fromiterable() |          iterable           |         p0, p1, … plast, q0, q1, …         |   chain.fromiterable(['ABC', 'DEF']) --> A B C D E F    |
|      compress()      |       data, selectors       |     (d[0] if s[0]), (d[1] if s[1]), …      |      compress('ABCDEF', [1,0,1,0,1,1]) --> A C E F      |
|     dropwhile()      |          pred, seq          | seq[n], seq[n+1], starting when pred fails |     dropwhile(lambda x: x<5, [1,4,6,4,1]) --> 6 4 1     |
|    filterfalse()     |          pred, seq          | elements of seq where pred(elem) is false  |   filterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8   |
|      groupby()       |       iterable[, key]       |  sub-iterators grouped by value of key(v)  |                                                         |
|       islice()       | seq, [start,] stop [, step] |     elements from seq[start:stop:step]     |        islice('ABCDEFG', 2, None) --> C D E F G         |
|      starmap()       |          func, seq          |      func(*seq[0]), func(*seq[1]), …       |   starmap(pow, [(2,5), (3,2), (10,3)]) --> 32 9 1000    |
|     takewhile()      |          pred, seq          |      seq[0], seq[1], until pred fails      |      takewhile(lambda x: x<5, [1,4,6,4,1]) --> 1 4      |
|        tee()         |            it, n            | it1, it2, … itn splits one iterator into n |                                                         |
|  <br >ziplongest()   |        <br >p, q, …         |     <br >(p[0], q[0]), (p[1], q[1]), …     | ziplongest('ABCD', 'xy', fillvalue='-') --> Ax By C- D- |

ATABA-BEADFunctional Thinking in Python

All rights reserved

## functools

- The functool s module is for higher-order functions: functions that act on or return other functions
  - functools cmptokey (func): Transform an old-style comparison function to a key function
  - @functools lrucache (maxsize=128, typed=False) Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls
  - @functools totalordering Given a class defining one or more rich comparison ordering methods, this class decorator supplies the rest
  - functools partial (func, \*args, \*\*keywords) Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords
  - functools reduce (function, iterable[, initializer]) Apply function of two arguments cumulatively to the items of sequence, from left to right, so as to reduce the sequence to a single value

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Generator Expressions

- A generator expression (iterable) is lazy and creates objects only as required; this can improve performance
  - Two important caveats on generator expressions, as follows:
- Generators appear to be sequence-like The few exceptions include using a function such as the len() function that needs to know the size of the collection
- Generators can be used only once After that, they appear empty

def pfactorsl(x: int) -> Iterator[int]: if x % 2 == 0:

yield 2

if x2 > 1:

yield from pfactorsl(x2)

return

for i in range(3, int(mathsqrt(x)+5)+1, 2):

if x % i == 0: yield i

if xi > 1:

yield from pfactorsl(xi) return

yield x

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Recursive generators

In a recursive generator function, the return statement is

tricky

Do not use the following command line:

for result in recursiveiter(args):

yield result

yield from recursiveiter(args)

return recursiveiter(args)

It returns only a generator object; it doesn't evaluate the function to return the generated values Use any of the following:

def pfactorsr(x: int) -> Iterator[int]:

def factorn(x: int, n: int) -> Iterator[int]: if n\*n > x:

yield x

return

if x % n == 0: yield n

if xn > 1:

yield from factorn(xn, n)

else:

yield from factorn(x, n+2) if x % 2 == 0:

yield 2

if x2 > 1:

yield from pfactorsr(x2) return

yield from factorn(x, 3)

If the candidate factor, n , is outside the range, then x is prime

Otherwise, we'll see whether n is a factor of x If

so, we'll yield n and all factors of xn

If n is not a factor, we'll evaluate the function recursively using n+2

ATABA-BEADFunctional Thinking in Python

All rights reserved

"Don’t worry if it doesn’t work right If everything did, you’d

be out of a job (Mosher’s Law of Software Engineering)"

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Python Tools

PyCharm provides smart code completion, code inspections, on-the-fly error highlighting and quick-fixes, along with automated code refactorings and rich navigation capabilities

PyCharm offers great framework-specific support for modern web development frameworks such as Django, Flask, Google App Engine, Pyramid, and web2py

PyCharm supports JavaScript, CoffeeScript, TypeScript, Cython, SQL, HTMLCSS, template languages, AngularJS, Nodejs, and more

Run, debug, test, and deploy applications on remote hosts or virtual machines, with remote interpreters, an integrated ssh terminal, and Docker and Vagrant integration

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Colab Sandbox

Big Data

Engineering For Analytics

Well suited to machine learning, data analysis and education

Provides access free of charge to

computing resources including GPUs

Write and execute code in Python

Document your code that supports mathematical equations

CreateUploadShare notebooks

ImportSave notebooks fromto Google Drive

ImportPublish notebooks from GitHub

Import external datasets eg from Kaggle

Integrate PyTorch, TensorFlow, Keras, OpenCV

Free Cloud service with free GPU

Files: Upload datasets and other files from both your computer and Google Drive

Code Snippets: Find prewritten snippets of code for different functionalities like adding new libraries or referencing one cell from another

Run Cell: This is the run button Clicking this will run any code that is inserted in the cell beside it

Table of Contents: Sections allow you to organize your code and improve readability

Menu Bar: Like in any other application, this menu bar can be used to manipulate the entire file or add new files

File Name: This is the name of your file You can click on it to change the name Do not edit the extension (ipynb) while editing the file name as this might make your file unopenable

Insert Code Cell: This button will add a code cell below the cell you currently have selected

Insert Text Cell: This button will add a text cell below the cell you currently have selected

Cell: This is the cell This is where you can write your code or add text

depending on the type of cell it is

Output: This is the output of your code, including any errors, will be shown

Clear Output: This button will remove the output

ATABA-BEADFunctional Thinking in Python

All rights reserved

"If builders built buildings the way programmers wrote programs, then the first woodpecker that came along would destroy civilization"

~Gerald Weinberg

ATABA-BEADFunctional Thinking in Python

All rights reserved

## In Essence

- We explored functional programming features of Python
- We learnt how to craft functional constructs using map , filter , curry and reduce
  - We looked at lambda expressions
  - We looked at recursive constructs
  - We looked at generator functions and how we can use these as the backbone of functional programming
- We examined the built-in collection classes to show how they're used in the functional paradigm
- We examined Python's collection-processing features from a functional programming viewpoint
- We looked at higher-order functions : functions that accept functions as arguments as well as returning functions

ATABA-BEADFunctional Thinking in Python

All rights reserved

"The best performance improvement is the transition from the

nonworking state to the working state"

~J Osterhout

ATABA-BEADFunctional Thinking in Python

All rights reserved

## Big Data Ingestion and Formats

## Data Example

Monitoring sensors : Climate or ocean wave monitoring sensors generate dat F a or Analyticsconsistently and in a good size, and there would be more than millions of sensors that capture data
Content(Posts to social media sites) : Social media websites such as Facebook, Twitter, and others have a huge amount of data in petabytes
Media(Digital pictures and videos posted online) : Websites such as YouTube, Netflix, and others process a huge amount of digital videos and data that can be petabytes
Transaction records of online purchases : E-commerce sites such as eBay, Amazon, Flipkart, and others process thousands of transactions on a single time
Serverapplication logs : Applications generate log data that grows consistently,and analysis on these data becomes difficult
CDR (call data records) : Roaming data and cell phone GPS signals to name a few
Scientific and Research Data : Science, genomics, biogeochemical, biological, and other complex andor interdisciplinary scientific research

## Definition

- Data ingestion can be done via manual, semi-automatic or automatic methods
- Data ingestion means the process of getting the data into the data system that we are building or using
  - How many data sources are there?
  - How many large data items are available?
  - Will the number of data sources grow over time?
  - What is the rate at which data will be consumed?

### ingestion policies

- When it comes to data ingestion, developers like to create a bunch of policies, called , ingestion policies that guide the handling of errors during the data ingestion, as well as the data incompleteness, and so on

## Types of Data sources

Data sensors : These are thousands of sensors, producing data continuously
Machine Data : Produces data which should be processed in near real time for avoiding huge loss
Telco Data : CDR data and other telecom data generates high volume of data
Healthcare system data : Genes, images, ECR records are unstructured and complex to process
Social Media : Facebook, Twitter, Google Plus, YouTube, and others get a huge volume of data
Geological Data : Semiconductors and other geological data produce huge volumes of data
Maps : Maps have a huge volume of data, and processing data is a challenge in Maps
Aerospace : Flight details and runway management systems produce high-volume data and processing in real time
Astronomy : Planets and other objects produce heavy images, which have to be processed at a faster rate
Mobile Data : Mobile generates many events and a huge volume of data at a high velocity rate

## Design Questions for Ingestion

Bounded versus unbounded
Frequency
Synchronous versus asynchronous
Serialization and deserialization
Throughput and scalability
Reliability and durability
Payload
Push versus pull versus poll patterns

## Common Challenges In Ingesting

Prioritizing each data source load
Tagging and indexing ingested data
Validating and cleansing the ingested data
Transforming and compressing before ingestion

- New data sources tend to deliver data at varying speed and frequencies; example: streaming and real time ingestion
- The volume of data to be ingested has grown manifold over the years
- applies to incremental data-ingestion processes is the detection and capture of changed data

## Stages In The Ingestion Process

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211355017.png?token=AKMX3HVVNRQWAVF2PSFTXKDF2WIJU)

- Enrich: Plug in the missing pieces in the data This stage often involves talking to external data sources to plug in the missing data attributes Data may be transformed from a specific form into a form to make it suitable for downstream processes
- Process – This stage is meant to do some lightweight processing to either further enrich the event or transform the event from one form into another The process stage usually computes using the existing attributes of the data and at times using external systems
- Segregate – Often times before the data is given to downstream systems, it makes sense to bundle similar data sets together While this stage may not always be necessary for compaction, segregation does make sense most of the time

## An Example of Data Ingestion Tools

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211356532.png?token=AKMX3HUMXOI7OPINVQYIT5LF2WIMW)

## Common Challenges in Ingestion Layer

Multiple data source load and prioritization
Ingested data indexing and tagging
Data validation and cleansing
Data transformation and compression

## Comparison of ETL, ELT, and reverse ETL

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211357605.png?token=AKMX3HS4RH3C65M3G7PFR3TF2WIRO)
当目标数据存储系统对于数据的格式和结构有特定的要求，需要在加载之前进行转换以适应目标系统的需求时，ETL 是非常合适的
ELT 的主要重点是在加载后对数据进行转换和处理。

- 这种方法适用于数据存储具有足够的处理能力来进行转换操作，例如大数据平台或数据湖。

## Data Ingestion with Apache Spark

- Apache Spark has generic API based connectors for any type of storage(CSV, binary files, JSON,Image files)
- Apache Spark also enables to work with batch and streaming data

## Data Processing Layer

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211401435.png?token=AKMX3HWZMHWPI24DNRJZBMDF2WJBE)

Typical data sources include

- transactional systems such as RDBMS,
- file-based data sources such as data lakes
- message queues such as Apache Kafka

## Spark SQL connectors and data sources

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211402970.png?token=AKMX3HWEF722MTJJIANPORDF2WJDO)

## Dat Source - RDBMS

Ingest data from an RDBMS table using PySpark

```python
dataframe_mysql = spark.read.format("jdbc").options( url="jdbc:mysql://localhost:3306/sampledb",
driver = "org.mariadb.jdbc.Driver",
dbtable = "authors",
user="#####",
password="@@@@@").load() dataframe_mysql.show()
```

## Data Source - File Store

```python
retail_df = (spark
         .read
.format("csv")
.option("inferSchema", "true") .option("header","true") .load("dbfs:/FileStore/shared_retail_uploads/")
) retail_df.show()
```

The file-based data source needs to be on a distributed filesystem HDFS and cloud-based data lakes, such as AWS S3, Azure Blob, and ADLS storage, are all distributed data storage layers that are good candidates to be used as file-based data sources with Apache Spark

## Data Source - Message Queue

Apache Kafka for instance is a distributed, scalable, and fault-tolerant message queue

```python
kafka_df = (spark.read
        .format("kafka")
		.option("kafka.bootstrap.servers", "localhost:9092")
		.option("subscribe", "wordcount")
		.option("startingOffsets", "earliest")
.load()
        )
kafka_df.show()
```

Kafka is more commonly used for streaming use cases with Apache Spark, this code example makes use of Kafka as a data source for batch processing of data

## Data Sink Examples

## Guiding principles

RowColumnar based
ReadWrite
Splittable (multiple tasks can run parallel on parts of file)
Schema Evolution (Continuously evolving schema)
Compression Support (Snappy, LZO etc)

## DataFrameReader methods, arguments, and options

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211408825.png?token=AKMX3HXFXAVCLM3GOPU3EN3F2WJZW)

## DataFrameWriter methods, arguments, and options

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211408701.png?token=AKMX3HT42JT2HLOE5Y64NUDF2WJ2K)

# CSV and JSON

## Delimiter Separated Values

“Jorge”,30,”Developer”
Pros : Human readable, all tools support it
Cons :IO/Storage inefficient
No richer types - all are strings
No support for schema evolution
Other issues : delimiter in data, new lines within data

## Reading CSV with Spark

```python
df = (spark.read.format("csv")
     .option("inferSchema", "true")
     .option("header", "true")
     .load(csv_file))
df.createOrReplaceTempView("us_delay_flights_tbl")
```

## Writing CSV with Spark

```python
frame.write.format("csv")
        .option(“header", "true")
        .option(“sep", “,")
        .mode(“append”)
        .save(csv_file)
```

## SaveMode

Specifies the behavior of the save operation when data already exists

- append : Append contents of this DataFrame to existing data
- overwrite : Overwrite existing data ignore : Silently ignore this operation if data already
  exists
- error or errorifexists (default case): Throw an exception if data already exists

## Specifying schema

Define your own schema(StructType)

```python
schema = StructType([
StructField("name",StringType(),True), \ StructField("age",IntegerType(),True), \ StructField("job",StringType(),True)])
```

Specify the schema during read

```py
peopleDF = spark.read.format('csv')
					.option("schema","schema")
                    .option("sep",";") \
                    .option("header","true") \
                    .load("people.csv")
```

## JSON

Pros : Readable, some level of schema support

Cons :

- Duplicated schema
- Horrible in terms of storage
- Not splittable, linear lookups
- Aggregations require all data to be loaded into memory

## Multiline vs Record per line

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211415189.png?token=AKMX3HVKKMAXMIUKUGDQF23F2WKTG)

## Reading and Writing JSON with Spark

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211415166.png?token=AKMX3HSBXVHMS4D2TWEBTKDF2WKUA)

# AVRO

## Avro is row oriented

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211416737.png?token=AKMX3HXQEEYLI4THYVNCOADF2WKWW)

Avro schema definition
Avro schemas are defined using JSON:

## Avro

Pros

- Splittable
- Support schema evolution
- Addition, Deletion of fields in beginning, middle, end doesn’t matter
- It is a good format for data exchange It has a data storage which is very
  compact, fast and efficient for analytics
- Avro is a data serialization system
  Cons
- Data is not readable
- Not integrated to every language

## Parquet

- Pros
  - Supports primitive, complex and logical types
  - Splittable
  - Supports richer schema evolution
  - Highly compressed
  - Great for analytical workloads
- Cons
  - Writes are slower - Typical of Columnar data formats
  - Not human readable

## Parquet is column oriented

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211419852.png?token=AKMX3HREKDGPH5LKBJ2F3WDF2WLEY)

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211420751.png?token=AKMX3HUUDKUQ3LBRM25HNZLF2WLF2)
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211420513.png?token=AKMX3HQWGCI763NRKIIOMGLF2WLGY)

## Parquet format

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211421730.png?token=AKMX3HVL6LPRRFC42TIMOCLF2WLJY)

## Apache Arrow

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211423953.png?token=AKMX3HV4EY3YVKBYI6AMT7LF2WLSA)

# Arrow

Columnar memory format for flat and hierarchical data
No copy to any ecosystem like JavaR language
Provide a universal data access layer to all applications
Low loading while streaming messaging
It supports flat and nested schemas
Support GPU

## Concluding Remarks

Big Data Engin e ering

- The ingestion stage of the data engineering lifecycle is engineeri F n o r g A n a lyti cs
- intensive
  - This stage sits at the edge of the data engineering domain and often interfaces with external systems, where software and data engineers have to build a variety of custom plumbing
- Ingestion is incredibly complicated, often with teams operating open source frameworks like Kafka or Pulsar, or some of the biggest tech companies running their own forked or homegrown ingestion solutions

## AVRO vs PARQUET

Avro - If you are reading all or most of the columns

- Select \* from the table which needs to scan the entire table
- Rich schema evolution

Write operations in AVRO are better than in PARQUET

- AVRO is more mature than PARQUET when it comes to schema evolution PARQUET only supports schema append whereas AVRO supports a much- featured schema evolution ie adding or modifying columns
  PARQUET is much better for analytical querying ie reads andquerying are much more efficient than writing
  - PARQUET is ideal for querying a subset of columns in a multi-column table AVRO is ideal in ETL operations where we must query all the columns
  - Highly compressed; Select columns, group by which only needs to perform implementation on a certain column; Nested data

# Spark Query Language with Python on COLAB

Dr Venkat Ramanathan [rvenkat@nusedusg ](mailto:rvenkat@nus.edu.sg) NUS-ISS

Total Slides: 37

© 2016-2024 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

## Agenda

- Introduction to Spark SQL
- Spark SQL Architecture
- Spark SQL components
  - Spark Context, Spark Session, SQL Contexts
  - DataFrames
  - Programming with Spark SQL
  - Working with various data sources and formats
- Catalyst optimizer

## Spark SQL

Big Data Engin e ering

- What is Spark SQL
  - Spark’s interface for working with structured and semi-structured data
  - Provides the foundation for Spark machine learning applications, streaming applications, graph applications, and many other types of application architectures
  - Supports relational processing both within Spark programs (on native RDDs) and on external data sources using a programmer-friendly API
  - Flexible in supporting new data sources, including semi-structured data and external databases amenable to query federation
  - Adopts extension with advanced analytics algorithms such as graph
- processing and machine learning
  - Scalable and provides high performance using established DBMS techniques
  - Built on top of SPARK CORE, Spark SQL is Spark’s advanced set of components that provide the foundation for Spark machine learning applications, streaming applications, graph applications, and many other types of application architectures

## Spark SQL Architecture

## Data Sources

## Spark SQL

- What does Spark SQL provide?
  - A SQL Engine and command line interface
  - DataFrame API – a library for working with data as tables
- Defines DataFrames containing Rows and Columns
- It provides a DataFrame abstraction in Python, Java, and Scala to simplify working with structured datasets DataFrames are similar to tables in a relational database
- It can read and write data in a variety of structured formats (eg, JSON, Hive Tables, and Parquet)
- It lets you query the data using SQL, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC ODBC), such as business intelligence tools like Tableau
- Catalyst Optimizer – an extensible optimization framework

Spark SQL APIs & Objects

## Spark Configuration

- The SparkConf object sets the configuration for the Spark Application
- The two important methods in the object are:
  - setAppName
- Sets the application name for reference, if required
- setMaster
- The master URL to connect to Eg:
- ⁻ "local" to run locally with one thread
- ⁻ "local[4]" to run locally with 4 cores, or
- ⁻ "spark:master:7077" to run on a Spark standalone cluster

## SparkContext

- The SparkContext is the entry point of Spark functionality
- It allows your Spark Application to access Spark Cluster with the help of Resource Manager
  - The resource manager can be one of these three:
- Spark Standalone, YARN, Apache Mesos

## Spark Session

- The SparkSession is the entry point into the Structured API
- In Spark SQL programming context:
  - It can be used to read data from various sources, such as CSV, JSON,
- JDBC, stream, and so on
  - It can be used to execute SQL statements, register User Defined Functions (UDFs), and work with Datasets and DataFrames

Note:

When using COLAB, we use Spark Session Builder to implicitly create the spark context and configuration and instantiate the Spark Session Object

## Data Frames

- A DataFrame is an immutable, table-like, distributed data organized into rows, where each one consists a set of columns and each column has a name and an associated type
  - DataFrames can process data that's available in different formats, such as CSV, AVRO, and JSON, or stored in any storage media, such as Hive, HDFS, and RDBMS
  - DataFrames can process data volumes from kilobytes to petabytes
  - Use the Spark-SQL query optimizer to process data in a distributed and optimized
- manner
  - Support for APIs in multiple languages, including Java, Scala, Python, and R
- age name pcode null Alice 94304
- 30 Brayden 94304
- 19 Carla 10036
- 46 Diana null
- null Étienne 94104

|     |                                                                                                                                                                                                            |     |
| :-: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-: |
| Fil |                                                                                               e: people.json                                                                                               |     |
|     | {"name":"Alice", "pcode":"94304"}<br >{"name":"Brayden", "age":30, "pcode":"94304"}<br >{"name":"Carla", "age":19, "pcode":"10036"}<br >{"name":"Diana", "age":46}<br >{"name":"Étienne", "pcode":"94104"} |     |

## Spark Built-in Data Sources

DataFrames can be created

From an existing structured data source (Parquet file, JSON file, etc)

From an existing RDD

By performing an operation or query on another DataFrame

By programmatically defining a schema

|   Name    | Data Format |                                                        Comments                                                         |
| :-------: | :---------: | :---------------------------------------------------------------------------------------------------------------------: |
| Text file |    Text     |                                                      No structure.                                                      |
| <br >CSV  |  <br >Text  | Comma-separated values. This can be used to specify another delimiter. The column name can be referred from the header. |
| <br >JSON |  <br >Text  |                Popular semi structured format. The column name and data type are inferred automatically.                |
|  Parquet  |   Binary    |                            (Default format.) Popular binary format in the Hadoop community.                             |
|    ORC    |   Binary    |                                 Another popular binary format in the Hadoop community.                                  |
|   JDBC    |   Binary    |                                  A common format for reading and writing to the RDBMS.                                  |

Spark Query Language Programming

## Spark SQL API Packages

- To run Spark SQL the following packages need to be import
  - PySpark:

from pyspark import SparkContext from pyspark import SparkConf from pysparksql import \*

from pysparksqlfunctions import _ from pysparksqltypes import _

## Python Programming Structure

The mai n method

Directive to run the main()

Imports

Indents

Multiline codes: use for continuing to next like or use ( … )

to encapsulate

## COLAB Pre-requisites

- To run Spark on COLAB, the following needs to be performed as one time operation per session
  - Install PySpark on the Notebook
  - Map the Notebook to the Google Drive where the
- data files are stored
- COLAB
- !pip install --ignore-install -q pyspark
- !pip install --ignore-install -q findspark

from googlecolab import drive drivemount( 'contentdrive' , forceremount= True )

## Code Pre-requisites

- Establishing the Configuration, Context & Session:
  - Create a spark configuration (SparkConf)
  - Instantiate a new SparkContext, using the configuration defined
  - Create a Spark Session (instantiate or use
- SparkSessionbuilder)
- COLAB
- Python
- spark = SparkSessionbuilder
- master("local")
- appName("Customer")
- getOrCreate()

## Loading data into Data Frame

- To read data into a data frame you should know the following:
  - The location of the file (or database) where data is stored
- Example:
- contentdriveMy DriveBEADDATACustomercsv  COLAB

 Windows

 Linux

D:WorkspaceDataCustomerCSV

file:HomeuserCustomercsv

hdfs:quickstartclouderauserCustomercsv  Hadoop

jdbc:mysql:localhostvideoshop

- The format of storageretrival
  - Example: CSV, Json, jdbc etc
- Driver information (if any)

We can use the read or load method

Python

inputFilePath = "contentdriveMy DriveBEADDATACustomercsv"

df = ( sparkread

option( "header" , "true" )

option( "inferSchema" , "true" )

csv(inputFilePath) )

Note : In the above example we have assumed that the first row of the CSV is the header (hence provided header as true and also have asked the framework to infer the schema which will make the data frame use the first row information for column headers and will examine the data of that column to infer a suitable data type

## Working with the Data Frame

- After loading the data into a DataFrame (df) you may perform various actions
  - Examining the schema:
- dfprintSchema()
  - Getting the number of records:
- dfcount()
  - Displaying records:
- dfshow()

## Spark SQL Examples

Live Demo

Refer also to Workshop Document

## DataFrame Structured Transformations - 1

|          Operation          |                                                                                                    Description                                                                                                     |
| :-------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|         <br >select         |   This selects one or more columns from an existing set of columns in the DataFrame. A more technical term for select is projection. During the projection process, columns can be transformed and manipulated.    |
|       <br >selectExpr       |                                                            This supports powerful SQL expressions in transforming columns while performing projection.                                                             |
|        filter where         | Both filter and where have the same semantics. where is more relational than filter, and it is similar to the where condition in SQL. They are both used for filtering rows based on the given Boolean conditions. |
| distinct<br >dropDuplicates |                                                                                <br >This removes duplicate rows from the DataFrame.                                                                                |
|        sort orderBy         |                                                                               <br >This sorts the DataFrame by the provided columns.                                                                               |

|     Operation     |                                                              Description                                                               |
| :---------------: | :------------------------------------------------------------------------------------------------------------------------------------: |
|       limit       |                                        This returns a new DataFrame by taking the first n rows.                                        |
|       union       |                                  This combines two DataFrames and return them as a new<br >DataFrame.                                  |
|    withColumn     |                            This is used to add a new column or replace an existing column in the DataFrame.                            |
| withColumnRenamed |              This renames an existing column. If a given column name<br >doesn’t exist in the schema, then it is a no-op.              |
|     <br >drop     |      This drops one or more columns from a DataFrame. This operation does nothing if a specified given column name doesn’t exist.      |
|    <br >sample    | This randomly selects a set of rows based on the given fraction parameter, an optional seed value, and an optional replacement option. |

This splits the DataFrames into one or more DataFrames based on the

given weights It is commonly used to split the master data set into training and test data sets in the machine learning model training process

This joins two DataFrames Spark supports many types of joins

This groups a DataFrame by one or more columns A common pattern is to perform some kind of aggregation after the groupBy operation

This computes the common statistics about numeric and string columns in the DataFrame Available statistics are count, mean, stddev, min, max, and arbitrary approximate percentiles

## Join Types

Type Description

Inner join

(aka equi-join)

Returns rows from both datasets when the join expression evaluates to

true

Returns rows from the left dataset even when the join expression evaluates to false

Returns rows from the right dataset even when the join expression

evaluates to false

Right outer join

Returns rows from both datasets even when the join expression evaluates to false

Returns rows only from the left dataset when the join expression evaluates to false

Returns rows only from the left dataset when the join expression

evaluates to true

Returns rows by combining each row from the left dataset with each row in the right dataset The number of rows will be a product of the size of each dataset

Cross

(aka Cartesian)

## Aggregation Functions - 1

|        Operation         |                              Description                              |
| :----------------------: | :-------------------------------------------------------------------: |
|        count(col)        |                Returns the number of items per group.                 |
|    countDistinct(col)    |             Returns the unique number of items per group.             |
| approxcountdistinct(col) |       Returns the approximate number of unique items per group.       |
|         min(col)         |       Returns the minimum value of the given column per group.        |
|         max(col)         |       Returns the maximum value of the given column per group.        |
|         sum(col)         |     Returns the sum of the values in the given column per group.      |
|     sumDistinct(col)     | Returns the sum of the distinct values of the given column per group. |
|         avg(col)         |   Returns the average of the values of the given column per group.    |

|       Operation       |                                                     Description                                                     |
| :-------------------: | :-----------------------------------------------------------------------------------------------------------------: |
|  <br >skewness(col)   |                Returns the skewness of the distribution of the values of the given column per group.                |
|  <br >kurtosis(col)   |                Returns the kurtosis of the distribution of the values of the given column per group.                |
|  <br >variance(col)   |                     Returns the unbiased variance of the values of the given column per group.                      |
|   <br >stddev(col)    |                     Returns the standard deviation of the values of the given column per group.                     |
| <br >collectlist(col) | Returns a collection of values of the given column per group. The returned collection may contain duplicate values. |
|    collectset(col)    |                                  Returns a collection of unique values per group.                                   |

## Defining Schema

If the file does not have a schema (or if you do not wish to use the original header), then a schema can be defined & used to read data

personschema = ( StructType([

StructField( "NRIC" , IntegerType(), True ), StructField( "Name" , StringType(), True ), StructField( "Age" , IntegerType(), True ) ])

df1 = (sparkreadschema( schema =personschema)csv(inputFilePath))

## Spark ScalaPython Types equivalent to SQL

|  Data Type  | Scala Type | Python Type |
| :---------: | :--------: | :---------: |
| BooleanType |  Boolean   |    bool     |
|  ByteType   |    Byte    | int or long |
|  ShortType  |   Short    | int or long |
| IntegerType |    Int     |     int     |
|  LongType   |    Long    |    long     |
|  FloatType  |   Float    |    float    |
| DoubleType  |   Double   |    float    |
| StringType  |   String   |     str     |

|   Data Type   |        Scala Type        |      Python Type      |
| :-----------: | :----------------------: | :-------------------: |
|  DecimalType  |   java.math.BigDecial    |        decimal        |
|  BinaryType   |       Array[Byte]        |       bytearray       |
| TimestampType |    java.sql.Timestamp    |   datetime.datetime   |
|   DateType    |      java.sql.Date       |     datetime.date     |
|   ArrayType   |   scala.collection.Seq   | list, tuple, or array |
|    MapType    |   scala.collection.Map   |         dict          |
|  StructType   | org.apache.spark.sql.Row |     list or tuple     |

## Other Supported Data Sources

- You can also use third party data source libraries, such as
  - Cassandra, Redis, HBase, MySQL and more being added all the time
- To work with third party data sources:
  - You may have to download the appropriate driver
  - You will need to add class paths andor set project references
  - Refer to the product API for driver and configuration details

Caution : Avoid direct access to databases in production environments, which may

overload the DB or be interpreted as service attacks Use Sqoop to import instead

Spark Query Optimization

“Big data is at the foundation of all the megatrends that are

happening”

~Chris Lynch

## Catalyst Optimizer

Catalyst optimizer primarily leverages functional programming constructs of Scala such as pattern matching

Catalyst optimizer has two

primary goals:

Visualization Tools

(Tablue ZoomData Qlik etc)

Make adding new optimization techniques easy

Enable external developers

to extend the optimizer

CATALYST OPTIMIZER

Catalyst Optimizer Tasks: (1)Analysis (2)Optimization (3)Physical Planning (4)Cost Model Application (5)Physical Plan Selection (6)Optimized Code Generation

RESILIENT DISTRIBUTED DATASET

## Logical and Physical Planning

“Without big data analytics, companies are blind and deaf,

wandering out onto the web like deer on a freeway”

– Ge o f f r ey Mo o r e, autho r o f Cr o s s in g the C h asm

## In Essence

Big Data Engin e ering

- We learnt about Spark SQL or Sparkql
  - Spark SQL is a Spark API for handling structured and semi-‐structured data
- Entry point is a SQLContext
- DataFrames are the key unit of data
- DataFrames are based on an underlying RDD of Row objects
- We described some of the internals of Spark SQL, including the Catalyst and Project Tungsten-based optimizations
- We learnt to use Spark SQL to explore structured and semi-
- structured data

“War is 90% information”

– Napoleon Bonaparte, French military and political leader


# Spark - Resilient Distributed Datasets
## Resilient Distributed Datasets
Spark offers a computing framework to handle iterative algorithms and interactive data mining tools
  - Keeps data in memory to improve performance

One Key Abstraction: Resilient Distributed Datasets (RDDs)
  - Distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner
  - provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state

## RDD Operations
Transfor-mations
- RDD Two Operations:
  - Transformations : Lazy Evaluated
  - Actions : Are final and trigger computation
## RDD’s are Lineage
RDDs can only be created through deterministic operations on either:
  - (1) data in stable storage or
  - (2) other RDDs
Essential Property: 
	- an RDD has enough information about how it was derived from other datasets (its lineage) to compute its partitions from data in stable storage
Directed Acyclic Graph (DAG) of transformations
  - Each time a transformation is applied to an immutable RDD
  - A new RDD gets created with new lineage information
- Anytime you know, what transformations to apply on a
- single “partition” of data
## Rich Set of Operations FIVE pieces of informatio

An RDD is represented as an abstraction and is defined by the following FIVE pieces of information :
- A set of partitions, which are the chunks that make up the entire dataset
- A set of dependencies on parent RDDs
- A function for computing all the rows in the data set
- Metadata about the partitioning scheme (optional)
- Where the data lives on the cluster (optional); if the data lives on HDFS, then it would be where the block locations are located
## Spark Operations
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211448448.png?token=AKMX3HUBI27I7YSHOO7EOPDF2WOQC)
## Create RDD
- Ways to create an RDD
  - Parallelizing existing collection
  - From data in an external storage system (Local location, HDFS,HBase, NoSQL …)
  - From another RDD
  - From a DataFrame or DataSet
## Why use RDD
For the vast majority of use cases, DataFrames will be more efficient, more stable, and more expressive than RDDs

Use RDDs when fine-grained control over the physical distribution of data (custom partitioning of data) is needed and you are working with plenty of unstructured data

## Parallelized collection
In the initial stage when we learn Spark, [RDDs](https:data-flair.trainingblogsapache-spark-rdd-tutorial) [ ](https:data-flair.trainingblogsapache-spark-rdd-tutorial)are generally created by parallelized collection ie by taking an existing collection in the program and passing ito SparkContext’s parallelize() method
```python
> val mydata = sc.parallelize(Array(1,2,3,4,5))
> mydata.collect()
```
## Rich Set of Operations
An RDD is represented as an abstraction and is defined by the following FIVE pieces of information :
A set of partitions, which are the chunks that make up the entire dataset
A set of dependencies on parent RDDs
A function for computing all the rows in the data set
Metadata about the partitioning scheme (optional)
Where the data lives on the cluster (optional); if the data lives on HDFS, then it would be where the block locations are located
## RDD Operations
Actions – return values
Transformations – define a new RDD based on the current one(s)
Two types of RDD operations

THINK: Which type of operation is count() ?
## Narrow and Wide Operations
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211518766.png?token=AKMX3HQN4JSUZIKNOPZUNJLF2WSBW)

## RDD Operation: Transformations
Transformations create a new RDD froman existing one
RDDs are immutable
Data in an RDD is never changed
- Transform in sequence to modify the
- data as needed
- Some common transformations
  - map(function) – creates a new RDD by performing a function on each record in the base RDD
  - filter(function) – creates a new RDD by including or excluding each record in the base RDD according to a boolean function
## RDD Operation: Actions
count() – return the number of elements
take( n ) – return an array of the first nelements
collect() – return an array of all elements
saveAsTextFile( file ) – save to textfile(s)

# !IMPOTANT PRINT 21-50 Details
## RDDs
RDDs can hold any type of element
–Primitive types: integers, characters, booleans, etc
–Sequence types: strings, lists, arrays, tuples, dicts, etc (including nested data types)
–ScalaJava Objects (if serializable)
–Mixed types

Some types of RDDs have additional functionality
–Pair RDDs
–RDDs consisting of Key-‐Value pairs
–Double RDDs
–RDDs consisting of numeric data
## Creating RDDs from Collection
- You can create RDDs from collections instead of ﬁles
  - sc.parallelize( collection )
Useful when
- Testing
- Generating data programmatically
- Integrating
## Creating RDDs from files (1)
SparkContext.textFile
  - Accepts a single file, a wildcard list of files, or a comma-‐separated list of files
  - Examples
	- sctextFile("myfiletxt")
	- sctextFile("mydata\*log")
	- sctextFile("myfile1txt,myfile2txt")
- Each line in the file(s) is a separate record in the RDD
- Files are referenced by absolute or relative URI
  - Absolute URI:
- file:hometrainingmyfiletxt
- hdfs:localhostloudacremyfiletxt
- Relative URI (uses default file system): myfiletxt


## Input and Output Formats

- Spark uses Hadoop InputFormat and OutputFormat Java classes
  - Some examples from core Hadoop
	- TextInputFormat TextOutputFormat – newline delimited text files
	- SequenceInputFormat SequenceOutputFormat
	- FixedLengthInputFormat
- Many implementations available in additional libraries
	- eg AvroInputFormat AvroOutputFormat in the Avro library
- Speci f y a n y in p ut f orm a t u s ing schadoopFile
	- or newAPIhadoopFil e for New API classes
- Specify any output format using rddsaveAsHadoopFile
	- or saveAsNewAPIhadoopFile for New API classes
- textFile and saveAsTextFile are convenience functions
	- textFile just calls hadoopFile specifyingTextInputFormat
	- saveAsTextFile calls saveAsHadoopFile specifying TextOutputFormat
## Whole File-Based RDDs (1)
sc.textFile maps each line in a file to a separate RDD element
– What about files with a multi-‐line input format, eg XML or JSON?
scwholeTextFiles( directory )
– Maps entire contents of each file in a directory to a single RDD element
– Works only for small files (element must fit in memory)

## Other General RDD Operations
- S i ng l e-‐RD D T r an s f orm a tions
  - flatMap – maps one element in the base RDD to multiple elements
  - distinct – filter out duplicates
  - sortBy – use provided function to sort
- Mu l t i- ‐R D D T r an s f o rm a ti on s
  - intersection – create a new RDD with all elements in both original RDDs
  - union – add all elements of two RDDs into a single new RDD
  - zip – pair each element of the first RDD with the corresponding element of the second
- Other RDD operations
  - first – return the first element of the RDD
  - foreach – apply a function to each element in an RDD
  - top( n ) – return the largest n elements using natural ordering
- Sampling operations
  - sample – create a new RDD with a sampling of elements
  - takeSample – return an array of sampled elements
- Double RDD operations
  - Statistical functions, eg, mean , sum , variance , stdev



## Multi-RDD Transformations

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211538929.png?token=AKMX3HQ5BMX2UMN2OKCJUKTF2WUMS)

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211539017.png?token=AKMX3HTVR57LOGE5FLSUTATF2WUNS)

## Essential Points
- The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel
  - The ability to always re-compute an RDD is actually why RDDs are called “resilient” 
	  - When a machine holding RDD data fails, Spark uses this ability to re-compute the missing partitions, transparent to the user

# Analytics using Spark Machine Learning
## Definition of Machine Learning
Tom Mitchell (1998): a computer program is said to learn from experience E with respect to some tasks T and performance measure P, if its performance on T, as measured by P, improves with experience E
## Three components of Machine Learning
The main goal of machine learning isto predict results based on incoming data
The greater the variety of sample in hand the easier it gets to find relevant patterns and hence predict results accurately
  - Data : The more and diverse the data,the better the result There are two main ways of collecting data — manual and automatic
  - Features : Also known as parameters or variables These are the factors for a machine to look at Picking the right set of features is an iterative time consuming process
  - Algorithms : The method chosen to predict results and it affects the precision, performance, and size of the final model
## Learning, Intelligence and other jargons
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402211545786.png?token=AKMX3HU3SOTEF2DJMPU2Q4DF2WVGI)


## Supervised Learning
Supervised learning algorithms are trained using labeled examples, such as an input where the desired output is known
The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns by comparing its actual output with correct outputs to find errors
## Classiﬁcation
- Classification algorithm discovers groups
  - Classification is a form of ‘supervised’ learning
  - Logistic Regression, Support Vector Machine (SVM), Naïve Bayes, Decision Trees, Ensembles and Neural Network, K-Nearest Neighbor
- The goal while solving a classification problem is to predict a
- class or category for an observation
  - A classification system takes a set of data records with known labels
  - Learns how to label new records based on that information
- Examples
  - Given a set of e-mails identified as spamnot spam, label new e-mails as spamnot spam
  - Given tumors identified as benign or malignant, classify new tumors



## Classification Use Cases
Predicting disease
  - A doctor or hospital might have a historical dataset of behavioral and physiological attributes of a set of patients
  - They could use this dataset to train a model on this historical data and then leverage it to predict whether or not a patient has heart disease or not
  - This is an example of binary classification (healthy heart, unhealthy heart) or multiclass classification (healthly heart, or one of several different diseases)
Classifying images
  - There are a number of applications from companies like Apple, Google, or
- Facebook that can predict who is in a given photo by running a classification model that has been trained on historical images of people in your past photos
  - Another common use case is to classify images or label the objects in images
Predicting customer churn
  - A more business-oriented use case might be predicting customer churn—that is,
- which customers are likely to stop using a service You can do this by training a
- binary classifier on past customers that have churned (and not churned) and using it to try and predict whether or not current customers will churn
Buy or won’t buy
  - Companies often want to predict whether visitors of their website will purchase a
- given product They might use information about users’ browsing pattern or
- attributes such as location in order to drive this prediction
## Supervised Learning In Image Classification
x = raw pixels of the image 
y = the main object
## Regression
- Regression is about predicting real values from observations
	- Unlike classification, the predicted value is not discrete, but rather it is continuous
- Used for:
	- Stock price forecast
	- Demand and sales volume analysis
	- Medical diagnosis
	- Any number-time correlations
- Popular algorithms are Linear and Polynomial regressions
## Regression Use Cases
- Predicting sales
  - A store may want to predict total product sales on given data using historical sales data There are a number of potential input variables, but a simple example might be using last week’s sales data to predict the next day’s data
- Predicting height
  - Based on the heights of two individuals, we might want to predict the heights of their potential children
- Predicting the number of viewers of a show
  - A media company like Netflix might try to predict how many of their
- subscribers will watch a particular show
## Unsupervised Learning
- Dataset contains no labels
- The goal is to explore the data and find some structure in space of data
  - Clustering: infer cluster label
- Typically one cluster per input
- Dimensionality reduction: discover lower dimensional subspaces
- Principle Component Analysis (PCA)
- Topic modelling: infer cluster labels
- Input can belong to multiple clusters
## Clustering
- In clustering, a dataset is split into a specified number of clusters or segments
	- Elements in the same cluster are more similar to each other than to those in other clusters
- Popular algorithms: K-meansclustering, Mean-Shift, DBSCAN
- Clustering algorithms discover structure in collections of data
  - Where no formal structure previously existed
  - They discover what clusters, or groupings, naturally occur in data
- Examples
  - Finding related news articles
  - Computer vision (groups of pixels that cohere into objects)
  - Customer Segmentation
  - Market segmentation (types of customers, loyalty)
  - To merge close points on the map
  - For image compression
  - To analyze and label new data
  - To detect abnormal behavior

## Dimensionality Reduction
- Assembles specific features into more high-level ones
- Nowadays is used for:
	- Recommender systems
	- Beautiful visualizations
	- Topic modeling and similar document search
	- Fake image analysis
	- Risk management

Popular algorithms: 
- Principal Component Analysis (PCA), 
- Singular Value Decomposition (SVD), 
- Latent Dirichlet allocation (LDA), 
- Latent Semantic Analysis (LSA, pLSA, GLSA), 
- t-SNE (for visualization)

## More Unsupervised Learning Use Cases
- Anomaly detection
	- Given some standard event type often occurring over time, we might want to report when a nonstandard type of event occurs
	- For example, a security officer might want to receive notifications when a strange object (think vehicle, skater, or bicyclist) is observed on a pathway
- User segmentation
	- Given a set of user behaviors, we might want to better understand what
- attributes certain users share with other users
	- For instance, a gaming company might cluster users based on properties like the number of hours played in a given game
	- The algorithm might reveal that casual players have very different behavior than hardcore gamers, for example, and allow the company to offer different recommendations or rewards to each player
- Topic modeling
	- Given a set of documents, we might analyze the different words contained therein to see if there is some underlying relation between them
	- For example, given a number of web pages on data analytics, a topic modeling algorithm can cluster them into pages about machine learning, SQL, streaming, and so on based on groups of words that are more common in one topic than in others

## Reinforcement Learning
- "Throw a robot into a maze and let it find an exit"
- Nowadays used for:
	- Self-driving cars
	- Robot vacuums
	- Games
	- Automating trading
	- Enterprise resource management
- Popular algorithms: Q-Learning, SARSA, DQN, A3C, Genetic algorithm
## Ensemble Methods
- Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone
- Nowadays is used for:
	- Everything that fits classical algorithms approaches (but works better)
	- Search systems
	- Computer vision
	- Object detection
- Popular algorithms: Random Forest, Gradient Boosting
## Neural Networks and Deep Learning
- Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns
	- They interpret sensory data through a kind of machine perception, labeling or clustering raw input
	- The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated
- Deep-learning networks are distinguished from single-hidden-layer neural networks by their depth; that is, the number of node layers through which data must pass in a multistep process of pattern recognition
- Used today for:
	  - Replacement of all algorithms above
	  - Object identification on photos and videos
	  - Speech recognition and synthesis
	  - Image processing, style transfer
	  - Machine translation
- Popular architectures: Perceptron, Convolutional Network (CNN), Recurrent Networks (RNN)

# Spark Machine Learning
## MLlib and ML Libraries
- MLlib is Spark’s machine learning library
- MLlib includes both the RDD-based API and the DataFrame-based API
- Note: as of spark 20, the RDD-based APIs have entered maintenance mode
- What is Spark ML?
  - Spark ML is used to refer to the MLlib DataFrame-based API
## Spark MLlib Tools
ML Algorithms : common learning algorithms such as classification, regression, clustering, and collaborative filtering
Feature Engineering : feature extraction, transformation, dimensionality reduction, and selection
Pipelines : tools for constructing, evaluating, and tuning ML Pipelines
Persistence : saving and load algorithms, models, and Pipelines
Utilities : linear algebra, statistics, data handling, etc
## Typical steps in a machine learning pipeline
Machine learning algorithms attempt to make predictions or decisions based on training data
  - All learning algorithms require defining set of features for each item, which will be fed into the learning function
  - Real-world ML pipelines will train multiple versions of a model and evaluate each one
## The machine learning workflow, in Spark
## Typical steps in a machine-learning project
1 Data Collection
2 Data clean up and preparation
3 Data analysis and visualization
4 Training the model
5 Model Evaluation
6 Use the model for predictions
## Project consists of multiple steps - 1
Collecting data
  - First the data needs to be gathered from various sources
  - The sources can be log files, database records, signals coming from sensors, and so on
  - Spark can help load the data from relational databases, CSV files, remote services, and distributed file systems like HDFS, or from real- time sources using Spark Streaming
Cleaning and preparing data
  - Data isn’t always available in a structured format appropriate for machine learning (text, images, sounds, binary data, and so forth) At times unstructured data is transformed into numerical features
  - Additionally, you need to handle missing data and the different forms in which the same values can be entered (for example, VW and Volkswagen are the same carmaker)
  - Often, data also needs to be scaled so that all dimensions are of comparable ranges
- Analyzing data and extracting features
  - Analyze the data, examine its correlations, and visualize them
  - Choose the appropriate machine-learning algorithm (or set of algorithms) and split the data into training and validation subsets
  - Decide on a cross-validation method, where the dataset is split into different training and validation datasets and average the results over the rounds
- Training the model
  - Train a model by running an algorithm that learns a set of algorithm- specific parameters from the input data
- Evaluating the model
  - Validate performance, decide if feature space needs refinement or different algorithms need to be tuned
- Using the model
  - Deploy the built model to the production environment
## ML Pipeline - 1
- ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines
- Main Concepts in Pipelines
  - DataFrame : This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types
  - Transformer : A transformer is an algorithm which can transform one Dataframe into another Dataframe
  - Estimator : An estimator is an algorithm which can be fit on a DataFrame to produce a Transformer
  - Pipeline : A pipeline chains multiple Transformers and Estimators together to specify an ML workflow
  - Parameter : All Transformers and Estimators now share a common API for specifying parameters



## ML Pipeline – 2.
- Pipeline Train time
Pipeline is an estimator, can use fit() to train the entire pipeline on a specific dataset
The top row represents a pipeline with three stages The first two (Tokenizer and HashingTF) are transformers and the third (Logistic Regression) is an Estimator
- Pipeline Test time
The PipelineModel is the result of fitting a Pipeline on a specific dataset, it is a transformer
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402221048781.png)

## In Essence
- Spark MLlib and Spark ML
	- Spark Mllib – RDDs and DataFrame
	- Spark ML - DataFrame（Recommended）
- MLlib Feature Engineering library
	- MLlib feature engineering library provides feature extraction, feature transformation, feature selection for models
- ML Pipeline
	- ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines

# Machine Learning on Spark
## PySpark ML & PySpark MLlib
- PySpark ML
  - DataFrame-based machine learning APIs to let user quickly assemble
- and configure practical machine learning pipelines
- PySpark MLlib
  - It is a wrapper over PySpark Core to do data analysis using machine- learning algorithms It works on distributed systems and is scalable We can find implementations of classification, clustering, linear regression, and other machine-learning algorithms in PySpark MLlib
## Measurement Formula

## Regression
- Linear regression
	- Ordinary Least Squares
	- Gradient Descent
- Regularized linear model
	- Ridge
	- Lasso
	- Elastic Net

## Linear Regression

Simple Linear Regression

Regression defines as to use the relationship between variables to find the best fit line or the regression equation that can be used to make predictions Linear regression refers to a linear relationship between the independent variable(s) and dependent variable If the independent variables include more than one variables, it is known as multiple linear regressions Polynomial regression refers to a non-linear relationship, it is rather a curve that fits into the data points

Simple Linear Regression

𝑦 = 𝜃0 + 𝜃1𝑥1

Multiple Linear Regression

𝑦 = 𝜃0 + 𝜃1𝑥1+…+ 𝜃𝑑𝑥𝑑

Polynomial Regression

Polynomial Regression

𝑦 = 𝜃0 + 𝜃1𝑥1+𝜃2𝑥2 + ⋯ + 𝜃𝑑 𝑥𝑑

1 1

ATABA-BEADMachine Learning on Spark

All rights reserved

Our goal with linear regression is to minimize the vertical distance between all the data points and our line

So in determining the best line, we are attempting to minimize the distance between all the points and their distance to our line

There are lots of different ways to minimize this, (sum of squared errors, sum of absolute errors, etc), but all these methods have a general goal of minimizing this distance

ATABA-BEADMachine Learning on Spark

All rights reserved

## Example

For example, one of the most popular methods is the least squares method

We’ll use the Least Squares Method, which is fitted by minimizing the sum of squares of the vertical distance (residuals)

Here we have blue data points along an x and y axis

The residuals for an observation is the difference between the observation and the fitted line (value predicted by the line) It is also known as ordinary least squares

ATABA-BEADMachine Learning on Spark

All rights reserved

## Cost Function

Cost function in linear regression is to describe how to evaluate

the quality of the predictions of our hypothesis is making, when compared to the actual value in the dataset

Common cost function for linear regression is the Mean Squared

𝑀𝑆𝐸 = ෍(𝑦ෝ − 𝑦 )

𝑛 is the number of data points

𝑦𝑖 is the observed (actual) value for the i -th data point

𝑦ෝ𝑖 is the predicted value for the i -th data point

ATABA-BEADMachine Learning on Spark

All rights reserved

## Ordinary Least Squares

Given a data set { 𝑋1, 𝑦1 , … , (𝑋𝑛, 𝑦𝑛)},which contains 𝑑 features (variables) and 𝑛 samples (data point), the matrix format equation of a simple linear regression model is:

𝑌෠ = 𝑊𝜃

𝜃 = [𝜃1, … , 𝜃𝑑]𝑇, 𝑦 = [𝑦1

We want to minimize the mean squared error (known as

ordinary least squares (OLS) ):

min 𝑊𝜃 − 𝑦

𝜃

We can find the closed-format of the solution is

𝜃 = (𝑊𝑇𝑊)−1𝑊𝑇𝑌

ATABA-BEADMachine Learning on Spark

All rights reserved

## Regularization (Optional)

- Issues with the closed-format of solution
  - 𝑊𝑇𝑊 is not invertible
  - Problem of overfitting
- Solution
  - Regularization
- Convert ill-posed problems to well-posed by adding additional information via the penalty parameter 𝜆
- Preventing overfitting
- Types of regularization
- Ridge regression shrinks the coefficients of correlated variables
- LASSO method picks one variable and discards the others
- Elastic Net is a mixture of these two

ATABA-BEADMachine Learning on Spark

All rights reserved

## Regularization

LinearRegression(maxIter=10, regParam=03, elasticNetParam=08)

regParam >0, elasticNetParam =0  Ridge regression (L2)

regParam >0, elasticNetParam =1  LASSO(L1)

regParam >0, elasticNetParam∈ (0 , 1)  elastic Net

ATABA-BEADMachine Learning on Spark

All rights reserved

## Example

from pysparkmlregression import LinearRegression

## Load training data

training = sparkreadformat("libsvm")

load("samplelinearregressiondatatxt")

lr = LinearRegression(maxIter=10, regParam=03, elasticNetParam=08)

## Fit the model

lrModel = lrfit(training)

## Print the coefficients and intercept for linear regression print("Coefficients: %s" % str(lrModelcoefficients)) print("Intercept: %s" % str(lrModelintercept))

## Summarize the model over the training set and print out some metrics trainingSummary = lrModelsummary

print("numIterations: %d" % trainingSummarytotalIterations) print("objectiveHistory: %s" % str(trainingSummaryobjectiveHistory)) trainingSummaryresidualsshow()

print("RMSE: %f" % trainingSummaryrootMeanSquaredError) print("r2: %f" % trainingSummaryr2)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Generalized Linear Regression

- Generalized Linear Model (GLM)
  - Specifications of linear regression
  - The response variable follows some distribution from the exponential family of distribution, not only the normal distribution
- The Akaike Information Criterion (AIC)
  - An evaluation parameter of relative performance of quality of models for the same dataset
  - AIC is mainly used to select among multiple models for a given dataset
  - A lesser value of AIC indicates that the model is of good quality
  - AIC tries to strike a balance between the variance and bias of the model
  - It deals with the chances both of overfitting and underfitting

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo Example

Refer to example:

PySpark-LinearRegressionAdvertisingDataipynb

ATABA-BEADMachine Learning on Spark

All rights reserved

ATABA-BEADMachine Learning on Spark

All rights reserved

## Classification

- Regression problems is to predict a continuous value, for
- classification problems, it is to predict discrete categories
- Examples of classification
  - Healthcare - Training a machine learning model on historical patient data can help healthcare specialists accurately analyze their diagnoses
  - Transportation - To predict which geographical location will have a
- rise in traffic volume
  - Sustainable agriculture - By using classification models to predict which type of land is suitable for a given type of seed
  - Finance – Credit card approval prediction, risk classification

ATABA-BEADMachine Learning on Spark

All rights reserved

## Types of Classification – Binary Classification

- Binary Classification
  - Classify the input data into two mutually exclusive categories
- Examples
  - Email spam detection (spam or not)
  - Churn prediction (churn or not)
- Algorithm
  - Logistic regression
  - Support Vector Machine
  - Decision Trees
  - Naïve Bayes
  - K-Nearest Neighbors

ATABA-BEADMachine Learning on Spark

All rights reserved

## Types of Classification – Multi-class Classification

- Multi-class Classification
  - It has more than two mutually exclusive class labels, the goal is to predict to which class a given input example belongs to
- Examples
  - Face classification
  - Plant species classification
  - Hand-written digits classification
- Algorithms
  - Random Forest
  - Naïve Bayes
  - Gradient Boosting
  - K-Nearest Neighbors
  - Support Vector Machine (one vs one or one vs rest)
  - Logistic Regression(one vs one or one vs rest)

ATABA-BEADMachine Learning on Spark

All rights reserved

- Multi-label classification
  - we try to predict 0 or more classes for each input example In this case, there is no mutual exclusion because the input example can have more than one label
- Examples
  - Text - topic classification
  - Music – theme classification
- Algorithms
  - Multi-label Decision Tree
  - Multi-label Random Forest
  - Multi-label Gradient Boosting

ATABA-BEADMachine Learning on Spark

All rights reserved

Logistic Regression Classifier

ATABA-BEADMachine Learning on Spark

All rights reserved

## Classification

Big Data Engin e ering

r Analytics

Imagine we plotted out some categorical data against one feature

The X axis represents a feature value and the Y axis represents the probability of belonging to class 1

ATABA-BEADMachine Learning on Spark

All rights reserved

We can’t use a normal linear regression model on binary groups It won’t lead to a good fit

It would be great if we could find a function with this sort of behavior

ATABA-BEADMachine Learning on Spark

All rights reserved

## Logistic Regression

A linear logistic classifier (LLC) :

ℎ 𝑥; 𝜃, 𝜃0 = 𝜎(𝜃𝑇𝑥 + 𝜃0)

The logistic function (sigmoid function) is defined as

- 𝜎 𝑧 ∈ (0,1) representing the estimated
- probability
- Three LLCs with different parameter settings: 𝜎 10𝑥 + 1 , 𝜎 −2𝑥 + 1 , 𝜎 2𝑥 − 3
- Question: which plot is which?
- Larger coefficient -> steeper curve
  - The probability of the binary outcome changes rapidly with small changes in the input features

ATABA-BEADMachine Learning on Spark

All rights reserved

The output of the logistic function is used to make predictions and classify the input into one of the two classes based on a chosen threshold (usually 05), anything below it results in class 0, anything above is class 1

ATABA-BEADMachine Learning on Spark

All rights reserved

## Loss function for logistic classifiers

Objective: Pick the parameters of the logistic classifier to maximize the

probability assigned by the classifier to the correct 𝑦 value, as specified in the

training set

Letting 𝑔(𝑖) = 𝜎 𝜃𝑇𝑥 𝑖 + 𝜃0 , that probability is:

𝑖𝑓 𝑦(𝑖) = 1

𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

Assumption: predictions are independent The labels in the training data are 𝑦𝑖

∈ {0,1}, the above formula can be rewritten:

𝑛

𝑖

𝖦 𝑔 𝑖 𝑦 (1 − 𝑔(𝑖))1−𝑦(𝑖)

𝑖=1

Apply log operator on the above formula:

( 𝑦(𝑖) log 𝑔(𝑖) + (1− 𝑦(𝑖)) log(1 − 𝑔(𝑖)))

Taking negative as we want to turn maximization to minimization, so the negative log-likelihood (nll) loss function is:

𝑔𝑢𝑒𝑠𝑠, 𝑎𝑐𝑡𝑢𝑎𝑙 = −(𝑎𝑐𝑡𝑢𝑎𝑙 ∙ log 𝑔𝑢𝑒𝑠𝑠 + (1 − 𝑎𝑐𝑡𝑢𝑎𝑙) ∙ (1 − 𝑔𝑢𝑒𝑠𝑠))

This loss function is also referred as log loss or cross entropy

ATABA-BEADMachine Learning on Spark

All rights reserved

## Types of Logistic Regression Classifiers

- Binomial logistic regression
  - classification with binary outcome (usually 0 or 1)
  - Logistic function
- Multinomial logistic regression (NOT COVER)
  - Classification with more than two outcomes
  - Softmax function

ATABA-BEADMachine Learning on Spark

All rights reserved

## Confusion Matrix – Binary Classification

Big Data Engin e ering

|                        |          Actual Positive          |           Actual Negative           |
| :--------------------: | :-------------------------------: | :---------------------------------: |
| Predicted<br >Positive |        True Positive (TP)         | False Positive(FP)<br >Type I error |
|   Predicted Negative   | False Negative (FN) Type II error |         True Negative (TN)          |

ATABA-BEADMachine Learning on Spark

All rights reserved

## Classification Metrics

𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =

𝑇𝑜𝑡𝑎𝑙 𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛𝑠

𝑇𝑃

𝑅𝑒𝑐𝑎𝑙𝑙𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 =

2∗𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙

ATABA-BEADMachine Learning on Spark

All rights reserved

## Evaluation – ROC and AUC

- ROC curve
- An ROC curve (receiver operating
- characteristic curve) is a graph showing the
- performance of a classification model at all
- classification thresholds This curve plots two parameters:
  - True Positive Rate
  - False Positive Rate
- An ROC curve plots TPR vs FPR at different
- classification thresholds Lowering the
- classification threshold classifies more items
- as positive, thus increasing both False Positives and True Positives
- AUC stands for "Area under the ROC Curve"
- AUC ranges in value from 0 to 1 A model
- whose predictions are 100% wrong has an
- AUC of 00; one whose predictions are 100% correct has an AUC of 10

https:towardsdatasciencecomunderstanding-auc-roc-curve-68b2303cc9c5

ATABA-BEADMachine Learning on Spark

All rights reserved

## Group Discussion

Case 1 - COVID 19 testing result: COVID 19 =1, Healthy = 0

Case 2 – email spam: email spam = 1, not spam = 0

Case 3 – bank loan: Bad loan =1, good loan =0

For each of case, define which metric is better evaluation metric and explain the reason

ATABA-BEADMachine Learning on Spark

All rights reserved

## Example – Logistic Regression

## Initializing Spark Session and import logistic regression module from pysparksql import SparkSession

from pysparkmlclassification import LogisticRegression spark = SparkSessionbuilderappName(‘lgr')getOrCreate()

## Loading training data

training = sparkreadformat("libsvm")load("samplelibsvmdatatxt")

lr = LogisticRegression(maxIter=10, regParam=03, elasticNetParam=08)

## Fit the mode and print out the coefficients and intercept lrModel = lrfit(training)

print("Coefficients: " + str(lrModelcoefficients)) print("Intercept: " + str(lrModelintercept))

ATABA-BEADMachine Learning on Spark

All rights reserved

## Confusion Matrix - Multi-class Classification

Example of Multi-class confusion matrix:

| Predicted value |            |                |               |               |
| :-------------: | :--------: | :------------: | :-----------: | :-----------: |
|  Actual value   |            |     Setosa     |  Versicolor   |   Virginica   |
|                 |   Setosa   | 13<br >(cell1) | 0<br >(cell2) | 0<br >(cell3) |
|                 | Versicolor | 0<br >(cell4)  | 6<br >(cell5) | 1<br >(cell6) |
|                 | Virginica  | 1<br >(cell7)  | 1<br >(cell8) | 8<br >(cell9) |

- The classes are listed in the same order in the rows as in the columns The correct classification are located on the diagonal from top left to bottom right
- For class Sentosa, calculate:
  - TP: The actual value and the predicted value should be the same:TP = 13
  - FN: the sum of values of corresponding rows except for the TP value: FN = (cell2+cell3)=0+0 = 0
  - FP: The sum of values of the corresponding column except for the TP value: FP
- =(cell4+cell7)= 0+1 = 1
  - TN: The sum of values of all columns and rows except the values of that class that we are calculating the values for: TN =(cell5+cell6+cell8+cell9)= 6+1+1+8 = 16

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo Example

Refer to example

PySpark-LogisticRegressionBankDataipynb

ATABA-BEADMachine Learning on Spark

All rights reserved

## Support Vector Machine (Optional)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Introduction to Support Vector Machine (SVM)

Supervised learning algorithm

Can be used for both regression and classification

First was introduced by Vladimir N Vapnik in 1992[1][2]

Find a hyperplane that best separates data into different

classes while maximizing the margin between the classes

Boser, B E, Guyon, I M, & Vapnik, V N (1992, July) A training algorithm for optimal margin classifiers In Proceedings of the fifth annual workshop on Computational learning theory (pp 144-152)

Cortes, C and Vapnik, V, 1995 Support-vector networks Machine learning, 20, pp273-297

ATABA-BEADMachine Learning on Spark

All rights reserved

## Key concepts

- Hyperplane
  - it is a subspace whose dimension is one less than that of its ambient space Eg if a space is 3-dimensional then its hyperplanes are 2- dimensional planes
- 1D - Point
- 2D – Line
- 3D – Plane
- 4D – Hyperplane
- For binary classification, this hyperplane is the decision boundary
- Margin : margin of a linear classifier is the width that the boundary could be increased by before hitting a data point

ATABA-BEADMachine Learning on Spark

All rights reserved

## Linear Classifier

Consider a two-class, linearly separable classification problem

A hyperplane separates the two classes

Class 1

There are many hyperplanes that can

separate the classes correctly

Which hyperplane is the best?

ATABA-BEADMachine Learning on Spark

All rights reserved

## Maximum Margin

Maximize the margin

ATABA-BEADMachine Learning on Spark

All rights reserved

## Mathematical Formula of Support Vector Machine

Let 𝑥1, 𝑥2, … , 𝑥𝑛 be our data set and let 𝑦𝑖 ∈ {1, −1} be the class label of 𝑥𝑖

The hyperplane equation is 𝑊𝑇𝑋 + 𝑏 = 0

The margin is 𝑚 =

The decision boundary should classify all points correctly, it is based on the sign of the expression: 𝑊𝑇𝑋 + 𝑏

𝑓 𝑋 = 𝑠𝑖𝑔𝑛(𝑊𝑇𝑋 + 𝑏)

0, the data point is classified as one class

< 0 the data point is classified as the other class

The goal of SVM is to maximize the margin while minimizing classification errors It can be solved the following constrained optimization problem:

𝑆𝑢𝑏𝑗𝑒𝑐𝑡 𝑡𝑜 𝑦𝑖(𝑊𝑇𝑥𝑖 + 𝑏) ≥ 1

It is a constrained optimization problem (Not Cover)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Non-Linear SVM

So far, we only considered large-margin linear classifier, how to generalized it to become nonlinear?

𝑥2

- 𝑥1
- Key idea: transform 𝑥𝑖 to a higher dimensional space 𝜙(𝑥𝑖)
  - Linear operation in the feature space is equivalent to non-linear operation in input space
  - Classification can become easier with a proper transformation
  - Input space: the space the point 𝑥𝑖 located
  - Feature space: the space of 𝜙(𝑥𝑖) after transformation

ATABA-BEADMachine Learning on Spark

All rights reserved

## Transforming the Data

( ) ( )

( )

( )

Computation in the transformed feature space can be costly because it is high dimensional

Key idea: Kernel trick

ATABA-BEADMachine Learning on Spark

All rights reserved

## Kernel Trick

The kernel trick is a powerful technique used in machine learning, especially in SVM and other kernel-based algorithms It can implicitly map data from a lower-dimensional space into a higher- dimensional space without explicitly calculating the transformed feature vectors We only need to calculate the inner product in the feature space

Define the kernel function K by

𝐾 𝑥𝑖 , 𝑥𝑗 = ∅(𝑥𝑖)𝑇∅(𝑥𝑗)

Polynomial kernel with degree 𝑑:

𝐾 𝑥, 𝑦 = (𝑥𝑇𝑦 + 1)𝑑

Radial basis function kernel with width𝜎:

Sigmoid with parameter 𝛼, 𝛽:

𝐾 𝑥, 𝑦 = tanh(𝛼(𝑥𝑇 𝑦 + 𝛽)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Types of SVM Kernel Functions

𝐾 𝑥, 𝑦 = (𝑥𝑇𝑦 + 1)𝑑

𝐾 𝑥, 𝑦 = tanh(𝛼(𝑥𝑇𝑦 + 𝛽)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Common Strategies for Multi-class Classification

- One-vs-One(OvO):
  - In the OvO, a binary classifier is trained for each pair of classes If

there are N classes, it results in

binary classifiers Each

- classifier is trained on a subset of the data that includes only the instances belonging to the two classes During the prediction, each classifiers votes for its chosen class and the class with the most votes is the final prediction
- One-vs-Rest(OvR)
  - In the OvR, a binary classifier is trained for each class One class is considered as the positive class and the rest are grouping as the negative class If there are N classes, N binary classifiers are trained In prediction, each classifier predicts whether an instance belongs to its positive class or not, the classifier gives the highest confidence becomes the final predicted class

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo Example

SVM for multi-class classification

PySpark-SVM-Classifier-Irisipynb

ATABA-BEADMachine Learning on Spark

All rights reserved

Random Forest (Optional)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Introduction to Decision Tree

Supervised learning method

Use a tree-like graph or model to make decision

Various variations such as Boosted Decision Tree, Random Forest

Can be used for both regression and classification

ATABA-BEADMachine Learning on Spark

All rights reserved

## Decision Tree Terms

Root Node – represents the entire population or sample this further gets divided into two or

more homogeneous sets

Splitting – a process of dividing a node into two or more sub-nodes

Decision Nodes - When a sub-node splits into further sub-nodes, then it is called decision node

Leafterminal nodes - Nodes do not split is called Leaf or Terminal node, represent the final

outcomes or predictions

Branchsub-tree – refers to a specific path or series of decision made from the root node to a

ATABA-BEADMachine Learning on Spark

All rights reserved

## Example - Loan Approval

| Loan ID | Income Level | Credit Score |  Employment   |   Approved   |
| :-----: | :----------: | :----------: | :-----------: | :----------: |
|    1    |     High     |     Good     |   Salaried    |   Approved   |
|    2    |    Medium    |  Excellent   |   Salaried    |   Approved   |
|    3    |     Low      |     Fair     | Self-Employed | Not Approved |
|    4    |    Medium    |     Good     |   Salaried    |   Approved   |
|    5    |     High     |  Excellent   | Self-Employed |   Approved   |
|    6    |     Low      |     Poor     | Self-Employed | Not Approved |
|    7    |     High     |  Excellent   |   Salaried    |   Approved   |
|    8    |    Medium    |     Fair     |   Salaried    | Not Approved |
|    9    |    Medium    |     Good     | Self-Employed |   Approved   |
|   10    |     Low      |  Excellent   | Self-Employed | Not Approved |

Split strategy – how to decide which feature has the best split?

Entropy

What should be the criterion?

Gini Index

ATABA-BEADMachine Learning on Spark

All rights reserved

## Measure of Impurity

- Gini impurity (Gini Index): measure the level of impurity or disorder in a dataset
  - A low Gini Impurity indicates the data is purer and

more homogeneous (less mixed) in terms of classes

𝐺𝑖𝑛𝑖 = 1 − ෍ 𝑝2

- 𝑖=1
- where 𝑝𝑖 is the probability (proportion of samples in class
- 𝑖 in dataset 𝐷)
- Entropy : a measure of the degree of the disorder or impurity of a dataset
  - A lower entropy value indicates a purer and more

homogeneous dataset

𝐸𝑛𝑡𝑟𝑜𝑝𝑦 = − ෍ 𝑝𝑖 log2 𝑝𝑖

𝑖=1

ATABA-BEADMachine Learning on Spark

All rights reserved

## Information Gain

- Information Gain
  - A measure of the reduction in impurity or disorder achieved by partitioning a dataset based on a particular

attribute or feature

Calculate as the difference between the impurity (either Gini impurity or entropy) of the parent dataset before the split and the weighted average impurity of the child datasets after the split

Information Gain using Gini impurity :

𝐷

𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛 𝐺𝑎𝑖𝑛 𝐷, 𝐴 = 𝐺𝑖𝑛𝑖 𝐷 − ෍

𝑐∈𝐶 𝐴

𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛 𝐺𝑎𝑖𝑛 𝐷, 𝐴 is the information gain for the feature A on dataset D

𝐺𝑖𝑛𝑖 𝐷 is the Gini impurity of the original dataset D

𝐶 𝐴 is the set of possible values of feature A

𝐷𝑐 is the number of instances in dataset D that have feature A qual to value c

𝐷 is the total number of instances in dataset D

𝐺𝑖𝑛𝑖(𝐷𝑐) is the Gini impurity of the subset of dataset D where feature A equals value c

Information Gain using Entropy :

𝐼𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛 𝐺𝑎𝑖𝑛 𝐷, 𝐴 = 𝐸𝑛𝑡𝑟𝑜𝑝𝑦 𝐷 − ෍

𝑐∈𝐶 𝐴

𝐸𝑛𝑡𝑟𝑜𝑝𝑦 𝐷 is the entropy of the original dataset D

𝐶 𝐴 is the set of possible values of feature A

𝐷𝑐 is the number of instances in dataset D that have feature A qual to value c

𝐷 is the total number of instances in dataset D

𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝐷𝑐) is the entropy of the subset of dataset D where feature A equals value c

Split Strategy : select the feature for splitting that maximizes information gain , which

minimizes impurity or disorder in the resulting child dataset

ATABA-BEADMachine Learning on Spark

All rights reserved

## Information Gain Example – Loan Approval

| Loan ID | Income Level | Credit Score |  Employment   |   Approved   |
| :-----: | :----------: | :----------: | :-----------: | :----------: |
|    1    |     High     |     Good     |   Salaried    |   Approved   |
|    2    |    Medium    |  Excellent   |   Salaried    |   Approved   |
|    3    |     Low      |     Fair     | Self-Employed | Not Approved |
|    4    |    Medium    |     Good     |   Salaried    |   Approved   |
|    5    |     High     |  Excellent   | Self-Employed |   Approved   |
|    6    |     Low      |     Fair     | Self-Employed | Not Approved |
|    7    |     High     |  Excellent   |   Salaried    |   Approved   |
|    8    |    Medium    |     Fair     |   Salaried    | Not Approved |
|    9    |    Medium    |     Good     | Self-Employed |   Approved   |
|   10    |     Low      |  Excellent   | Self-Employed | Not Approved |

- Gini impurity:

  - Gini(D) = 1- [(610)^2+(410)^2] = 048

- For Income level split:
- High income:
  - GiniHighIncome =1−[(33​)^2+(03)^2]=0
  - GiniMediumIncome =1−[(34​)^2+(14​)^2]=0375
  - GiniLowIncome =1−[(03)^2+(33)^2]=0
- Weighted average Gini for the Income level: (310)*0+(410)*0375+(310)\*0=015
- Information gain(income level) = 048-015=033
- Information gain(Credit score) =048-015 =033
- Information gain(Employment) =048-04=008

Either income level or credit score as the split attribute

ATABA-BEADMachine Learning on Spark

All rights reserved

## Ensemble Learning

- All algorithms have errors
- Generate a group of base learners and combined results gives higher accuracy
- Different base learners can use different parameters, sequence, training sets etc
- Two major ensemble learning methods:
  - Bagging
  - Boosting

ATABA-BEADMachine Learning on Spark

All rights reserved

## Bagging

Random sampling with replacement

Bagging makes each model run independently and then aggregates the outputs at the end

without preference to any model

Various models are built in parallel

All models vote to give the final prediction

ATABA-BEADMachine Learning on Spark

All rights reserved

## Boosting

Train the Decision Tree in sequence

Learn from the previous tree by focusing on incorrect observations

Build new model with higher weight for incorrect observations from

previous sequence

Gradient Boost is an extension of boosting where the process of additively generating weak models is formalized as a gradient descent algorithm over an objective function

XGBoost , which stands for Extreme Gradient Boosting , is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems

ATABA-BEADMachine Learning on Spark

All rights reserved

## Random Forest Classifier

Random Forest is a bagging technique The trees in random forests are run in parallel There is no interaction between these trees while building the trees

It operates by constructing a multiple of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees

Create trees have no correlation or weak

correlation

Random Forests reduce variance is by (1) training on different samples of the data

(2) using a random subset of features

ATABA-BEADMachine Learning on Spark

All rights reserved

## Random Forests Algorithm -1

Step 1: Create bootstrapped sample

Take a random sample of size N with replacement from the dataset This sample will be the training set for growing the tree

ATABA-BEADMachine Learning on Spark

All rights reserved

Step 2: Create a decision tree using the bootstrapped sample, but only use a number of size m of randomly selected variables at each step

Variable sampling without replacement

In general, at each split, only 𝑚 =

the total number of variables

It decorrelates the trees, avoiding that strong predictors always appear on top

Each tree is grown to the largest extent possible There is no pruning

𝑀 variables are selected, M is

ATABA-BEADMachine Learning on Spark

All rights reserved

Step 3: Repeats steps 1- 2 a large number of times

There is no magic number for large number Suggest odd number of trees in case

of ties The final result is the majority voting or average

ATABA-BEADMachine Learning on Spark

All rights reserved

## Estimating generalization Error: Out-of- Bag(OOB) error

While making the bootstrapped samples, the data points which fail to be a part of that particular sample are known as Out-of-Bag points

Every data point is passed for prediction to trees where it would be behaving as OOB and an aggregated prediction is recorded for each row

OOB score is computed as the number of correctly predicted rows from the OOB sample

OOB error is the number of wrongly predicting the OOB sample

ATABA-BEADMachine Learning on Spark

All rights reserved

## Variable Importance

- Gini-based importance
  - The decrease of Gini impurity when this variable is chosen to split a node
- Permutation importance
  - How much the accuracy decreases when this particular variable is
- excluded
- Higher the value of mean decrease accuracy or mean decrease Gini impurity index, higher the importance of the variable in the model

ATABA-BEADMachine Learning on Spark

All rights reserved

## Differences to Decision Tree

Train each tree on bootstrapped sample

For each split, consider only a subset of randomly selected variables

Don’t prune

Create multiple trees in such a way and use average or majority voting to aggregate results

ATABA-BEADMachine Learning on Spark

All rights reserved

## Pros and Cons of Decision Tree and Random Forest

Decision Tree

⁺ Trees yield insight into

decision rules

⁺ Rather fast

⁺ Easy to tune parameters

⁻ Prediction of trees tend to have high variance

Random Forest

⁺ Better general performance

⁺ Easy to tune parameters

⁺ Lower variance

⁻ Rather slow

⁻ “Black Box”: hard to get

insights into decision rules

ATABA-BEADMachine Learning on Spark

All rights reserved

## PySpark.ml.DecisionTreeClassifier - Parameters

maxDepth – The maximum depth of the decision tree Deeper trees are more expressive but can be prone to overfitting

maxBins – The maximum number of bins used for discretizing continuous features Increasing this number allows the algorithm to consider more split

points, but it also increases computation cost

minInstancesPerNode - The minimum number of instances each child must have after a split It is a parameter for controlling overfitting

minInfoGain - The minimum information gain required for a split to happen It's

another parameter for controlling overfitting

impurity - The impurity measure used for information gain calculation It can be "gini" or "entropy“

see d - The seed for random number generation Setting a seed ensures reproducibility of results

minWeightFractionPerNode - Minimum fraction of the weighted sample

count that each child must have after split

ATABA-BEADMachine Learning on Spark

All rights reserved

## PySpark.ml. RandomForestClassifier - Parameters

Bootstrap - Whether bootstrap samples are used when building trees

n umTrees – Number of trees to train

subsamplingR a te - Fraction of the training data used for learning each decision tree, in range (0, 1]

featureSubse t S t r a te g y - The number of features to consider for splits at each tree node

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo Example

PySpark-DecisionTreeClassifier-Irisipynb

PySpark-RandomForestClassifierIrisipynb

ATABA-BEADMachine Learning on Spark

All rights reserved

Feed-Forward Neural Network (Optional)

© 2016-2024 NUS The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied

ATABA-BEADMachine Learning on Spark

All rights reserved

## Basic Element

The basic element of a neural network is a “neuron” , also known as a ”unit” or

“node”

Neuron function representation:

𝑚

= 𝑓 ෍ 𝑥𝑗𝑤𝑗 + 𝑤0 = 𝑓(𝑤𝑇𝑥 + 𝑤0)

𝑗=1

Note that w is a vector of weights (𝑤1, … , 𝑤𝑚), 𝑤0 is a threshold, f is activation function, it can be any function and should be differentiable

Two special cases of the neuron: linear logistic classifier (with activation function is the Sigmoid function 𝑓 𝑥 = 𝜎(𝑥)) and linear regression (with activation function

𝑓 𝑥 = 𝑥)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Common Activation Functions

Rectified linear unit: common used in hidden layer

Sigmoid function also known as logistic function, can be interpreted as probability, for any value of z the output is in [0,1], common for the output for binary classification

Softmax function: take a whole vector 𝑍 ∈ 𝑅𝑛 and generates as a vector of 𝐴 ∈

[0,1]𝑛 with the property σ𝑛 𝐴𝑖 = 1which means we can interpret it as a

probability distribution over n items, common for the output of multi-

classification

ATABA-BEADMachine Learning on Spark

All rights reserved

## Multi-Layer Neural Network

- Feed-forward multiple-layer network
  - Combines multiple layers, typically by feeding the outputs of one layer into the inputs of another layer
- We will use 𝑙 to name a layer, and let 𝑚𝑙 be the number of inputs to the layer and
- 𝑛𝑙 be the number of outputs from the layer Let 𝑓𝑙 be the activation function of layer 𝑙, then the pre-activation outputs are the 𝑛𝑙 × 1 vector

and the activation outputs are the 𝑛𝑙 × 1 vector

ATABA-BEADMachine Learning on Spark

All rights reserved

- Diagram of a multi-layer NN

  - Two blocks for each layer, one representing the linear part of the operation and one representing the non-linear activation function

- Training neural networks
  - Gradient descent
- Batch gradient descent - sum up the gradient over all the points
- Stochastic gradient descent (SGD) – take a small step with respect to the gradient
- considering a single point at a time
  - Error back-propagation
- Do a forward pass to compute all the inputs and outputs at all layers, and finally the actual loss on this example
- Work backward and compute the gradient of the loss with respect to the weights in each layer, staring at the last layer and going back to the first layer

ATABA-BEADMachine Learning on Spark

All rights reserved

## MLP Classifier Example

Extend the negative log likelihood (NLL) to multi-class classification with 𝐾 classes

Training label: one-hot vector 𝑦 = [𝑦1 𝑦𝐾]𝑇, where

𝑦𝑘 = 1 if the example is of class 𝑘

The activation function in the last layer is softmax function

Output of the last layer: 𝑎 = [𝑦1 𝑦𝐾]𝑇 represents a

probability distribution over the 𝐾 possible classes

ATABA-BEADMachine Learning on Spark

All rights reserved

## Build a Multi-layer Perceptron Model

- Big Data Deep Learning Libraries
  - Spark’s MLlib
  - TensorFlow on Spark
  - Deep Learning Pipeline
  - DeepLearning4J

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo Example

Refer to MLP classifier example PySpark-MLP-Classifier-WineDataipynb

ATABA-BEADMachine Learning on Spark

All rights reserved

ATABA-BEADMachine Learning on Spark

All rights reserved

## Clustering

Big Data Engin e ering

For unlabelled data, usually we try to create groups from data, instead of trying to predict classes or values

This type of problem is known as clustering

The input are unlabelled data, and the unsupervised learning algorithm returns back possible clusters of the data

By the nature of this problem, it can be difficult to evaluate

the groups or clusters for “correctness”

A large part of being able to interpret the clusters assigned comes down to domain knowledge

A lot of clustering problems have no 100% correct approach or answer, that is the nature of unsupervised learning

ATABA-BEADMachine Learning on Spark

All rights reserved

## K Means Clustering

Big Data Engin e ering

- K Means Clustering is unsupervised learning algorithm that will attempt to group similar cluster together
- Some typical clustering problems:
  - Clustering similar documentation
  - Clustering customers based on features
  - Market segmentation
  - Identify similar physical groups
  - Clustering users group based on there usage pattern (telecom, online purchase)

ATABA-BEADMachine Learning on Spark

All rights reserved

## Clustering with K Means

Theory

The K-Means algorithm iteratively attempts to determine clusters within the test data by minimizing the distance between the mean value of cluster center vectors, and the new candidate cluster member vectors The following equation assumes data set members that range from X1 to Xn ; it also assumes K cluster sets that range from S1 to Sk where K <= n

ATABA-BEADMachine Learning on Spark

All rights reserved

- The K Means Algorithm
  - Choose a number of Clusters “K”
  - Randomly assign each point to a cluster
  - Until clusters stop changing, repeat the following:
  - For each cluster, compute the cluster centroid by taking the mean
- vector of points in the cluster
  - Assign each data point to the cluster for which the centroid is the closest

ATABA-BEADMachine Learning on Spark

All rights reserved

## Demo: K Means Clustering

- K Means Clustering
  - A common iterative algorithm used in graph analysis and machine learning
- Goal: Find “clusters” of data points

ATABA-BEADMachine Learning on Spark

All rights reserved

Big Data Engin e ering

nalytics

| Param name  | Type(s) |  Default   | Description For A |
| :---------: | :-----: | :--------: | :---------------: |
| featuresCol | Vector  | "features" |  Feature vector   |
| Param name  | Type(s) |  Default   |    Description    |

Predicted cluster center

Choose K random points as starting centers

Find all points closest to each center

Find the center (mean) of each cluster

If the centers changed by

more than c, iterate again

Close Enough - Done!

ATABA-BEADMachine Learning on Spark

All rights reserved

## How to choose a K value

- It is not easy to choose a best K value
- One method is the elbow method
  - Compute the sum of the squared error (SSE) for some values of K
  - The SSE is the sum of the squared distance between each member of the cluster and its centroid
  - Plot K against the SSE, you will see that the error decreases as k gets larger; this is because when the number of clusters increases, they should be smaller, so distortion is also smaller
  - The idea of the elbow method is to choose the k at which the SSE decreases abruptly

ATABA-BEADMachine Learning on Spark

All rights reserved

## Evaluation

- Silhouette Coefficient
- If the ground truth labels are not known, evaluation must be performed using the model itself The Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters The Silhouette Coefficient is defined for each sample and is composed of two scores:
  - a: The mean distance between a sample and all other points in the same cluster
  - b: The mean distance between a sample and all other points in the next nearest cluster
- The Silhouette Coefficient s for a single sample is then given as:

ATABA-BEADMachine Learning on Spark

All rights reserved

## Example - KMeans

Big Data Engin e ering

## Initializing Spark Session

from pysparksql import SparkSession

from pysparkmlevaluation import ClusteringEvaluator

spark = SparkSessionbuilderappName(‘kmeans')getOrCreate()

#Importing Kmeans Library and loading the Dataset from pysparkmlclustering import KMeans

dataset = sparkreadformat("libsvm")load("samplekmeansdatatxt")

## Training a kmeans model

kmeans = KMeans()setK(2)setSeed(1)

model = kmeansfit(dataset)

## Making predictions

predictions = modeltransform(dataset)

## Evaluate clustering by computing silhouette score evaluator = ClusteringEvaluator()

silhouette = evaluatorevaluate(predictions)

print("Silhouette with squared euclidean distance = " + str(silhouette))

## Print cluster centers

centers = modelclusterCenters() print("Cluster Centers: ")

for center in centers: print(center)

ATABA-BEADMachine Learning on Spark

All rights reserved

“The real question is, when will we draft an artificial intelligence bill of rights? What will that consist of? And who will get to decide that?”

Gray Scott

ATABA-BEADMachine Learning on Spark

All rights reserved

## In Essence

PySpark is the Python API for Apache Spark in Python It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment PySpark supports most of Spark’s features such as Spark SQL , DataFrame , Streaming , Mllib and Spark Core

Spark MLlib is a library for scalable machine learning built on top of Apache Spark It provides a set of high-level APIs for distributed machine learning algorithms

Spark ML , introduced in Spark 20, is the successor to Spark MLlib It is DataFrame-based API for machine learning It is a unified Machine Learning API It also provide ML pipeline allowing users to chain multiple data processing and machine learning stages together This makes it easier to create end-to-end machine learning workflows

ATABA-BEADMachine Learning on Spark

All rights reserved

“The key to artificial intelligence has always been the representation”

—Jeff Hawkins

ATABA-BEADMachine Learning on Spark

All rights reserved

## References

Machine Learning and Security, by David Freeman , Clarence Chio, O'Reilly Media, Inc, February 2018

Machine Learning, by Eihab Mohammed Bashier , Muhammad Badruddin Khan , Mohssen Mohammed, CRC Press, August 2016

Machine Learning, 2nd Edition by Stephen Marsland, Chapman and HallCRC, September 2015

[https:sparkapacheorgdocs246apipythonindexhtml](https:spark.apache.orgdocs2.4.6apipythonindex.html)

[https:sparkapacheorgdocslatestml-guidehtml](https:spark.apache.orgdocslatestml-guide.html)

Introduction to Machine Learning, MIT open learning library

ATABA-BEADMachine Learning on Spark

All rights reserved

ATABA-BEADMachine Learning on Spark

All rights reserved

## Case Study results

# Spark Best Practices
## Partitioning
- Choice of partition
	- Business logic
		- Reduce the working dataset size 
		- Repartition before multiple joins
			- Both dataframe have a common partitioner
			- One of the dataframe is smaller enough to fit into the memory
- Data
	- Data skew The join key is not evenly distributed among the partitions
	- Repartition data on a more evenly distributed key
	- Broadcast the smaller dataframe if possible
	- Use an additional random key for better distribution
	- Iterative broadcast join
- Environment
	- Cluster core count, memory per executor, etc

## Job tuning
Cache (with appropriate persistence)
  - Leverage caching (‘persist’ or “cache” methods) to avoid recomputing the same data multiple times, especially in iterative algorithms
Shuffle partitions
  - Shuffle partitions refer to the number of partitions into which the data is divided during a shuffle operation
	- confset("sparksqlshufflepartitions", "25")
	- confset("sparkdefaultparallelism", "25")
Broadcast Join
	- confset("sparksqlautoBroadcastJoinThreshold", "52428800")
- Use higher level abstractions (Dataframe and Datasets than RDD)
Use FAIR scheduler
## Shuffle partitions 
  - On one hand, when you have too much data and too few partitions, it causes fewer tasks to be processed in excutors, it will increase the load on each individual executor and often leads to memeory error
  - On the other hand, when you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with many partitioned files with a fewer number of records in each partition, which results in running many tasks with lesser data to process
From Spark 20, Spark support dynamic resource allocation, which allows it to adjust the number of shuffle partitions dynamically based on the workload sparkconfset("sparkdynamicAllocationenabled", "true")

## Broadcast join
Broadcast join can be very efficient for joins between a large table with relatively small tables
Broadcast joins are easier to run on a cluster
- Spark can “broadcast” a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster After the small DataFrame is broadcasted, Spark can perform a join without shuffling any of the data in the large DataFrame

## FAIR scheduler
Any action in Spark is a Job, Jobs are scheduled in FIFO pool
- By default, Spark’s internal scheduler runs jobs in FIFO fashion
- Can also in FAIR scheduler
The FAIR scheduler supports the grouping of jobs into pools
It also allows setting different scheduling options (eg weight) for each pool

FAIR scheduler mode is a good way to optimize the execution time of multiple jobs inside one Apache Spark program 
- Unlike FIFO mode, it shares the resources between tasks and therefore, do not penalize short jobs by the resources lock caused by the long- running jobs


## Shuffles are expensive
![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402230933740.png)

![](https://raw.githubusercontent.com/2Lavine/ImgRep/main/img/202402230933020.png)



## Managing the Big Data
## Learning Objectives

#Bigideahuangig Data Management

- Big data management incorporates the policies, techniques, and processes used for data collection, storage, administration, and the delivery of large repositories
- Major management concerns are around:
  - Data ingestion
  - Data storage
  - Data quality
  - Data operations
  - Data scalability and security

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Big data management services

Different vendors support different technological stacks, and have different pricing models

There are vendors that offer a variety of standalone or multi-featured big data management tools

The management can also involve additional cleaning, integration, migration, and reporting

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Data Cleansing

- Data cleansing is the process of identifying and fixing corrupt or fallacious records in a record set, table, or database
- It also deals with identifying incomplete, incorrect, inaccurate, or irrelevant parts of the data, and then replacing, modifying, or deleting the infected data
- Data cleansing is important because of:
  - Heterogeneity The main data is usually spread across different legacy systems, including spreadsheets, text files, and web pages
  - Accuracy By ensuring that the data is as accurate as possible, an organization can maintain good relationships with its customers, improving the organization's efficiency
  - Completeness Correct and complete data provides better insights into the process that the data concerns

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Data Integration

Data integration is one of the techniques of combining data from disparate sources and providing end users with a unified view of that data

This gives a sense of abstraction to the end users

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Other management capabilities

Big Data Engin e ering

Data migration : This is the process of transferring data from one

environment to another Most migration occurs between

computers and storage devices (for example, transferring data from in-house data centers to the cloud)

Data preparation : Data that is used for analysis is often messy and

inconsistent, and not standardized This data must be collected

and cleaned into one file or data table, before an actual analysis

can take place This step is referred to as data preparation It

involves handling messy data, trying to combine data from multiple sources, and reporting on the data sources

Data enrichment : This step involves enhancing the existing set of

data by refining the data, in order to improve its quality It can be

done in several ways Some common ways are by adding new

datasets, correcting miniature errors, or extrapolating new information from raw data

Data quality : This is the act of confirming that the data is accurate

and reliable There are several ways in which data quality is controlled

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

Data analytics : This is the process of drawing insights from datasets by analyzing them with a variety of algorithms Most steps are automated by using various tools

Master data management (MDM) : This is a method that is used to define and manage the important data of any enterprise, in order to facilitate the process of linking critical enterprise data to one master set The master set works as a single source of truth for the organization

Data governance : This is a data management concept that deals with the ability of a company to ensure high data quality throughout the analytical process This process

includes warranting the availability, usability, integrity, and accuracy of data

Extract transform load ( ETL ): As the name implies, this is the process of moving data from an existing repository to a different database, or a new data warehouse

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Vendors

Alation

AtScale

Cloudera & Hortonworks

Collibra

Confluent

SAS

Verato

TIBCO

Talend

Reference: https:[wwwreltiocomsolutionsmaster-data-management](http:www.reltio.comsolutionsmaster-data-management)

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

Analytics Lifecycle Toolkit

You never lose a dream It just incubates as a hobby

~Larry Page

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## The Analytics Lifecycle begins with a definition of the fundamental question or problem and continues through data exploration, analysis, and results activation. The loop is closed when analytics insights are operationalized into the business workflow in some way.

https:supportsascomresourcespapersproceedings170832-2017pdf

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Goals of Analytics

Big Data Engin e ering

To solve a problem —Applied analytics is concerned with the discovery

of solutions to practical problems, and we measure success by its

immediate utility or application In the Analytics Lifecycle, the emphasis is usually on speed and explanatory value

To support a narrative —Analytics professionals often use techniques to

directly support a story, such as confirming a hypothesis or visualizing a

relationship The outcome is of primary benefit to emphasize accuracy and reliability, and not necessarily the repeatability of the process

To understand a phenomenon —We often embark on an analytics

project to understand a phenomenon more fully in the most general and

parsimonious way possible Techniques such as visual analytics or

exploratory data analysis can support the discovery of these relationships

To discover something new —Analytics can be used to tell us something

that we didn't already know We can trace this objective back to its

heritage in data mining; it often takes advantage of new analytics

methods or unstructured and unorganized data, and it is often focused

on innovation-type endpoints Curiosity and inquisitiveness motivate the discovery about the relationships and associations

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Motivation for Analytics

Exploratory data analysis, statistical analysis, advanced analytics, time series, design of experiments

Machine learning, data mining, time series, statistics, text mining, visual statistics, design of experiments

Solve a Problem

Understand a phenomenon

Discover something new

Support a narrative

Visual analytics, decision support, exploratory data analysis, forecasting

Visual analytics, machine learning, text analytics, artificial intelligence, design of experiments

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Problem framing

Problem

definition

Root cause investigation

Hypothesis generation

Question design

Business case B ig D a ta

prioritization For Analytics

Data sense making

Data identification & priority

Data collection & preparation

Data profiling

& characterization

Visual exploration

Analytics model development

Making comparison

Measuring associations

Making predictions

Detecting patterns

Results activation

Solution evaluation

Presentation and storytelling

Operationalization

Analytics Product Lifecycle Management

Analytics

lifecycle

Stakeholder

engagement

Quality processes

Capability and talent development

ATABA-BEADCourse Conduct ManagiengxtheeBcig uDatta ion

© 2016-2023 NUS All rights reserved

|      Problem definition      | Identify and characterize the business problemneed.<br >Manage the problem definition and impact.<br >Support the justification for effort.<br >Reformulate problem statement as an analytics problem andor technical requirements.<br >Identify assumptions related to the problem and proposed solution.<br >Refine the business and analytics problem statements. |
| :--------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Root cause<br >investigation |                                               Utilize brainstorming techniques and effectively use divergent thinking<br >processes to uncover potential cause-and-effect relationships.<br >Classify requirements appropriately and determine feasibility.<br >Apply root cause analysis to requirement definitions.                                                |
|    Hypothesis generation     |           Generate (and manage) testable hypotheses.<br >Validate expected results and key requirements information with stakeholders.<br >Generate testable theories and validate their reasonableness.<br >Shadow workflows that are not understood.<br >Conduct primary and secondary research as needed to understand potential sources of the issue.            |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

|      Problem definition      |                                                  Utilize the FINER criteria (Car, 2013) to evaluate whether a problem can be translated into a question that can be answered.<br >Convert a question into a proper study design.                                                  |
| :--------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Business case prioritization | Prioritize requirements based on business value, cost to deliver, and time constraints.<br >Validate that solution design meets the business need.<br >Define the capabilities needed to support solution.<br >Manage the metrics related to solution implementation and success. |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Data sense making

| Data identification and prioritization |                                            Articulate the data required to solve the problem.<br >Reconcile the difference between the data we can get versus data that<br >we want.<br >Trace back the business and operational workflows reflected in the data.<br >Articulate the provenance and governance assumptions of the data.                                             |
| :------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|    Data collection and preparation     | Extract data from large, structured data stores.<br >Extract data from unstructured data sources.<br >Integrate data from multiple sources.<br >Ensure privacy and protection of data.<br >Utilize a variety of methods to cleanse andor enrich data.<br >Map results back to business and operational workflows.<br >Model the data appropriately for the type of analysis needed. |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

| Data profiling and characterization | Identify relationships in the data.<br >Perform exploration of unknown data.<br >Profile datasets.<br >Develop and execute a structured process to describe the aggregate trends, features, and culture of a data set.<br >Generate descriptive statistics, frequency analysis, and distributions of data (aka, exploratory data analysis – EDA).<br >Identify and investigate outlier data.<br >Develop theories that might address the problem. |
| :---------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|         Visual exploration          |                                                                                            Utilize a variety of programmatic and menu-driven visualization tools to examine associations.<br >Utilize principles of good design to craft visuals appropriate to their type.<br >Create graphics that help express the context and insight of the data.                                                                                            |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Analytics model development

Maki n g c o m p aris o n s •

Determine appropriate statistical tests and utilize them in basing conclusions

Apply a wide variety of statistical models, processes, routines, and

measures to compare two or more groups

Compare and contrast features of categorical and numerical data sets using appropriate tests

Apply quantitative measures to describe the properties of a sample of data

Define and apply statistical significance, confidence intervals, effect size,

and hypothesis testing

Differentiate between categorical versus continuous data and the appropriateness of various testing strategies used for making inferences

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

| Measuring associations |     Utilize visualization methods to examine relationships between different types of data.<br >Distinguish between an explanatory and response variable and their role<br >in tests of association.<br >Describe the types of tests used in measuring associations including those in parametric and non-parametric testing.<br >Relay the difference between an association and a cause-and-effect relationship.     |
| :--------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|   Making predictions   | Identify the two classes of prediction models.<br >Enumerate the types and methods of supervised and unsupervised methods used for prediction models.<br >Relate the type of prediction problem being asked back to the methods available in statistics, data mining, and machine learning.<br >Recognize common analytics methods such as predictive models, cluster analysis, neural networks, and machine learning. |
|   Detecting patterns   |                                                                 Classify the types of problems that we can solve using pattern recognition.<br >Describe the various classification approaches.<br >Illustrate the difference between feature selection and feature extraction.<br >Describe the difference between classification and discrimination.                                                                 |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Results activation

|        Value management        |                                                                                                                                                         Conduct dataanalytics output interpretation.<br >Coach and mentor stakeholders.<br >Perform business validation of the model.<br >Compare results from various models.<br >Explore alternative explanations.                                                                                                                                                         |
| :----------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|       Operationalization       | Incorporate a set of analytics and insights into business workflow such that a continual, positive benefit is seen and the organizational learning paradigm is realized.<br >Create model, usability, and system requirements for production.<br >Deliver production model.<br >Support the business process change.<br >Support the implementation of the model.<br >Assess actionability and impact to operational workflows.<br >Document and communicate findings (including assumptions, limitations, and constraints). |
| Presentations and storytelling |                                                                                                                                             Communicate effectively with various audiences.<br >Create data visualizations that convey meaning.<br >Deliver report with findings.<br >Evangelize value of analyticsbusiness benefits.<br >Socialize analytics results, advances.                                                                                                                                             |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Analytics Product Lifecycle management

|       Value management        |                                                                                                    Strategic alignment.<br >Develop and support a collaborative product management culture.<br >Analytics evangelism.<br >Evaluate the business benefit of analytics over time.                                                                                                    |
| :---------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Analytics lifecycle execution |                     Analytics prioritization.<br >Utilize project management principles to define, execute, and manage project activities.<br >Develop and establish project goals and milestones.<br >Implement project standards and procedures.<br >Monitor and analyze project costs.<br >Estimate project work.<br >Manage capital and expense reporting.                     |
|       Quality processes       | Improve the way in which data is governed.<br >Create and follow quality management plans.<br >Promote continuous improvement.<br >Follow robust testing and quality processes.<br >Utilize a risk-based validation approach.<br >Participate in peer reviews of both code and data products.<br >Document and improve quality principles.<br >Track model quality and durability. |
|                               |                                                                                                                                                                          Recalibrate and maintain models.                                                                                                                                                                          |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

| Stakeholder engagement and feedback |                                                                                  Analyze impact of change.<br >Support training and communication activities.<br >Manage change.<br >Promote processes for managing change and measuring the impact of decisions<br >Promote a culture of sharing and collaboration.                                                                                  |
| :---------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|  Capability and talent development  | Manage resources and evaluate performance.<br >Manage portfolio of projects with available resources.<br >Manage talent development.<br >Provide performance improvement coaching.<br >Manage conflict.<br >Demonstrate leadership and influence with the team and with external stakeholders.<br >Assess lessons learned.<br >Catalog data assets and ensure that metadata is accessible and usable. |

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

"What is the one sentence summary of how you change the world? Always Work

hard on something uncomfortably exciting!"

~Larry Page

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Essential Points

Big Data Engin e ering

- Big Data analysis blends traditional statistical data analysis approaches with computational ones
  - The overall goal of data analysis is to support better decision-making
  - Carrying out data analysis helps establish patterns and relationships among the data being analysed
- The Big Data analytics lifecycle can be divided into the
- following nine stages
  - Business Case Evaluation; Big Data Identification; Big Data Acquisition & Filtering; Big Data Extraction; Big Data Validation & Cleansing; Big Data Aggregation & Representation; Big Data Analysis; Big Data Visualization; Big Data Utilization of Analysis Results;
- There are many analysis techniques such as Quantitative analysis, Qualitative analysis, Data mining, Statistical analysis, Machine learning, Semantic analysis and Visual analysis

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

"If you're not doing some things that are crazy, then you're doing the

wrong things"

~Larry Page

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## References

The Analytics Lifecycle Toolkit, by Gregory S Nelson, Published by Wiley,

2018

Big Data Fundamentals: Concepts, Drivers & Techniques by Thomas Erl; Wajid Khattak; Paul Buhler, Published by Prentice Hall, 2016

Hadoop Essentials by Swizec Teller, Published by Packt Publishing, 2015

Data Science from Scratch by Joel Grus, Published by O'Reilly Media, Inc,

2015

Designing Data-Intensive Applications, 1st Edition, by Martin Kleppmann, Published by O'Reilly Media, Inc, 2017

Data Analytics with Hadoop, by Jenny Kim; Benjamin Bengfort, Published

by O'Reilly Media, Inc, 2016

YARN Essentials by Amol Fasale; Nirmal KumarPublished by Packt Publishing, 2015

Big Data Bootcamp: What Managers Need to Know to Profit from the Big Data Revolution by David Feinleib Published by Apress, 2014

Instant Apache Sqoop by Ankit Jain Published by Packt Publishing, 2013

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## MUBI Movie Recommendation System

Build a recommendation system which can recommend MUBI users with their most-interested-and-loved movies, providing personalized experience

Combine with social media analysis (Twitter streaming data) to track people’s current points of interest and popularity of movies, trying to reinforce the movie recommendation system

Combine with text analytics to predict the lack rating score from a user on a movie based on the user’s review text, trying to reinforce the movie recommendation system

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Overall Architecture

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Data Sources

The first dataset is from Kaggle website, called MUBI SVOD Platform Database for Movie Lovers

The second data source is real-time social media data (text messages) from Twitter API (https:developertwittercomendocstwitter-api), which will be used for social media analysis to track current points of interest and popularity of movies

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Batch data ingestion and processing

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Twitter streaming data processing

Movie Recommendation System

Collaborative Filtering

Text processing (Reviews)

Social Media Analysis(Twitter Streaming Data)

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved

## Web Front-end Deployment

Used Django framework to build a demo website to show the results of movies and movie

recommendation

ATABA-BEADCourse Conduct Managing the Big Data

All rights reserved
